[
  {
    "question": "In the context of Databricks, what is the primary advantage of using a Serverless Databricks SQL endpoint/warehouse?",
    "options": [
      "It specializes in real-time data streaming and complex event processing for IoT applications.",
      "It is primarily designed for large-scale machine learning workloads requiring extensive computational resources.",
      "It provides a dedicated environment for developing complex data pipelines and ETL processes.",
      "It enables quick-start capabilities, significantly reducing the time to initiate SQL queries and data analysis tasks.",
      "It offers enhanced data security and privacy controls suitable for sensitive data processing."
    ],
    "answer": 3,
    "category": "Databricks SQL",
    "explanation": "Serverless SQL endpoints in Databricks are designed to quickly start SQL queries and data tasks without infrastructure management."
  },
  {
    "question": "In Databricks, how do you identify the owner of a table using Catalog Explorer?",
    "options": [
      "The owner is displayed when clicking on a table in the Catalog Explorer.",
      "By inspecting the table's creation script in Catalog Explorer.",
      "Through the 'Owner' column in the Catalog Explorerâ€™s table list.",
      "Table ownership is not visible in Catalog Explorer.",
      "By executing a SQL query in Catalog Explorer to retrieve the table owner."
    ],
    "answer": 2,
    "category": "Databricks Catalog",
    "explanation": "The 'Owner' column in the table list of Catalog Explorer directly shows the ownership information."
  },
  {
    "question": "You are analyzing a dataset in Databricks SQL named WeatherReadings which includes the columns StationID (integer), ReadingTimestamp (timestamp), and Temperature (float). You need to calculate the average temperature for each station in 1-hour windows, sliding every 30 minutes. Which SQL query correctly uses the windowing function to achieve this?",
    "options": [
      "SELECT StationID, window(ReadingTimestamp, '1 hour', '30 minutes'), AVG(Temperature) FROM WeatherReadings GROUP BY StationID, window(ReadingTimestamp, '1 hour', '30 minutes');",
      "SELECT StationID, AVG(Temperature) OVER (PARTITION BY StationID ORDER BY ReadingTimestamp RANGE BETWEEN INTERVAL '30 minutes' PRECEDING AND INTERVAL '30 minutes' FOLLOWING) FROM WeatherReadings;",
      "SELECT StationID, AVG(Temperature) OVER (PARTITION BY StationID ORDER BY ReadingTimestamp ROWS BETWEEN INTERVAL '30 minutes' PRECEDING AND CURRENT ROW) FROM WeatherReadings;",
      "SELECT StationID, window(ReadingTimestamp, '1 hour'), AVG(Temperature) FROM WeatherReadings GROUP BY StationID, window(ReadingTimestamp, '1 hour');",
      "SELECT StationID, AVG(Temperature) OVER (PARTITION BY StationID ORDER BY ReadingTimestamp) FROM WeatherReadings;"
    ],
    "answer": 0,
    "category": "SQL Window Functions",
    "explanation": "The window function allows calculating the average temperature within defined time intervals, in this case, 1-hour windows with a 30-minute sliding interval."
  },
  {
    "question": "In Databricks, what distinguishes a managed table from an unmanaged (external) table in terms of data and metadata management?",
    "options": [
      "Managed tables allow for external file storage, whereas unmanaged tables store data within the Databricks environment.",
      "Managed tables store both data and metadata in the cloud, while unmanaged tables store only metadata.",
      "Unmanaged tables enable ACID transactions, unlike managed tables.",
      "Managed tables require manual data deletion after dropping the table, whereas unmanaged tables automatically delete data.",
      "Managed tables have their data and metadata managed by Databricks, while unmanaged tables have only metadata managed."
    ],
    "answer": 4,
    "category": "Table Management",
    "explanation": "Databricks fully manages both data and metadata for managed tables, whereas for unmanaged tables only the metadata is managed."
  },
  {
    "question": "In the context of Databricks dashboards, how do query parameters influence the output of underlying SQL queries within a dashboard?",
    "options": [
      "Query parameters serve as placeholders in SQL queries, allowing for dynamic data filtering based on user input, thus altering the output of the query according to the specified parameter values.",
      "Query parameters are used to format the visual aspects of the output, such as color and font, without changing the actual data returned by the query.",
      "They modify the layout of the dashboard, rearranging the visualizations based on user preferences, but do not change the data output of the queries.",
      "Query parameters act as static reference points in SQL queries, ensuring that the output remains constant irrespective of user interactions.",
      "They automatically update the SQL queries on a set schedule, such as daily or weekly, to change the output data based on temporal parameters."
    ],
    "answer": 0,
    "category": "Dashboard Parameters",
    "explanation": "Query parameters in Databricks dashboards allow for dynamic filtering, adjusting the data returned based on user input and thus making dashboards more interactive and customizable."
  },
  {
    "question": "How does Delta Lake manage table metadata in Databricks?",
    "options": [
      "By storing metadata in a separate, dedicated cloud storage.",
      "By automatically synchronizing metadata with the primary data storage.",
      "Delta Lake does not manage table metadata.",
      "Through manual updates by the database administrator.",
      "By maintaining a transaction log that records metadata changes."
    ],
    "answer": 4,
    "category": "Delta Lake",
    "explanation": "Delta Lake uses a transaction log to track metadata changes, ensuring data consistency and enabling time travel queries."
  },
  {
    "question": "Consider the following two tables: Colors and Shapes. After executing an SQL JOIN query, you receive the following result. What type of JOIN was used to produce this result?",
    "tables": {
      "Colors": [
        {
          "ColorId": 1,
          "ColorName": "Red"
        },
        {
          "ColorId": 2,
          "ColorName": "Blue"
        }
      ],
      "Shapes": [
        {
          "ShapeId": 1,
          "ShapeName": "Circle"
        },
        {
          "ShapeId": 2,
          "ShapeName": "Square"
        }
      ]
    },
    "result": [
      {
        "ColorId": 1,
        "ColorName": "Red",
        "ShapeId": 1,
        "ShapeName": "Circle"
      },
      {
        "ColorId": 1,
        "ColorName": "Red",
        "ShapeId": 2,
        "ShapeName": "Square"
      },
      {
        "ColorId": 2,
        "ColorName": "Blue",
        "ShapeId": 1,
        "ShapeName": "Circle"
      },
      {
        "ColorId": 2,
        "ColorName": "Blue",
        "ShapeId": 2,
        "ShapeName": "Square"
      }
    ],
    "options": [
      "FULL JOIN",
      "LEFT JOIN",
      "INNER JOIN",
      "RIGHT JOIN",
      "CROSS JOIN"
    ],
    "answer": 4,
    "category": "SQL Joins",
    "explanation": "The result shows all possible combinations between the two tables, indicating a CROSS JOIN, which returns the Cartesian product of the tables."
  },
  {
    "question": "In Databricks SQL, how is a 'Query Based Dropdown List' used to enhance the functionality of a dashboard with query parameters?",
    "options": [
      "It dynamically generates a dropdown list based on the distinct output of a separate query, allowing users to select values as query parameters.",
      "It creates a fixed list of predefined options that users can choose from to filter dashboard data.",
      "It automatically updates query parameters based on external data sources, without user interaction.",
      "It's used to manually input query parameters, unrelated to the output of any other query.",
      "The dropdown list is purely aesthetic, with no impact on the actual queries or data displayed."
    ],
    "answer": 0,
    "category": "Dashboard Enhancement",
    "explanation": "A 'Query Based Dropdown List' is used to dynamically populate options based on the result of a query, enhancing user interaction and filtering on the dashboard."
  },
  {
    "question": "Assuming you have a table TrafficData with columns Timestamp (timestamp) and VehicleCount (integer), and you need to calculate the sum of VehicleCount for every 10-minute window. Which SQL query using the window_time function correctly achieves this in a Databricks environment?",
    "options": [
      "SELECT window_time(Timestamp, '10 minutes'), COUNT(VehicleCount) FROM TrafficData GROUP BY Timestamp;",
      "SELECT window_time(Timestamp, '10 minutes'), SUM(VehicleCount) FROM TrafficData GROUP BY window_time(Timestamp, '10 minutes');",
      "SELECT window_time(Timestamp, '10 minutes'), AVG(VehicleCount) FROM TrafficData GROUP BY window_time(Timestamp, '10 minutes');",
      "SELECT Timestamp, SUM(VehicleCount) OVER (ORDER BY Timestamp RANGE BETWEEN INTERVAL 10 MINUTES PRECEDING AND CURRENT ROW) FROM TrafficData;",
      "SELECT Timestamp, SUM(VehicleCount) OVER (PARTITION BY window_time(Timestamp, '10 minutes')) FROM TrafficData;"
    ],
    "answer": 1,
    "category": "Window Functions",
    "explanation": "The query groups data by 10-minute intervals using the window_time function and calculates the sum of VehicleCount for each interval."
  },
  {
    "question": "When using the schema browser in the Query Editor of Databricks, what type of information can you expect to find displayed? Choose the most appropriate option.",
    "options": [
      "List of all users and groups who have access to the Databricks workspace.",
      "Only the SQL queries that have been executed in the past sessions.",
      "Real-time performance metrics and logs of the Databricks cluster.",
      "Detailed documentation and syntax for all SQL functions and commands.",
      "Available databases, tables, and columns, along with their data types."
    ],
    "answer": 4,
    "category": "Schema Browser",
    "explanation": "The schema browser displays available databases, tables, and columns with data types, aiding users in query construction."
  },
  {
    "question": "In the context of analytics, what is an example of effectively enhancing data in a common application?",
    "options": [
      "Integrating weather data into a retail sales analysis to understand the impact of weather on sales trends.",
      "Restricting data access to a limited number of users to ensure data security.",
      "Strictly categorizing data based on its source without additional processing.",
      "Keeping data in its original, raw format for archival purposes.",
      "Performing routine software updates on data analysis tools without modifying data."
    ],
    "answer": 0,
    "category": "Data Enhancement",
    "explanation": "Integrating external data, such as weather, enhances analytics by providing additional context that may affect sales trends."
  },
  {
    "question": "Consider two tables in a database:\n\nTable 1: Employees\n\n| EmployeeID | Name        | Department |\n|------------|-------------|------------|\n| 101        | Alan Smith  | HR         |\n| 102        | Betty Jones | Marketing  |\n| 103        | Carl Brown  | IT         |\n| 104        | Donna Ray   | Sales      |\n\nTable 2: Salaries\n\n| EmployeeID | Salary |\n|------------|--------|\n| 101        | 70000  |\n| 102        | 60000  |\n| 103        | 50000  |\n| 104        | 80000  |\n\nGiven the output of a join between the two tables:\n\n| Name       | Salary |\n|------------|--------|\n| Donna Ray  | 80000  |\n| Alan Smith | 70000  |\n\nWhich of the queries below could have been used to generate the output?",
    "options": [
      "SELECT E.Name, S.Salary FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary > (SELECT AVG(Salary) FROM Salaries) ORDER BY S.Salary DESC;",
      "SELECT E.Name, S.Salary FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary >= (SELECT AVG(Salary) FROM Salaries) ORDER BY S.Salary DESC;",
      "SELECT E.Name, S.Salary FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary > (SELECT AVG(Salary) FROM Salaries) ORDER BY S.Salary ASC;",
      "SELECT * FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary > (SELECT AVG(Salary) FROM Salaries) ORDER BY S.Salary DESC;",
      "SELECT E.Name, S.Salary FROM Employees E INNER JOIN Salaries S ON E.EmployeeID = S.EmployeeID WHERE S.Salary >= (SELECT MAX(Salary) FROM Salaries) ORDER BY S.Salary DESC;"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "The correct query joins the Employees and Salaries tables on EmployeeID, filters salaries greater than the average salary, and orders the result by salary in descending order to match the output requirement. This produces only the rows for Donna Ray and Alan Smith, with their respective salaries of 80000 and 70000."
  },
  {
    "question": "What is the correct sequence of steps to execute a SQL query in Databricks?",
    "options": [
      "Create a query using Terraform, execute the query in a Databricks job, and use COPY INTO to load data.",
      "Manually input data, write a query in Databricks notebook, execute the query, and export the results.",
      "Choose a SQL warehouse, construct and edit the query, execute the query, and visualize results.",
      "Write the query in an external tool, import it into Databricks, select a data source, and execute the query.",
      "Open SQL Editor, select a SQL warehouse, construct and edit the query, execute the query."
    ],
    "answer": 4,
    "category": "SQL Execution",
    "explanation": "The standard approach in Databricks for executing a SQL query involves using the SQL Editor, selecting the appropriate warehouse, constructing the query, and executing it."
  },
  {
    "question": "A data analyst is tasked with presenting yearly revenue data to stakeholders in a way that is both informative and visually appealing. The analyst decides to use a line graph to show the revenue trends over the year. What is the best approach the analyst should take in terms of formatting the graph?",
    "options": [
      "Add multiple background images related to the companyâ€™s business to make the graph more engaging.",
      "Use 3D effects on the graph lines to give a more modern and advanced look.",
      "Use a wide range of vibrant colors for different data points to make the graph more colorful.",
      "Apply a minimalistic design with a consistent color scheme and clear labeling to enhance focus on the data.",
      "Incorporate various font styles and sizes for each data point to make the graph more dynamic."
    ],
    "answer": 3,
    "category": "Data Visualization",
    "explanation": "A minimalistic design with clear labeling and a consistent color scheme enhances readability and maintains focus on the data trends rather than unnecessary visual elements."
  },
  {
    "question": "You need to apply a custom scaling function to normalize transaction amounts for analysis. You decide to create a UDF (User-Defined Function) in Python. Which of the following approaches correctly illustrates the creation and application of a UDF for this purpose?",
    "options": [
      "Use a built-in SQL function to normalize TransactionAmount directly within a SQL query without defining a UDF.",
      "Create a Python script outside Databricks to preprocess the data, then import the normalized dataset into Databricks.",
      "Define a Python function normalize(amount) and register it as a UDF, then use SELECT normalize(TransactionAmount) FROM Transactions;",
      "Implement a machine learning model within Databricks to automatically normalize TransactionAmount.",
      "Manually apply the normalization function to each row of the Transactions dataset using a Spark DataFrame loop."
    ],
    "answer": 2,
    "category": "Python UDF",
    "explanation": "Defining and registering a Python function as a UDF allows for the normalization of TransactionAmount directly within SQL queries in Databricks."
  },
  {
    "question": "Given a sales database with a table SalesData containing columns Region, ProductType, and SalesAmount, you are tasked with creating a report that includes the total sales amount for each combination of Region and ProductType, as well as totals for each Region alone and the overall total. Which SQL query correctly generates this report?",
    "table": [
      {
        "Region": "North",
        "ProductType": "Electronics",
        "SalesAmount": 1000
      },
      {
        "Region": "North",
        "ProductType": "Clothing",
        "SalesAmount": 500
      },
      {
        "Region": "South",
        "ProductType": "Electronics",
        "SalesAmount": 800
      },
      {
        "Region": "South",
        "ProductType": "Clothing",
        "SalesAmount": 700
      },
      {
        "Region": null,
        "ProductType": "Electronics",
        "SalesAmount": 1800
      },
      {
        "Region": null,
        "ProductType": "Clothing",
        "SalesAmount": 1200
      },
      {
        "Region": "North",
        "ProductType": null,
        "SalesAmount": 1500
      },
      {
        "Region": "South",
        "ProductType": null,
        "SalesAmount": 1500
      },
      {
        "Region": null,
        "ProductType": null,
        "SalesAmount": 3000
      }
    ],
    "options": [
      "SELECT Region, ProductType, SUM(SalesAmount) FROM SalesData GROUP BY CUBE(Region, ProductType);",
      "SELECT Region, ProductType, SUM(SalesAmount) FROM SalesData GROUP BY Region, ProductType WITH CUBE;",
      "SELECT Region, ProductType, SUM(SalesAmount) FROM SalesData GROUP BY ROLLUP(Region, ProductType);",
      "SELECT Region, SUM(SalesAmount) FROM SalesData GROUP BY CUBE(Region);",
      "SELECT Region, ProductType, COUNT(SalesAmount) FROM SalesData GROUP BY CUBE(Region, ProductType);"
    ],
    "answer": 0,
    "category": "SQL Aggregation",
    "explanation": "The CUBE function generates all combinations of specified columns, which is needed to calculate totals at multiple hierarchical levels (e.g., by Region and ProductType, by Region only, and overall total)."
  },
  {
    "question": "In Databricks SQL, when dealing with the ingestion of data, how does the platform handle directories containing multiple files?",
    "options": [
      "Databricks SQL requires manual conversion of all files in a directory to a uniform format before ingestion.",
      "Databricks SQL automatically converts and ingests files of different types from a directory into a standard format.",
      "Databricks SQL can ingest directories of files, provided all files in the directory are of the same type, such as all CSV or all JSON.",
      "Databricks SQL can only ingest a single file at a time, regardless of file type.",
      "Databricks SQL can ingest directories containing files of mixed types, such as CSV and JSON, simultaneously."
    ],
    "answer": 2,
    "category": "Data Ingestion",
    "explanation": "Databricks SQL can ingest directories containing multiple files as long as all files in the directory are of the same type, such as CSV or JSON."
  },
  {
    "question": "In the context of Databricks, when dealing with the import of small text files, such as lookup tables or for quick data integrations, which approach is recommended to optimize performance and ease of use?",
    "options": [
      "Employ small-file upload techniques specifically designed for handling small text files efficiently.",
      "Upload small text files to a temporary storage service before importing them into Databricks.",
      "Always compress text files into larger archives before uploading, regardless of the original file size.",
      "Utilize large-file upload methods for all types of data, regardless of file size.",
      "Convert small text files to a binary format to increase upload speed and efficiency."
    ],
    "answer": 0,
    "category": "Data Import",
    "explanation": "Optimizing for small text files involves specific techniques that efficiently handle smaller data volumes without additional steps."
  },
  {
    "question": "In managing Databricks SQL endpoints or warehouses, a key consideration is the balance between computational power and cost. Which statement best encapsulates this trade-off?",
    "options": [
      "Smaller clusters are more expensive and offer high performance, suitable for complex analytics.",
      "Larger clusters enhance performance but increase costs, suitable for demanding data tasks, whereas smaller clusters reduce costs but may limit performance.",
      "Cluster size impacts only the storage capacity, not the cost or performance.",
      "Larger clusters offer lower performance but are more cost-effective, ideal for high-volume data processing.",
      "Cluster size has no impact on cost or performance in Databricks SQL."
    ],
    "answer": 1,
    "category": "Databricks SQL Management",
    "explanation": "Larger clusters improve performance but come with higher costs, making them suitable for demanding tasks, while smaller clusters are more cost-effective but may not perform as well."
  },
  {
    "question": "In the Databricks Unity Catalog, which SQL command correctly creates a new table named customer_data in a database sales under the catalog us_catalog, based on a SELECT query from an existing table transactions in the same database and catalog?",
    "options": [
      "NEW TABLE us_catalog.sales.customer_data FROM SELECT * IN transactions;",
      "TABLE CREATE us_catalog.sales.customer_data AS (SELECT * FROM transactions);",
      "CREATE TABLE customer_data IN us_catalog.sales AS SELECT * FROM transactions;",
      "CREATE TABLE us_catalog.sales.customer_data AS SELECT * FROM us_catalog.sales.transactions;",
      "us_catalog.sales: CREATE TABLE customer_data AS SELECT * FROM transactions;"
    ],
    "answer": 3,
    "category": "Unity Catalog",
    "explanation": "The correct syntax specifies the catalog and schema, followed by the CREATE TABLE command and the SELECT query."
  },
  {
    "question": "The Medallion Architecture in Databricks is a conceptual framework for data organization and pipeline management. How does it structure the data processing pipeline?",
    "options": [
      "It involves a single stage of data processing where raw, refined, and curated data are merged into a unified table known as the Platinum table.",
      "It uses a reverse-pyramid structure starting with the most refined data in the base layer, moving to semi-processed data, and ending with raw data at the top.",
      "None of the above.",
      "It starts with unstructured data in Gold tables, then structures the data in Silver tables, and finally stores the raw data in Bronze tables.",
      "It begins with raw data in Bronze tables, moves to refined data in Silver tables, and culminates in curated data in Gold tables."
    ],
    "answer": 4,
    "category": "Medallion Architecture",
    "explanation": "The Medallion Architecture in Databricks organizes data through a Bronze-Silver-Gold structure, progressing from raw data to curated data."
  },
  {
    "question": "In the context of a Databricks Lakehouse architecture, you are working with silver-level data that has been aggregated from various bronze tables. You notice inconsistencies in customer names due to variations in casing and spacing (e.g., 'John Doe', 'john doe', 'John    Doe'). What would be an appropriate SQL query to standardize these customer names in the silver table CustomerData?",
    "options": [
      "CREATE VIEW CleanCustomerData AS SELECT DISTINCT TRIM(LOWER(customer_name)) FROM CustomerData;",
      "SELECT customer_name FROM CustomerData GROUP BY customer_name;",
      "UPDATE CustomerData SET customer_name = TRIM(UPPER(customer_name));",
      "SELECT DISTINCT TRIM(UPPER(customer_name)) FROM CustomerData;",
      "ALTER TABLE CustomerData MODIFY COLUMN customer_name SET DATA TYPE VARCHAR(255) NOT NULL;"
    ],
    "answer": 0,
    "category": "Data Standardization",
    "explanation": "Creating a view with TRIM and LOWER functions standardizes names by removing extra spaces and making all letters lowercase."
  },
  {
    "question": "In a data analytics environment, how can dashboards be configured to automatically refresh and display the most current data?",
    "options": [
      "Dashboards automatically refresh only when the underlying data source is replaced with a new one.",
      "Dashboards can be set up to refresh automatically at specified intervals using built-in scheduling features.",
      "Automatic refreshes are achieved by scripting a periodic page reload in the web browser displaying the dashboard.",
      "Automatic dashboard refreshes require a complete system reboot at regular intervals.",
      "Dashboards must be manually refreshed by the user to display the latest data."
    ],
    "answer": 1,
    "category": "Dashboard Configuration",
    "explanation": "Databricks SQL allows setting up automatic refresh intervals for dashboards, enabling real-time data monitoring and updates."
  },
  {
    "question": "In data analytics, identify a scenario where data enhancement would significantly improve the outcome.",
    "options": [
      "In a marketing campaign, to enrich customer data with additional demographic and psychographic information for targeted advertising.",
      "When migrating data from one storage system to another without changing its format or content.",
      "When performing routine data backup and recovery processes.",
      "When aggregating large volumes of data for storage efficiency without analysis.",
      "During the initial stages of data collection where data volume is more critical than data quality."
    ],
    "answer": 0,
    "category": "Data Enhancement",
    "explanation": "Data enhancement is valuable in targeted advertising because it enables more precise targeting by enriching customer data."
  },
  {
    "question": "A data analyst at an e-commerce company is using Databricks SQL to visualize customer feedback scores (ranging from 1 to 5) against product categories. The analyst wants to identify patterns and outliers in the feedback scores across different categories. Which visualization type should the analyst select in Databricks SQL to effectively communicate these insights?",
    "options": [
      "Stacked bar chart, for showing the total feedback scores by category.",
      "Bubble chart, for showing the volume of feedback in relation to scores across categories.",
      "Radar chart, to compare the feedback scores of different categories in a circular format.",
      "Treemap, to represent feedback scores as proportions within each category.",
      "Box chart, to display the distribution of feedback scores within each category, highlighting the median, quartiles, and outliers."
    ],
    "answer": 4,
    "category": "Data Visualization",
    "explanation": "A box chart effectively shows the distribution of feedback scores by category, with additional insight into median, quartiles, and outliers."
  },
  {
    "question": "In the realm of data visualization and analysis, Databricks SQL dashboards serve a specific purpose. What is the primary function of Databricks SQL dashboards in the context of data query results?",
    "options": [
      "To display the output of a single, complex SQL query.",
      "To serve as an interactive platform for writing and testing new SQL queries.",
      "To showcase the results of multiple SQL queries simultaneously in a unified view.",
      "To execute real-time data transformations without displaying results.",
      "To store raw data for long-term archival purposes."
    ],
    "answer": 2,
    "category": "Databricks SQL Dashboards",
    "explanation": "Databricks SQL dashboards are designed to present results from multiple SQL queries in a single, unified view, allowing for comprehensive data visualization."
  },
  {
    "question": "What is a key benefit of using ANSI SQL as the standard query language in the Lakehouse architecture?",
    "options": [
      "It allows for real-time data streaming and complex event processing.",
      "It supports native machine learning algorithms.",
      "It ensures compatibility and interoperability across different database systems.",
      "It enables automatic data encryption and security.",
      "It provides enhanced graphical data visualization tools."
    ],
    "answer": 2,
    "category": "SQL Standards",
    "explanation": "ANSI SQL enables compatibility and interoperability, making it easier to integrate with various database systems within the Lakehouse architecture."
  },
  {
    "question": "Consider two tables in a database: Employees and Salaries. Given the output of a join between these tables, which of the queries below could have been used to generate the output?",
    "options": [
      "SELECT e.Name, s.Salary FROM Employees e JOIN Salaries s ON e.EmployeeID = s.EmployeeID WHERE s.Salary > 60000;",
      "SELECT e.Name, s.Salary FROM Employees e INNER JOIN Salaries s ON e.EmployeeID = s.EmployeeID WHERE s.Salary > 60000;",
      "SELECT e.Name, s.Salary FROM Employees e LEFT JOIN Salaries s ON e.EmployeeID = s.EmployeeID WHERE s.Salary > 60000;",
      "SELECT e.Name, s.Salary FROM Employees e RIGHT JOIN Salaries s ON e.EmployeeID = s.EmployeeID AND s.Salary > 60000;",
      "SELECT e.Name, s.Salary FROM Employees e LEFT JOIN Salaries s ON e.EmployeeID = s.EmployeeID AND s.Salary > 60000;"
    ],
    "answer": 1,
    "category": "SQL Joins",
    "explanation": "The INNER JOIN is used to match rows where EmployeeID is present in both tables, filtering with the condition WHERE s.Salary > 60000 to display employees with salaries over 60,000."
  },
  {
    "question": "You are working with a sales data table in Databricks SQL that contains columns for Region, Product, and SalesAmount. You want to generate a report that includes the total sales amount for each combination of Region and Product, as well as the total for each Region and the overall total. Which SQL query would you use to achieve this?",
    "options": [
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY Region, Product WITH CUBE;",
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY CUBE(Region, Product);",
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY Region, Product WITH ROLLUP;",
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY GROUPING SETS ((Region, Product), (Region), ());",
      "SELECT Region, Product, SUM(SalesAmount) FROM sales GROUP BY ROLLUP(Region, Product);"
    ],
    "answer": 4,
    "category": "SQL Aggregation",
    "explanation": "ROLLUP provides subtotals for each level of grouping, as well as the overall total, making it ideal for hierarchical summaries."
  },
  {
    "question": "Which of the following best describes the purpose of descriptive statistics in data analysis?",
    "options": [
      "To summarize and describe the main features of a dataset.",
      "To establish causal relationships between different variables.",
      "To categorize data into distinct groups based on algorithmic models.",
      "To test hypotheses about the relationship between variables.",
      "To make predictions about future trends based on historical data."
    ],
    "answer": 0,
    "category": "Statistics",
    "explanation": "Descriptive statistics are used to summarize and describe the main features of a dataset, providing an overview of its characteristics."
  },
  {
    "question": "What are the key differences in behavior between managed and unmanaged tables in Databricks?",
    "options": [
      "Managed tables support ACID transactions, while unmanaged tables do not.",
      "Managed tables automatically back up data, whereas unmanaged tables require manual backups.",
      "Managed tables can only store structured data, while unmanaged tables can store both structured and unstructured data.",
      "Managed tables store their data in a default location managed by Databricks, while unmanaged tables allow specifying a storage location.",
      "Unmanaged tables are optimized for streaming data, whereas managed tables are not."
    ],
    "answer": 3,
    "category": "Table Management",
    "explanation": "Managed tables are fully controlled by Databricks and stored in a default location, whereas unmanaged tables require a specified storage path."
  },
  {
    "question": "How do you set the location of a table in Databricks when creating or altering it?",
    "options": [
      "By using the CREATE TABLE ... LOCATION or ALTER TABLE ... SET LOCATION command.",
      "By using the SET LOCATION command in the table creation query.",
      "By moving the data files to the desired location manually.",
      "Location setting is not supported in Databricks.",
      "By specifying the location in the Databricks UI during table creation."
    ],
    "answer": 0,
    "category": "Table Management",
    "explanation": "The CREATE TABLE ... LOCATION and ALTER TABLE ... SET LOCATION commands allow users to specify or change the location of a table's data files."
  },
  {
    "question": "In Databricks, a data analyst needs to share a dashboard with colleagues (with relevant access to the workspace), ensuring that the dashboard displays up-to-date results whenever it is accessed. Which method should the analyst use to share the dashboard for this purpose?",
    "options": [
      "Export the dashboard as a static PDF and email it to colleagues.",
      "Share a direct link to the dashboard that is hosted on the Databricks platform, allowing colleagues to view the latest results in real-time.",
      "Take screenshots of the dashboard and share them via a messaging platform.",
      "Print out the dashboard and distribute physical copies to colleagues.",
      "Save the dashboard as a static HTML file and share it via a file-sharing service."
    ],
    "answer": 1,
    "category": "Data Sharing",
    "explanation": "Sharing a direct link within Databricks allows colleagues to access the dashboard with real-time updates, ensuring data is current."
  },
  {
    "question": "Which of the following features is a primary function of the Catalog Explorer in Databricks?",
    "options": [
      "Previewing and exploring data, along with configuring security and access controls.",
      "Scheduling and automating data pipeline workflows.",
      "Generating real-time analytics and visualizations without the need for coding.",
      "Automatically transforming data into normalized forms for analysis.",
      "Executing complex machine learning algorithms on large datasets."
    ],
    "answer": 0,
    "category": "Catalog Explorer",
    "explanation": "The Catalog Explorer in Databricks primarily supports data preview and exploration, while also allowing for security and access configuration."
  },
  {
    "question": "Consider the following two tables: Employees and Departments. Given the output from joining both tables, which of the queries below could have been used to generate the output?",
    "tables": {
      "Employees": [
        {
          "EmployeeID": 1,
          "Name": "John Smith"
        },
        {
          "EmployeeID": 2,
          "Name": "Jane Doe"
        },
        {
          "EmployeeID": 3,
          "Name": "Alice Johnson"
        },
        {
          "EmployeeID": 4,
          "Name": "Bob Ray"
        }
      ],
      "Departments": [
        {
          "EmployeeID": 2,
          "Department": "Marketing"
        },
        {
          "EmployeeID": 3,
          "Department": "Sales"
        },
        {
          "EmployeeID": 4,
          "Department": "IT"
        },
        {
          "EmployeeID": 5,
          "Department": "HR"
        }
      ]
    },
    "output": [
      {
        "Name": "John Smith",
        "Department": null
      },
      {
        "Name": "Jane Doe",
        "Department": "Marketing"
      },
      {
        "Name": "Alice Johnson",
        "Department": "Sales"
      },
      {
        "Name": "Bob Ray",
        "Department": "IT"
      }
    ],
    "options": [
      "SELECT Employees.Name, Departments.Department FROM Employees FULL JOIN Departments ON Employees.EmployeeID = Departments.EmployeeID;",
      "SELECT Employees.Name, Departments.Department FROM Employees LEFT JOIN Departments ON Employees.EmployeeID = Departments.EmployeeID;",
      "SELECT * FROM Employees INNER JOIN Departments ON Employees.EmployeeID = Departments.EmployeeID;",
      "SELECT * FROM Employees LEFT JOIN Departments ON Employees.EmployeeID = Departments.EmployeeID;"
    ],
    "answer": 1,
    "category": "SQL Joins",
    "explanation": "A LEFT JOIN returns all records from the left table (Employees) and the matched records from the right table (Departments). Where there is no match, NULL values are returned for columns from the right table, which matches the output shown."
  },
  {
    "question": "In data engineering, how is 'performing last-mile ETL as project-specific data enhancement' best described?",
    "options": [
      "Conducting final data transformations and enrichments specific to the needs of a particular project.",
      "Performing initial data extraction from various source systems into a staging area.",
      "Implementing general ETL processes applicable to all datasets across the organization.",
      "Migrating all data to a centralized data warehouse for unified access.",
      "Utilizing advanced data analytics techniques to generate predictive insights."
    ],
    "answer": 0,
    "category": "ETL Processes",
    "explanation": "Last-mile ETL focuses on final, specific transformations or enhancements tailored to individual project requirements, adding value to data at the end of the pipeline."
  },
  {
    "question": "In a Spark SQL dataset EmployeeData, you have a column monthlyPerformanceRatings which is an array of integers representing monthly performance ratings of employees. You are tasked with identifying employees whose performance has consistently improved over the last three months. Which Spark SQL query utilizing a higher-order function is best suited for this task?",
    "options": [
      "SELECT employeeId FROM EmployeeData WHERE ZIP_WITH(monthlyPerformanceRatings, monthlyPerformanceRatings, (current, next) -> next > current);",
      "SELECT employeeId FROM EmployeeData WHERE REDUCE(monthlyPerformanceRatings, 0, (acc, rating) -> acc + rating, acc -> acc) > 3;",
      "SELECT employeeId FROM EmployeeData WHERE ARRAY_SORT(monthlyPerformanceRatings) = monthlyPerformanceRatings;",
      "SELECT employeeId FROM EmployeeData WHERE EXISTS(monthlyPerformanceRatings, rating -> rating > 3);",
      "SELECT employeeId FROM EmployeeData WHERE SLICE(monthlyPerformanceRatings, -3, 3) = ARRAY_SORT(SLICE(monthlyPerformanceRatings, -3, 3));"
    ],
    "answer": 4,
    "category": "Spark SQL",
    "explanation": "The SLICE and ARRAY_SORT functions are used to check if the last three ratings are in ascending order, indicating improvement."
  },
  {
    "question": "In Databricks SQL, which types of visualizations can be developed to represent data?",
    "options": [
      "Bar Chart, Line Graph, Heatmap, Gauge.",
      "Table, Chart, Sankey Diagram, Scatter Plot.",
      "Line Chart, Bubble Chart, Word Cloud, Counter.",
      "Table, Details, Counter, Pivot.",
      "Histogram, Radar Chart, Tree Map, Choropleth."
    ],
    "answer": 0,
    "category": "Data Visualization",
    "explanation": "Common visualization options in Databricks SQL include bar charts, line graphs, heatmaps, and gauges for representing data insights."
  },
  {
    "question": "In the landscape of Business Intelligence (BI) and data analytics, what role does Databricks SQL play when integrated with other BI tools?",
    "options": [
      "Databricks SQL primarily enhances data visualization capabilities.",
      "Databricks SQL and BI tools cannot be used together.",
      "Databricks SQL is used only for data storage, with BI tools handling all data processing and analysis.",
      "Databricks SQL acts as a data processing and query engine, complementing BI tools for enhanced data analysis and reporting.",
      "Databricks SQL replaces traditional BI tools for data analysis and reporting."
    ],
    "answer": 3,
    "category": "BI Integration",
    "explanation": "Databricks SQL serves as a processing engine that complements BI tools by handling data queries and processing, which BI tools can then use for visualization and reporting."
  },
  {
    "question": "What is the minimum permission a user needs to configure a refresh schedule on a Databricks SQL Dashboard?",
    "options": [
      "Modify Permissions.",
      "Can Edit.",
      "Can View.",
      "No permissions.",
      "Owner."
    ],
    "answer": 1,
    "category": "Permissions",
    "explanation": "A user with 'Can Edit' permissions has sufficient access to configure a refresh schedule on a Databricks SQL Dashboard."
  },
  {
    "question": "In Databricks SQL, when alerts are configured based on specific criteria, how are notifications typically sent to inform users or administrators of the triggered alerts?",
    "options": [
      "Notifications are not supported for alerts in Databricks SQL Analytics.",
      "Notifications are sent through SMS messages to designated phone numbers when alerts are triggered.",
      "Notifications are automatically sent to the dashboardâ€™s viewers via email when alerts are triggered.",
      "Alerts generate a pop-up notification within the Databricks SQL Analytics interface, visible to all users.",
      "Alerts trigger notifications via a variety of channels, such as email, Slack, or webhook integrations, based on the defined configuration."
    ],
    "answer": 4,
    "category": "Databricks Alerts",
    "explanation": "Databricks SQL Analytics allows notifications to be configured across multiple channels, including email and webhook integrations."
  },
  {
    "question": "In a Databricks environment, you're analyzing query performance improvements. After several runs of a complex query on a large dataset, you notice a significant reduction in latency. What feature of Databricks is most likely contributing to this decrease in query execution time?",
    "options": [
      "Use of persistent tables instead of temporary views.",
      "Increased hardware resources allocation.",
      "Automatic query rewriting for optimization.",
      "Caching of intermediate data and results from previous query executions.",
      "Improved data indexing mechanisms."
    ],
    "answer": 3,
    "category": "Query Optimization",
    "explanation": "Databricks caches intermediate data, reducing latency for repeated query executions on large datasets."
  },
  {
    "question": "In the context of Databricks, there are distinct types of parameters used in dashboards and visualizations. Based on the descriptions provided, how do Widget Parameters, Dashboard Parameters, and Static Values differ in their application and impact?",
    "options": [
      "Widget Parameters are tied to a single visualization and affect only the query underlying that specific visualization. Dashboard Parameters can influence multiple visualizations within a dashboard and are configured at the dashboard level. Static Values are used to replace parameters, making them 'disappear' and setting a fixed value in place.",
      "Dashboard Parameters are specific to individual visualizations and cannot be shared across multiple visualizations within a dashboard. Widget Parameters are used at the dashboard level to influence all visualizations. Static Values change dynamically in response to user interactions.",
      "Both Widget Parameters and Dashboard Parameters have the same functionality and impact, allowing for dynamic changes across all visualizations in a dashboard. Static Values provide temporary placeholders for these parameters.",
      "Static Values are used to create interactive elements in dashboards, while Widget and Dashboard Parameters are used for aesthetic modifications only, without impacting the data or queries.",
      "Widget Parameters apply to the entire dashboard and can change the layout, whereas Dashboard Parameters are fixed and do not allow for interactive changes. Static Values are dynamic and change frequently based on user input."
    ],
    "answer": 0,
    "category": "Dashboard Parameters",
    "explanation": "Widget Parameters affect single visualizations, Dashboard Parameters can impact multiple visualizations at once, and Static Values lock in specific parameter values without user input."
  },
  {
    "question": "How can you change access rights to a table in Databricks using Catalog Explorer?",
    "options": [
      "By editing the table schema in the 'Schema' section of Data Explorer.",
      "Access rights can only be changed via the Databricks CLI, not in Data Explorer.",
      "By executing a SQL command in Data Explorer for access modification.",
      "By selecting the table, navigating to the 'Permissions' tab, and modifying permissions.",
      "Through the 'Access Rights' menu in the table's context menu in Data Explorer."
    ],
    "answer": 3,
    "category": "Access Management",
    "explanation": "In Databricks, access rights can be modified by navigating to the 'Permissions' tab in the table settings within Catalog Explorer."
  },
  {
    "question": "How do Delta Lake tables in Databricks maintain their historical data?",
    "options": [
      "By storing historical data in a separate cloud storage.",
      "By maintaining a versioned history of data changes for a configurable period of time.",
      "Delta Lake tables do not maintain historical data.",
      "By creating a new table for each update.",
      "Through automatic backups at regular intervals."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Delta Lake uses versioning to track historical changes, allowing data retrieval from different points in time."
  }
]