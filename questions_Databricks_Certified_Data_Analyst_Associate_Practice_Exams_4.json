[
  {
    "question": "What is the purpose of using 'Z-Ordering' in Databricks Delta Lake tables?",
    "options": [
      "To compress data files for storage efficiency.",
      "To optimize data skipping by colocating related information.",
      "To encrypt data at rest for security.",
      "To enable real-time data streaming capabilities.",
      "To partition data based on a hash of a column."
    ],
    "answer": 1,
    "category": "Delta Lake Optimization",
    "explanation": "Option B is correct because Z-Ordering improves data skipping during query execution by organizing data files based on the values of specified columns.\n\nOption A is incorrect; Z-Ordering is not for compression.\n\nOption C is incorrect; encryption is handled differently.\n\nOption D is incorrect; streaming is not enabled by Z-Ordering.\n\nOption E is incorrect; Z-Ordering is not about partitioning by hash."
  },
  {
    "question": "In Databricks, what is the 'Jobs' feature used for?",
    "options": [
      "To manage clusters.",
      "To schedule and run automated tasks.",
      "To create and edit notebooks.",
      "To set up data storage options.",
      "To configure user permissions."
    ],
    "answer": 1,
    "category": "Databricks Features",
    "explanation": "Option B is correct because the Jobs feature allows you to schedule and run automated tasks, including notebooks and workflows.\n\nOptions A, C, D, and E describe other features."
  },
  {
    "question": "In data visualization, what is a 'treemap' used for?",
    "options": [
      "Displaying hierarchical data as nested rectangles.",
      "Showing trends over time.",
      "Comparing individual data points.",
      "Displaying the frequency distribution of a dataset.",
      "Representing data using a geographical map."
    ],
    "answer": 0,
    "category": "Data Visualization",
    "explanation": "Option A is correct because a treemap visualizes hierarchical data using nested rectangles, where the size and color represent different attributes."
  },
  {
    "question": "What is the purpose of the 'CREATE VIEW' statement in SQL?",
    "options": [
      "To create a temporary table.",
      "To create a virtual table based on the result-set of a SELECT query.",
      "To insert data into an existing table.",
      "To modify data in an existing table.",
      "To delete data from an existing table."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because `CREATE VIEW` creates a virtual table (view) based on the result of a SELECT statement.\n\nOptions A, C, D, and E do not describe the purpose of `CREATE VIEW`."
  },
  {
    "question": "In Databricks, what is a 'table' in the context of the Data Catalog?",
    "options": [
      "A collection of notebooks.",
      "A structured data set stored in a database.",
      "A cluster configuration.",
      "A user permission setting.",
      "An external data source."
    ],
    "answer": 1,
    "category": "Databricks Features",
    "explanation": "Option B is correct because a table in the Data Catalog represents a structured data set stored within a database in Databricks."
  },
  {
    "question": "No Databricks, o que é um 'Cluster de Alta Concurrência' e quando deve ser usado?",
    "options": [
      "Um cluster otimizado para cargas de trabalho de streaming em tempo real.",
      "Um cluster que permite que múltiplos usuários compartilhem recursos de forma segura, ideal para ambientes multiusuário.",
      "Um cluster configurado para executar tarefas de machine learning intensivas.",
      "Um cluster dedicado exclusivamente a trabalhos agendados.",
      "Um cluster que automaticamente escala para o tamanho máximo sem restrições."
    ],
    "answer": 1,
    "category": "Gerenciamento de Clusters",
    "explanation": "A opção B está correta porque um 'Cluster de Alta Concurrência' no Databricks é projetado para suportar múltiplos usuários simultâneos, oferecendo isolamento e segurança.\n\nAs outras opções não definem corretamente um cluster de alta concorrência."
  },
  {
    "question": "In Databricks SQL, which command is used to display the list of columns and their data types for a specific table named 'customer_data'?",
    "options": [
      "SHOW COLUMNS FROM customer_data;",
      "DESCRIBE TABLE customer_data;",
      "LIST COLUMNS IN customer_data;",
      "SELECT * FROM customer_data;",
      "DISPLAY SCHEMA OF customer_data;"
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Option B is correct because the `DESCRIBE TABLE customer_data;` command provides the schema information, listing all columns and their data types.\n\nOption A is incorrect because `SHOW COLUMNS FROM` may not display data types in all SQL dialects.\n\nOption C is incorrect because `LIST COLUMNS IN` is not standard SQL syntax.\n\nOption D is incorrect because `SELECT *` retrieves data, not schema information.\n\nOption E is incorrect because `DISPLAY SCHEMA OF` is not a valid SQL command."
  },
  {
    "question": "What is the primary purpose of the Databricks Workspace in the Lakehouse architecture?",
    "options": [
      "To store raw data in its original format.",
      "To provide a collaborative environment for data engineers, data scientists, and analysts to develop and manage data and AI applications.",
      "To serve as a data visualization tool for creating dashboards.",
      "To act as a data warehouse for structured data storage.",
      "To manage user authentication and access control only."
    ],
    "answer": 1,
    "category": "Databricks Architecture",
    "explanation": "Option B is correct because the Databricks Workspace is designed to be a collaborative environment where different roles can work together on data engineering, data science, and analytics tasks.\n\nOption A is incorrect because raw data storage is typically handled by the data lake storage layer.\n\nOption C is incorrect because while Databricks does offer visualization capabilities, the primary purpose of the Workspace is broader.\n\nOption D is incorrect because the data warehouse functionality is part of the Lakehouse but not the primary purpose of the Workspace.\n\nOption E is incorrect because while access control is part of the Workspace, it is not its sole purpose."
  },
  {
    "question": "What is the main advantage of using 'broadcast join' in Spark?",
    "options": [
      "It allows joining large DataFrames efficiently.",
      "It sends a copy of a small DataFrame to all executor nodes to optimize join performance.",
      "It automatically repartitions data before joining.",
      "It caches data in memory to speed up processing.",
      "It avoids the need for shuffling data across the network."
    ],
    "answer": 1,
    "category": "Spark Optimization",
    "explanation": "Option B is correct because a broadcast join sends a small DataFrame to all executor nodes, allowing each node to join with the local partition of the large DataFrame without shuffling the large DataFrame.\n\nOption A is partially correct but not specific.\n\nOption C is incorrect.\n\nOption D is about caching.\n\nOption E is incorrect; shuffling is still involved but minimized."
  },
  {
    "question": "In Databricks, which API allows you to interact with object storage directly from a notebook?",
    "options": [
      "dbutils.fs",
      "dbutils.secrets",
      "dbutils.widgets",
      "dbutils.jobs",
      "dbutils.data"
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because `dbutils.fs` provides filesystem commands to interact with object storage directly from a Databricks notebook.\n\nOptions B, C, D, and E are incorrect; they serve different purposes within Databricks."
  },
  {
    "question": "In data processing, what is 'schema evolution'?",
    "options": [
      "The process of encrypting data schemas for security.",
      "Automatically adapting to changes in data structure over time without breaking applications.",
      "Normalizing data to eliminate redundancy.",
      "Transforming data from one format to another.",
      "Version controlling code changes in database schemas."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because schema evolution refers to the capability of data systems to handle changes in the data schema over time."
  },
  {
    "question": "What is 'data locality' in the context of distributed computing?",
    "options": [
      "Storing all data in a central location.",
      "Processing data on or near the node where it is stored to reduce network traffic.",
      "Encrypting data to secure it during processing.",
      "Distributing data evenly across all nodes regardless of location.",
      "Backing up data to multiple geographical locations."
    ],
    "answer": 1,
    "category": "Distributed Systems",
    "explanation": "Option B is correct because data locality refers to processing data close to where it is stored to minimize data movement and network congestion.\n\nOptions A, C, D, and E are different concepts."
  },
  {
    "question": "In Spark, what is the 'Catalyst Optimizer'?",
    "options": [
      "A tool for managing cluster resources.",
      "A query optimization engine in Spark SQL.",
      "A library for machine learning algorithms.",
      "A function for data serialization.",
      "An interface for streaming data processing."
    ],
    "answer": 1,
    "category": "Spark SQL",
    "explanation": "Option B is correct because the Catalyst Optimizer is Spark SQL's query optimization engine that automatically optimizes queries for better performance."
  },
  {
    "question": "In data analysis, what is 'data wrangling'?",
    "options": [
      "The process of collecting raw data.",
      "The initial exploration of data to discover patterns.",
      "The process of cleaning and unifying complex data sets for easy access and analysis.",
      "The deployment of machine learning models.",
      "The process of storing data securely."
    ],
    "answer": 2,
    "category": "Data Preprocessing",
    "explanation": "Option C is correct because data wrangling involves cleaning, transforming, and mapping raw data into a more usable format for analysis."
  },
  {
    "question": "In PySpark, which method is used to write a DataFrame 'df' to a table named 'users' in a database?",
    "options": [
      "df.write.saveAsTable('users')",
      "df.saveTable('users')",
      "df.write.table('users')",
      "df.toTable('users')",
      "df.write.save('users')"
    ],
    "answer": 0,
    "category": "Data Export",
    "explanation": "Option A is correct because `df.write.saveAsTable('users')` writes the DataFrame to a table named 'users'."
  },
  {
    "question": "In SQL, which function calculates the number of items in a group, ignoring NULL values?",
    "options": [
      "SUM()",
      "COUNT(*)",
      "COUNT(column_name)",
      "AVG()",
      "MAX()"
    ],
    "answer": 2,
    "category": "SQL Functions",
    "explanation": "Option C is correct because 'COUNT(column_name)' counts the number of non-NULL values in that column.\n\nOption B counts all rows, including those with NULLs."
  },
  {
    "question": "What is 'data democratization' in the context of data analytics?",
    "options": [
      "Restricting data access to top management.",
      "Making data accessible to all users in an organization without barriers.",
      "Encrypting data to prevent unauthorized access.",
      "Centralizing data storage in a single location.",
      "Deleting redundant data to save storage space."
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "Option B is correct because data democratization refers to making data accessible to all users within an organization, enabling them to make data-driven decisions without barriers.\n\nOptions A, C, D, and E do not define data democratization."
  },
  {
    "question": "In SQL, which function returns the smallest value in a specified column?",
    "options": [
      "MAX()",
      "SUM()",
      "MIN()",
      "AVG()",
      "COUNT()"
    ],
    "answer": 2,
    "category": "SQL Functions",
    "explanation": "Option C is correct because `MIN()` returns the smallest value in a specified column.\n\nOption A returns the largest value.\n\nOption B calculates the total.\n\nOption D calculates the average.\n\nOption E counts the number of rows."
  },
  {
    "question": "In PySpark, which method is used to remove duplicates based on specific columns 'col1' and 'col2' in DataFrame 'df'?",
    "options": [
      "df.dropDuplicates(['col1', 'col2'])",
      "df.removeDuplicates(['col1', 'col2'])",
      "df.deleteDuplicates(['col1', 'col2'])",
      "df.distinct(['col1', 'col2'])",
      "df.drop(['col1', 'col2'])"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because 'df.dropDuplicates(['col1', 'col2'])' removes duplicate rows based on the specified columns."
  },
  {
    "question": "In PySpark, how do you rename a column 'old_name' to 'new_name' in DataFrame 'df'?",
    "options": [
      "df.withColumnRenamed('old_name', 'new_name')",
      "df.renameColumn('old_name', 'new_name')",
      "df.column('old_name').alias('new_name')",
      "df.rename('old_name', 'new_name')",
      "df.withColumn('new_name', df['old_name'])"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `withColumnRenamed('old_name', 'new_name')` renames a column.\n\nOption B is not a valid method.\n\nOption C aliases the column in a select statement.\n\nOption D is not valid.\n\nOption E adds a new column but doesn't remove the old one."
  },
  {
    "question": "In SQL, what is the difference between 'INNER JOIN' and 'FULL OUTER JOIN'?",
    "options": [
      "'INNER JOIN' returns matched records; 'FULL OUTER JOIN' returns all records when there is a match in either left or right table.",
      "'INNER JOIN' returns all records from both tables; 'FULL OUTER JOIN' returns only matched records.",
      "'INNER JOIN' returns unmatched records; 'FULL OUTER JOIN' returns matched records only.",
      "There is no difference between 'INNER JOIN' and 'FULL OUTER JOIN'.",
      "'INNER JOIN' returns all records from the left table; 'FULL OUTER JOIN' returns all records from the right table."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because 'INNER JOIN' returns records that have matching values in both tables, while 'FULL OUTER JOIN' returns all records when there is a match in either left or right table, filling with NULLs where there is no match."
  },
  {
    "question": "What is 'data normalization' in data preprocessing?",
    "options": [
      "Converting categorical data into numerical form.",
      "Scaling numerical data to a common range without distorting differences in the ranges of values.",
      "Detecting and correcting errors in data.",
      "Organizing data into a database according to certain rules.",
      "Removing duplicate records from data."
    ],
    "answer": 1,
    "category": "Data Preprocessing",
    "explanation": "Option B is correct because data normalization scales numeric data to a common range, often between 0 and 1, which is important for certain algorithms."
  },
  {
    "question": "In machine learning, what is 'k-fold cross-validation' used for?",
    "options": [
      "To divide the dataset into k subsets and use one as validation while training on the others, rotating this process k times.",
      "To increase the size of the training dataset by a factor of k.",
      "To cluster data into k groups before training.",
      "To reduce the dimensionality of data to k features.",
      "To select the top k models based on performance."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because k-fold cross-validation involves partitioning the data into k subsets, training the model k times, each time using a different subset as the validation set."
  },
  {
    "question": "In Databricks, what is a 'notebook'?",
    "options": [
      "A document that contains executable code, visualizations, and narrative text.",
      "A cluster configuration file.",
      "A dataset stored in the cloud.",
      "A log file for monitoring applications.",
      "A user profile setting."
    ],
    "answer": 0,
    "category": "Databricks Notebooks",
    "explanation": "Option A is correct because a notebook in Databricks is an interactive document where users can write code, visualize data, and add explanatory text."
  },
  {
    "question": "In PySpark, how do you select only the 'name' and 'age' columns from DataFrame 'df'?",
    "options": [
      "df.select('name', 'age')",
      "df['name', 'age']",
      "df.selectColumns('name', 'age')",
      "df.getColumns('name', 'age')",
      "df.pick('name', 'age')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.select('name', 'age')` selects the specified columns from the DataFrame.\n\nOption B would cause an error.\n\nOptions C, D, and E are not valid PySpark methods."
  },
  {
    "question": "In SQL, what function would you use to return the first non-null value in a list?",
    "options": [
      "COALESCE",
      "NVL",
      "IFNULL",
      "ISNULL",
      "NULLIF"
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because `COALESCE` returns the first non-null value in a list of expressions.\n\nOptions B, C, and D are database-specific functions.\n\nOption E returns null if two expressions are equal."
  },
  {
    "question": "In SQL, which keyword is used to sort the result set of a query?",
    "options": [
      "GROUP BY",
      "ORDER BY",
      "SORT BY",
      "HAVING",
      "FILTER BY"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'ORDER BY' is used to sort the result set in ascending or descending order based on one or more columns."
  },
  {
    "question": "In PySpark, which method is used to replace null values in a DataFrame column 'age' with the value 0?",
    "options": [
      "df.na.fill({'age': 0})",
      "df.fillna({'age': 0})",
      "df.na.replace('age', 0)",
      "df.replaceNull('age', 0)",
      "df.fillNull('age', 0)"
    ],
    "answer": [0, 1],
    "category": "Data Transformation",
    "explanation": "Options A and B are correct because both `df.na.fill({'age': 0})` and `df.fillna({'age': 0})` replace null values in the 'age' column with 0.\n\nOption C is incorrect syntax for replacing nulls.\n\nOptions D and E are not valid PySpark methods."
  },
  {
    "question": "In Databricks SQL, which of the following commands is used to remove old versions of data files that are no longer needed to save storage space?",
    "options": [
      "CLEAN TABLE my_table;",
      "VACUUM my_table;",
      "PURGE my_table;",
      "TRUNCATE HISTORY ON my_table;",
      "DELETE OLD VERSIONS FROM my_table;"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because the `VACUUM` command in Delta Lake is used to remove files that are no longer referenced by a Delta table, helping to save storage space.\n\nOption A is incorrect because `CLEAN TABLE` is not a valid command in this context.\n\nOption C is incorrect because `PURGE` is not the correct command for this operation in Databricks SQL.\n\nOption D is incorrect because `TRUNCATE HISTORY` is not a valid command in Delta Lake.\n\nOption E is incorrect because `DELETE OLD VERSIONS` is not a recognized command."
  },
  {
    "question": "Which visualization is best suited for comparing the proportions of categories within a dataset?",
    "options": [
      "Line Chart",
      "Scatter Plot",
      "Pie Chart",
      "Histogram",
      "Box Plot"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a pie chart is designed to show the proportions of categories within a whole dataset.\n\nOption A is for trends over time.\n\nOption B shows relationships between two numerical variables.\n\nOption D shows the distribution of a single variable.\n\nOption E displays distribution based on quartiles."
  },
  {
    "question": "In SQL, which command is used to change the data type of a column in an existing table?",
    "options": [
      "MODIFY TABLE",
      "ALTER COLUMN",
      "CHANGE COLUMN",
      "ALTER TABLE ... MODIFY COLUMN",
      "UPDATE COLUMN"
    ],
    "answer": 3,
    "category": "SQL",
    "explanation": "Option D is correct because the `ALTER TABLE ... MODIFY COLUMN` command is used to change a column's data type in many SQL dialects.\n\nOptions A, B, and C are not standard or may vary by SQL implementation.\n\nOption E is not used for changing data types."
  },
  {
    "question": "Which of the following is true about Delta Lake's 'Time Travel' feature?",
    "options": [
      "It allows users to query older snapshots of a Delta Lake table using a version number or timestamp.",
      "It automatically backs up data to a remote location for disaster recovery.",
      "It enables real-time streaming of data into the Delta Lake table.",
      "It compresses historical data to save storage space.",
      "It provides real-time alerts when data is updated."
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because Delta Lake's Time Travel feature allows users to access previous versions of the table using version numbers or timestamps.\n\nOption B is incorrect because Time Travel does not back up data to a remote location.\n\nOption C is incorrect because streaming data ingestion is handled differently.\n\nOption D is incorrect because Time Travel does not compress historical data.\n\nOption E is incorrect because Time Travel does not provide real-time alerts."
  },
  {
    "question": "In data analysis, what is 'clustering'?",
    "options": [
      "A supervised learning technique for classification.",
      "A method of reducing the number of features in a dataset.",
      "An unsupervised learning technique for grouping similar data points.",
      "A way to normalize data distributions.",
      "A process of cleaning data by removing outliers."
    ],
    "answer": 2,
    "category": "Machine Learning Concepts",
    "explanation": "Option C is correct because clustering is an unsupervised learning method used to group similar data points based on their features."
  },
  {
    "question": "What is the main advantage of using 'Apache Arrow' in PySpark for data transfer?",
    "options": [
      "It provides built-in machine learning algorithms.",
      "It speeds up data transfer between JVM and Python processes by using a shared memory format.",
      "It compresses data to save storage space.",
      "It ensures data encryption during transfer.",
      "It allows for real-time streaming of data."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because Apache Arrow optimizes data transfer between processes by using a columnar memory format that reduces serialization overhead.\n\nOptions A, C, D, and E are not primary advantages of Apache Arrow in this context."
  },
  {
    "question": "In PySpark, which method is used to add a new column to a DataFrame by applying a function to existing columns?",
    "options": [
      "withColumn()",
      "addColumn()",
      "insertColumn()",
      "appendColumn()",
      "modifyColumn()"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `withColumn()` is used to add or replace a column in a DataFrame.\n\nOptions B, C, D, and E are not valid PySpark DataFrame methods for this purpose."
  },
  {
    "question": "In Databricks SQL, which statement is used to modify the structure of an existing table?",
    "options": [
      "CHANGE TABLE",
      "UPDATE TABLE",
      "MODIFY TABLE",
      "ALTER TABLE",
      "RENAME TABLE"
    ],
    "answer": 3,
    "category": "Databricks SQL",
    "explanation": "Option D is correct because `ALTER TABLE` is used to modify the structure of an existing table.\n\nOptions A, B, and C are not valid SQL statements for this purpose.\n\nOption E is used to rename a table."
  },
  {
    "question": "Which of the following statements is true about the 'JOIN' operations in SQL?",
    "options": [
      "An INNER JOIN returns all records from both tables, regardless of matching keys.",
      "A LEFT JOIN returns all records from the left table and matched records from the right table.",
      "A FULL OUTER JOIN returns only the records that have matching keys in both tables.",
      "A RIGHT JOIN returns all records from the right table only.",
      "JOIN operations cannot be performed on more than two tables."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because a LEFT JOIN returns all records from the left table and the matched records from the right table; unmatched records from the right table will be null.\n\nOption A describes a FULL OUTER JOIN incorrectly.\n\nOption C describes an INNER JOIN incorrectly.\n\nOption D is incorrect; a RIGHT JOIN returns all records from the right table and matched records from the left.\n\nOption E is incorrect; JOIN operations can be performed on multiple tables."
  },
  {
    "question": "In Spark SQL, what does the 'EXPLAIN' command do when used before a SELECT statement?",
    "options": [
      "It executes the query and displays the results.",
      "It shows the logical and physical execution plan of the query.",
      "It checks the syntax of the query without executing it.",
      "It optimizes the query for better performance.",
      "It converts the query into a stored procedure."
    ],
    "answer": 1,
    "category": "SQL Optimization",
    "explanation": "Option B is correct because `EXPLAIN` provides the execution plan, showing how Spark will execute the query.\n\nOption A is incorrect; it does not execute the query.\n\nOption C is incorrect; it does more than syntax checking.\n\nOption D is incorrect; it does not optimize the query.\n\nOption E is incorrect; it does not create stored procedures."
  },
  {
    "question": "In Spark Streaming, what does the 'window' operation allow you to do?",
    "options": [
      "Process data in micro-batches.",
      "Group data from a DStream over a sliding interval.",
      "Filter out duplicate records.",
      "Join two DStreams based on a key.",
      "Persist data to disk storage."
    ],
    "answer": 1,
    "category": "Streaming Data",
    "explanation": "Option B is correct because window operations in Spark Streaming allow you to apply transformations over a sliding window of data from a DStream."
  },
  {
    "question": "In Databricks SQL, how do you convert a string '2021-12-31' to a DATE data type?",
    "options": [
      "CAST('2021-12-31' AS DATE)",
      "CONVERT(DATE, '2021-12-31')",
      "TO_DATE('2021-12-31')",
      "PARSE_DATE('2021-12-31')",
      "FORMAT_DATE('2021-12-31')"
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because using `CAST('2021-12-31' AS DATE)` converts the string to a DATE data type.\n\nOption B is SQL Server syntax.\n\nOption C is a common function in some SQL dialects but not standard.\n\nOption D and E are not standard SQL functions for type conversion."
  },
  {
    "question": "When using Databricks SQL, how can you schedule a query to run automatically at regular intervals?",
    "options": [
      "By configuring a job in the Jobs tab and setting a schedule.",
      "By adding a REFRESH SCHEDULE command at the end of the query.",
      "By setting up a cron job on your local machine to execute the query.",
      "By enabling auto-refresh in the query editor.",
      "Scheduling is not possible in Databricks SQL."
    ],
    "answer": 0,
    "category": "Query Scheduling",
    "explanation": "Option A is correct because Databricks allows you to schedule queries by creating a job in the Jobs tab and setting up the schedule.\n\nOption B is incorrect because `REFRESH SCHEDULE` is not a valid SQL command.\n\nOption C is impractical and not integrated with Databricks.\n\nOption D is incorrect because auto-refresh in the query editor is not used for scheduling query execution.\n\nOption E is incorrect because scheduling is possible."
  },
  {
    "question": "In the context of data privacy, what does PII stand for, and why is it important?",
    "options": [
      "Personal Information Index; important for data backup.",
      "Publicly Indexed Information; important for marketing.",
      "Personally Identifiable Information; important for compliance with privacy laws.",
      "Personal Internet Information; important for network security.",
      "Protected Internal Insights; important for business strategy."
    ],
    "answer": 2,
    "category": "Data Privacy",
    "explanation": "Option C is correct because PII stands for Personally Identifiable Information, which includes any data that could potentially identify a specific individual, making it critical for compliance with privacy laws.\n\nOther options do not correctly define PII."
  },
  {
    "question": "Which Spark SQL function would you use to remove duplicate rows from a DataFrame?",
    "options": [
      "dropDuplicates()",
      "distinct()",
      "unique()",
      "dropna()",
      "filterDuplicates()"
    ],
    "answer": [0, 1],
    "category": "Data Transformation",
    "explanation": "Options A and B are correct because both `dropDuplicates()` and `distinct()` can be used to remove duplicate rows from a DataFrame.\n\nOption C is incorrect because `unique()` is not a standard method in Spark DataFrames.\n\nOption D is incorrect because `dropna()` removes rows with null values, not duplicates.\n\nOption E is incorrect because `filterDuplicates()` is not a standard Spark method."
  },
  {
    "question": "When performing data ingestion into Databricks, which file format is columnar and supports efficient compression and encoding schemes?",
    "options": [
      "CSV",
      "JSON",
      "Parquet",
      "TXT",
      "XML"
    ],
    "answer": 2,
    "category": "Data Import",
    "explanation": "Option C is correct because Parquet is a columnar storage file format that supports efficient compression and encoding.\n\nOptions A, B, D, and E are row-based formats and do not offer the same efficiency for large-scale analytics."
  },
  {
    "question": "In machine learning, what is 'feature scaling'?",
    "options": [
      "Selecting the most important features for a model.",
      "Reducing the number of features in a dataset.",
      "Adjusting the scale of features to a common range.",
      "Transforming categorical variables into numerical variables.",
      "Creating new features from existing ones."
    ],
    "answer": 2,
    "category": "Data Preprocessing",
    "explanation": "Option C is correct because feature scaling involves adjusting the scale of features so they contribute equally to the result, often using normalization or standardization."
  },
  {
    "question": "In Databricks, what is the purpose of the 'Jobs UI'?",
    "options": [
      "To write and execute code in notebooks.",
      "To manage and monitor scheduled jobs.",
      "To configure cluster hardware settings.",
      "To set up user access permissions.",
      "To visualize data using built-in tools."
    ],
    "answer": 1,
    "category": "Databricks Features",
    "explanation": "Option B is correct because the Jobs UI in Databricks is used to create, manage, and monitor scheduled jobs.\n\nOptions A, C, D, and E describe other features in Databricks."
  },
  {
    "question": "In Databricks, what is the effect of the 'DROP TABLE' command when used on a managed Delta Lake table?",
    "options": [
      "It deletes the table metadata but leaves the data files intact.",
      "It deletes both the table metadata and the data files.",
      "It renames the table to 'deleted_table'.",
      "It archives the table data for recovery.",
      "It returns an error because 'DROP TABLE' cannot be used on Delta Lake tables."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because dropping a managed table in Delta Lake removes both the metadata and the data files.\n\nOption A is incorrect because unmanaged tables behave that way, not managed tables.\n\nOption C is incorrect; the table is not renamed.\n\nOption D is incorrect; data is not archived unless specifically configured.\n\nOption E is incorrect; 'DROP TABLE' can be used on Delta Lake tables."
  },
  {
    "question": "In machine learning, what is 'overfitting'?",
    "options": [
      "When a model performs well on training data but poorly on unseen data.",
      "When a model is too simple to capture underlying patterns.",
      "When a model has a high bias and low variance.",
      "When a model's performance improves with more features.",
      "When a model underutilizes the available data."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because overfitting occurs when a model learns the training data too well, including noise, and fails to generalize to new data."
  },
  {
    "question": "In PySpark, which method allows you to read a CSV file located at '/data/sales.csv' into a DataFrame with headers included?",
    "options": [
      "df = spark.read.csv('/data/sales.csv', header=True)",
      "df = spark.read.csv('/data/sales.csv', headers=True)",
      "df = spark.read.csv('/data/sales.csv', includeHeaders=True)",
      "df = spark.read.format('csv').load('/data/sales.csv', header=True)",
      "df = spark.read.csv('/data/sales.csv', withHeader=True)"
    ],
    "answer": 0,
    "category": "Data Import",
    "explanation": "Option A is correct because setting `header=True` tells Spark to use the first line as column names.\n\nOption B is incorrect; the parameter is `header`, not `headers`.\n\nOption C and E use incorrect parameter names.\n\nOption D is partially correct but typically you pass options differently when using `format` and `load`."
  },
  {
    "question": "In SQL, what does the 'ROW_NUMBER()' window function do?",
    "options": [
      "Assigns a unique sequential integer to rows within a partition.",
      "Calculates the average value over a window of rows.",
      "Ranks rows based on the value of a specified column with possible ties.",
      "Calculates the cumulative sum over a partition.",
      "Returns the number of rows in a table."
    ],
    "answer": 0,
    "category": "SQL Window Functions",
    "explanation": "Option A is correct because 'ROW_NUMBER()' assigns a unique sequential integer to rows within a partition of a result set."
  },
  {
    "question": "In Databricks SQL, how do you create a global temporary view named 'global_temp_view' from a table 'sales_data'?",
    "options": [
      "CREATE TEMP VIEW global_temp_view AS SELECT * FROM sales_data;",
      "CREATE GLOBAL TEMPORARY VIEW global_temp_view AS SELECT * FROM sales_data;",
      "CREATE VIEW global_temp_view AS SELECT * FROM sales_data;",
      "CREATE TEMPORARY VIEW global_temp_view AS SELECT * FROM sales_data;",
      "CREATE GLOBAL VIEW global_temp_view AS SELECT * FROM sales_data;"
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Option B is correct because `CREATE GLOBAL TEMPORARY VIEW` creates a view that is available across all sessions and notebooks within the same Spark application.\n\nOptions A and D create session-scoped temporary views.\n\nOption C creates a persistent view.\n\nOption E is not valid syntax in Databricks SQL."
  },
  {
    "question": "Which of the following best describes the term 'data governance'?",
    "options": [
      "Technical process of moving data from one system to another.",
      "Policies and processes that ensure high data quality and data management.",
      "The physical storage of data in databases and data warehouses.",
      "The use of data visualization tools to present data insights.",
      "The encryption of data to protect it from unauthorized access."
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "Option B is correct because data governance refers to the overall management, policies, and processes to ensure data quality, availability, integrity, and security.\n\nOptions A, C, D, and E refer to specific technical aspects but not the broader concept of data governance."
  },
  {
    "question": "In a Databricks job, what is the purpose of setting up 'Retry' configurations?",
    "options": [
      "To automatically restart the cluster if it fails.",
      "To rerun failed tasks within a job without manual intervention.",
      "To test the job multiple times before final execution.",
      "To schedule the job to run multiple times a day.",
      "To replicate the job's output to multiple destinations."
    ],
    "answer": 1,
    "category": "Job Management",
    "explanation": "Option B is correct because 'Retry' settings allow Databricks to automatically retry failed tasks in a job, increasing reliability.\n\nOption A is incorrect; cluster restarts are handled differently.\n\nOption C is incorrect; retries are not for testing.\n\nOption D is incorrect; scheduling is set elsewhere.\n\nOption E is incorrect; retries do not replicate output."
  },
  {
    "question": "In SQL, what is the 'TRIGGER' used for?",
    "options": [
      "To enforce data integrity by automatically updating data in response to certain events.",
      "To schedule routine database backups.",
      "To improve query performance with indexes.",
      "To define user roles and permissions.",
      "To create temporary tables during sessions."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because a TRIGGER is a database object that is automatically executed or fired when certain events occur, such as INSERT, UPDATE, or DELETE."
  },
  {
    "question": "In Databricks, what is the function of the 'dbutils.widgets' module?",
    "options": [
      "To create interactive input widgets in notebooks.",
      "To manage files in the Databricks File System.",
      "To handle secret management for credentials.",
      "To monitor and manage jobs.",
      "To interact with cluster configurations."
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because `dbutils.widgets` allows you to create interactive widgets in Databricks notebooks for parameters like dropdowns and text inputs.\n\nOptions B, C, D, and E are handled by other modules or interfaces."
  },
  {
    "question": "In machine learning, what is 'hyperparameter tuning'?",
    "options": [
      "Adjusting the parameters learned during training.",
      "Optimizing the configuration settings that guide the learning process.",
      "Scaling the input features to a standard range.",
      "Reducing the dimensionality of data.",
      "Validating the model on test data."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because hyperparameter tuning involves finding the best configuration settings (hyperparameters) that guide the learning process but are not learned from the data."
  },
  {
    "question": "What is the primary function of the 'collect()' action in Spark?",
    "options": [
      "To aggregate data across partitions.",
      "To bring all the data from executors to the driver node.",
      "To persist data in memory.",
      "To distribute data evenly across partitions.",
      "To write data to external storage."
    ],
    "answer": 1,
    "category": "Spark Actions",
    "explanation": "Option B is correct because `collect()` retrieves all the elements of the RDD or DataFrame to the driver node.\n\nOption A describes aggregation functions.\n\nOption C is about caching or persisting data.\n\nOption D is achieved with `repartition()` or `coalesce()`.\n\nOption E is done using `save` or `write` functions."
  },
  {
    "question": "In PySpark, which method would you use to apply a function to each element of a DataFrame column?",
    "options": [
      "map()",
      "apply()",
      "withColumn()",
      "transform()",
      "foreach()"
    ],
    "answer": 2,
    "category": "Data Transformation",
    "explanation": "Option C is correct because `withColumn()` allows you to create a new column or replace an existing one by applying a function to a column.\n\nOption A (`map()`) is used on RDDs, not DataFrames.\n\nOption B (`apply()`) is not a standard PySpark DataFrame method.\n\nOption D is used for array columns.\n\nOption E (`foreach()`) is used for actions, not transformations."
  },
  {
    "question": "No contexto de Data Lakes, o que significa o termo 'esquema sob leitura'?",
    "options": [
      "Definir o esquema dos dados antes da ingestão.",
      "Aplicar o esquema aos dados somente no momento da leitura.",
      "Transformar dados estruturados em não estruturados.",
      "Enforçar o esquema durante a escrita dos dados.",
      "Ignorar o esquema completamente em todas as etapas."
    ],
    "answer": 1,
    "category": "Data Lakes",
    "explanation": "A opção B está correta porque 'esquema sob leitura' significa que o esquema é aplicado aos dados somente quando eles são lidos, permitindo flexibilidade na ingestão de dados variados.\n\nAs outras opções não descrevem corretamente o conceito."
  },
  {
    "question": "In SQL, what does the 'ALTER TABLE' statement do when used with 'ADD COLUMN'?",
    "options": [
      "It modifies an existing column.",
      "It deletes a column from the table.",
      "It adds a new column to the table.",
      "It renames the table.",
      "It changes the data type of a column."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `ALTER TABLE ... ADD COLUMN` adds a new column to an existing table."
  },
  {
    "question": "In PySpark, how can you stop a SparkSession named 'spark'?",
    "options": [
      "spark.stop()",
      "spark.end()",
      "spark.shutdown()",
      "spark.terminate()",
      "spark.close()"
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because `spark.stop()` is used to stop the SparkSession."
  },
  {
    "question": "What is the primary benefit of using 'Parquet' files in big data processing with Spark?",
    "options": [
      "Parquet files are human-readable, making debugging easier.",
      "Parquet is a row-based storage format that speeds up data ingestion.",
      "Parquet files support schema evolution without requiring changes.",
      "Parquet is a columnar storage format that provides efficient data compression and encoding schemes.",
      "Parquet files are the default format for all Spark DataFrames."
    ],
    "answer": 3,
    "category": "Data Storage Formats",
    "explanation": "Option D is correct because Parquet is a columnar storage format that provides efficient data compression and encoding, which optimizes performance in big data processing.\n\nOption A is incorrect; Parquet files are binary and not human-readable.\n\nOption B is incorrect; Parquet is columnar, not row-based.\n\nOption C is partially true but not the primary benefit.\n\nOption E is incorrect; Spark DataFrames can be in various formats."
  },
  {
    "question": "In Spark, what does the term 'lazy evaluation' mean?",
    "options": [
      "Transformations are executed immediately when called.",
      "Actions and transformations are executed in parallel.",
      "Transformations are not executed until an action is called.",
      "Data is cached in memory to speed up processing.",
      "The Spark engine automatically optimizes code at runtime."
    ],
    "answer": 2,
    "category": "Spark Concepts",
    "explanation": "Option C is correct because in Spark, transformations are lazily evaluated, meaning they are not executed until an action triggers the computation.\n\nOption A is incorrect; transformations are not executed immediately.\n\nOption B is partially correct but doesn't define lazy evaluation.\n\nOption D is about caching, not lazy evaluation.\n\nOption E refers to Spark's optimization but is not the definition of lazy evaluation."
  },
  {
    "question": "Which of the following is an advantage of using JSON files in data storage?",
    "options": [
      "They are optimized for high-speed read and write operations.",
      "They support schema evolution and data types efficiently.",
      "They are human-readable and support hierarchical data structures.",
      "They provide the best compression ratios for big data.",
      "They enforce a strict schema, reducing data inconsistency."
    ],
    "answer": 2,
    "category": "Data Storage Formats",
    "explanation": "Option C is correct because JSON files are human-readable and support nested, hierarchical data structures, making them flexible for storing complex data.\n\nOptions A, B, D, and E are not primary advantages of JSON files compared to other formats."
  },
  {
    "question": "In the context of data analysis, what does ETL stand for, and what is its primary purpose?",
    "options": [
      "Extract, Transform, Load; to move and reshape data from sources to destinations.",
      "Evaluate, Test, Learn; to improve machine learning models.",
      "Extract, Transfer, Link; to connect data between systems.",
      "Encrypt, Transmit, Log; to secure data during transfer.",
      "Estimate, Transform, Load; to approximate data before storage."
    ],
    "answer": 0,
    "category": "Data Integration",
    "explanation": "Option A is correct because ETL stands for Extract, Transform, Load, which involves extracting data from sources, transforming it to fit operational needs, and loading it into a destination system.\n\nOptions B, C, D, and E do not correctly define ETL."
  },
  {
    "question": "In SQL, what does the 'ALTER TABLE' command with 'ADD CONSTRAINT' do?",
    "options": [
      "Adds a new column to the table.",
      "Changes the data type of an existing column.",
      "Adds a rule to enforce data integrity, such as a primary key or foreign key.",
      "Removes an existing constraint from the table.",
      "Deletes data from the table based on a condition."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `ALTER TABLE ... ADD CONSTRAINT` adds a new constraint to enforce data integrity rules.\n\nOptions A and B are different uses of `ALTER TABLE`.\n\nOption D would use `DROP CONSTRAINT`.\n\nOption E is done with `DELETE`."
  },
  {
    "question": "Em Spark, qual é a diferença entre as operações 'map()' e 'flatMap()'?",
    "options": [
      "'map()' aplica uma função a cada elemento; 'flatMap()' faz o mesmo mas retorna apenas elementos únicos.",
      "'map()' pode alterar o número de elementos; 'flatMap()' sempre retorna o mesmo número de elementos.",
      "'map()' sempre retorna uma lista; 'flatMap()' retorna elementos individuais.",
      "'map()' aplica uma função a cada elemento; 'flatMap()' pode retornar múltiplos elementos para cada entrada, achatando o resultado.",
      "Não há diferença funcional entre 'map()' e 'flatMap()' em Spark."
    ],
    "answer": 3,
    "category": "Spark Transformações",
    "explanation": "A opção D está correta porque `map()` aplica uma função a cada elemento, enquanto `flatMap()` pode retornar múltiplos elementos para cada entrada e achata o resultado em uma única lista.\n\nAs outras opções não descrevem corretamente as diferenças."
  },
  {
    "question": "In SQL, what is the purpose of the 'INNER JOIN' clause?",
    "options": [
      "To return all rows from both tables, matching and non-matching.",
      "To return rows when there is a match in either left or right table.",
      "To return rows when there is a match in both tables.",
      "To return all rows from the left table, and matched rows from the right table.",
      "To combine rows from two tables without any condition."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because an 'INNER JOIN' returns rows when there is a match in both tables based on the specified condition."
  },
  {
    "question": "Which of the following best describes the 'CAP theorem' in distributed computing?",
    "options": [
      "It states that a distributed system can simultaneously provide consistency, availability, and partition tolerance.",
      "It asserts that it's impossible for a distributed data store to simultaneously provide more than two out of three guarantees: consistency, availability, and partition tolerance.",
      "It defines the best practices for capacity planning in cloud computing.",
      "It is a protocol for secure data transmission.",
      "It is a method for compressing data in distributed systems."
    ],
    "answer": 1,
    "category": "Distributed Systems",
    "explanation": "Option B is correct because the CAP theorem states that in the presence of a network partition, a distributed system can choose either consistency or availability, but not both.\n\nOption A is incorrect; it cannot provide all three simultaneously.\n\nOptions C, D, and E are unrelated."
  },
  {
    "question": "In PySpark, which method is used to join two DataFrames 'df1' and 'df2' on a common column 'id'?",
    "options": [
      "df1.join(df2, 'id')",
      "df1.merge(df2, on='id')",
      "df1.append(df2, on='id')",
      "df1.concat(df2, on='id')",
      "df1.combine(df2, 'id')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df1.join(df2, 'id')` joins the two DataFrames on the 'id' column.\n\nOption B is a pandas method.\n\nOptions C and D are used for combining data but not for joining on a key in PySpark.\n\nOption E is not a valid PySpark method."
  },
  {
    "question": "In SQL, which function calculates the average value of a numeric column?",
    "options": [
      "SUM()",
      "COUNT()",
      "AVG()",
      "MAX()",
      "MIN()"
    ],
    "answer": 2,
    "category": "SQL Functions",
    "explanation": "Option C is correct because `AVG()` computes the average of the values in a numeric column."
  },
  {
    "question": "Em aprendizagem de máquina, o que é 'regularização' e por que é importante?",
    "options": [
      "Um método para aumentar o tamanho do conjunto de dados.",
      "Uma técnica para reduzir o sobreajuste adicionando uma penalidade à função de custo.",
      "Uma forma de normalizar os dados de entrada.",
      "Um algoritmo de otimização para acelerar o treinamento.",
      "Uma técnica para lidar com dados faltantes."
    ],
    "answer": 1,
    "category": "Aprendizagem de Máquina",
    "explanation": "A opção B está correta porque a regularização adiciona uma penalidade à função de custo para reduzir a complexidade do modelo e evitar o sobreajuste.\n\nAs outras opções não descrevem corretamente a regularização."
  },
  {
    "question": "In Spark, what does the 'persist(StorageLevel.DISK_ONLY)' method do?",
    "options": [
      "Caches the RDD in memory only.",
      "Caches the RDD on disk only.",
      "Caches the RDD both in memory and disk.",
      "Removes the RDD from cache.",
      "Serializes the RDD for network transmission."
    ],
    "answer": 1,
    "category": "Spark Optimization",
    "explanation": "Option B is correct because 'persist(StorageLevel.DISK_ONLY)' stores the RDD partitions on disk, not in memory."
  },
  {
    "question": "What is 'overfitting' in machine learning models?",
    "options": [
      "When a model is too simple and fails to capture underlying patterns.",
      "When a model performs well on training data but poorly on new, unseen data.",
      "When a model has too few parameters to make accurate predictions.",
      "When a model's performance improves with more data.",
      "When a model has balanced bias and variance."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because overfitting occurs when a model learns the training data too well, including noise, and does not generalize well to new data."
  },
  {
    "question": "In PySpark, how do you filter rows in a DataFrame 'df' where the column 'age' is greater than 30?",
    "options": [
      "df.filter(df.age > 30)",
      "df.where(df.age > 30)",
      "df.select('*').where('age > 30')",
      "All of the above",
      "None of the above"
    ],
    "answer": 3,
    "category": "Data Transformation",
    "explanation": "Option D is correct because all the methods listed are valid ways to filter rows where 'age' is greater than 30 in PySpark.\n\nOption A uses `filter` with a condition.\n\nOption B uses `where`, which is an alias for `filter`.\n\nOption C uses `select` followed by `where` with a SQL expression."
  },
  {
    "question": "In PySpark, how do you drop a column named 'age' from DataFrame 'df'?",
    "options": [
      "df.drop('age')",
      "df.removeColumn('age')",
      "df.delete('age')",
      "df.dropColumn('age')",
      "df.withoutColumn('age')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.drop('age')` returns a new DataFrame without the 'age' column.\n\nOptions B, C, D, and E are not valid PySpark DataFrame methods."
  },
  {
    "question": "What does the 'GROUP BY' clause do in a SQL query?",
    "options": [
      "It sorts the result set based on one or more columns.",
      "It filters rows based on a condition.",
      "It groups rows that have the same values in specified columns into summary rows.",
      "It combines the result sets of two SELECT statements.",
      "It specifies the database to use for the query."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because the `GROUP BY` clause groups rows sharing a value in specified columns, allowing aggregate functions to be applied.\n\nOption A describes `ORDER BY`.\n\nOption B describes `WHERE`.\n\nOption D describes `UNION`.\n\nOption E is incorrect; selecting a database is done with `USE database_name;`."
  },
  {
    "question": "In Spark, what is the purpose of the 'mapPartitions' transformation?",
    "options": [
      "To apply a function to each element of the RDD individually.",
      "To apply a function to each partition of the RDD.",
      "To reduce the number of partitions.",
      "To cache the RDD in memory.",
      "To group elements based on a key."
    ],
    "answer": 1,
    "category": "Spark Transformations",
    "explanation": "Option B is correct because `mapPartitions` allows you to apply a function to each partition of the RDD, offering performance optimizations over `map`.\n\nOption A describes `map`.\n\nOption C is done with `coalesce` or `repartition`.\n\nOption D is done with `cache`.\n\nOption E is done with `groupByKey`."
  },
  {
    "question": "Which of the following Spark transformations is an action that triggers computation and returns a value to the driver program?",
    "options": [
      "map()",
      "filter()",
      "reduce()",
      "flatMap()",
      "groupBy()"
    ],
    "answer": 2,
    "category": "Spark Actions",
    "explanation": "Option C is correct because `reduce()` is an action that aggregates data and returns a value to the driver.\n\nOptions A, B, D, and E are transformations, not actions. Transformations are lazy and define a new RDD/DataFrame but do not compute until an action is called."
  },
  {
    "question": "In Spark SQL, how do you select distinct values from a column 'category' in a DataFrame 'df'?",
    "options": [
      "df.select('category').distinct()",
      "df.selectDistinct('category')",
      "df.distinct('category')",
      "df.select('category').unique()",
      "df.unique('category')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.select('category').distinct()` selects the 'category' column and returns distinct values.\n\nOptions B, C, D, and E are not valid methods in PySpark."
  },
  {
    "question": "Which of the following is NOT a valid Spark shuffle partitioning method?",
    "options": [
      "HashPartitioner",
      "RangePartitioner",
      "RoundRobinPartitioner",
      "CustomPartitioner",
      "BroadcastPartitioner"
    ],
    "answer": 4,
    "category": "Spark Concepts",
    "explanation": "Option E is correct because there is no `BroadcastPartitioner` in Spark.\n\nOptions A, B, C, and D are valid partitioners."
  },
  {
    "question": "In SQL, what does the 'EXPLAIN' command do?",
    "options": [
      "Executes a query and returns the results.",
      "Provides the execution plan for a query without executing it.",
      "Optimizes a query for better performance.",
      "Creates an index on a table.",
      "Displays the schema of a table."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'EXPLAIN' provides the execution plan for a query, which helps in understanding and optimizing query performance."
  },
  {
    "question": "In PySpark, what does the 'groupBy' method return?",
    "options": [
      "A DataFrame object.",
      "A RelationalGroupedDataset object.",
      "A list of grouped DataFrames.",
      "A key-value pair RDD.",
      "An aggregated DataFrame."
    ],
    "answer": 1,
    "category": "Data Transformation",
    "explanation": "Option B is correct because `groupBy` returns a `GroupedData` object (also known as `RelationalGroupedDataset`), which can be used to perform aggregate functions.\n\nOption A is incorrect; `groupBy` does not return a DataFrame directly.\n\nOption C is incorrect.\n\nOption D is related to RDDs, not DataFrames.\n\nOption E is the result after applying an aggregation."
  },
  {
    "question": "In Databricks, what is the 'Auto Loader' feature primarily used for?",
    "options": [
      "To automatically scale clusters based on workload.",
      "To incrementally and efficiently process new data files as they arrive in cloud storage.",
      "To load data into memory for faster processing.",
      "To automate the deployment of machine learning models.",
      "To schedule notebooks to run at specified intervals."
    ],
    "answer": 1,
    "category": "Data Ingestion",
    "explanation": "Option B is correct because Auto Loader is a feature in Databricks that incrementally processes new data files as they arrive in cloud storage, supporting efficient ingestion for streaming and batch workloads.\n\nOptions A, C, D, and E describe different functionalities."
  },
  {
    "question": "Which command in Databricks SQL allows you to view all tables within the current database?",
    "options": [
      "SHOW DATABASES;",
      "LIST TABLES;",
      "SHOW TABLES;",
      "DESCRIBE DATABASE;",
      "SELECT * FROM information_schema.tables;"
    ],
    "answer": 2,
    "category": "Databricks SQL",
    "explanation": "Option C is correct because `SHOW TABLES;` lists all tables in the current database.\n\nOption A is incorrect because it shows databases, not tables.\n\nOption B is incorrect because `LIST TABLES;` is not standard SQL syntax.\n\nOption D is incorrect because it describes the database but doesn't list tables.\n\nOption E is incorrect in Databricks SQL context as accessing `information_schema` may not be supported directly."
  },
  {
    "question": "In SQL, which clause is used to rename a column in the result set of a query?",
    "options": [
      "RENAME COLUMN",
      "AS",
      "MODIFY COLUMN",
      "CHANGE COLUMN",
      "ALIAS"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the 'AS' keyword is used to provide an alias for a column in the result set.\n\nOption A is not standard SQL for renaming in the result set.\n\nOptions C and D are used to alter table structures.\n\nOption E is a concept but not a SQL keyword."
  },
  {
    "question": "What does the 'persist' method do in Spark, and how is it different from 'cache'?",
    "options": [
      "Persist saves the DataFrame to disk; cache saves it to memory.",
      "Persist allows storage level specification; cache uses the default storage level (memory only).",
      "Persist is used for streaming data; cache is for batch data.",
      "Persist removes the DataFrame from memory; cache keeps it.",
      "There is no difference; they are interchangeable."
    ],
    "answer": 1,
    "category": "Spark Optimization",
    "explanation": "Option B is correct because `persist` allows you to specify the storage level (e.g., memory, disk), while `cache` is a shorthand for `persist` with the default storage level of memory only.\n\nOption A is not entirely accurate.\n\nOption C is incorrect; both can be used for streaming or batch.\n\nOption D is incorrect.\n\nOption E is incorrect; there is a difference in flexibility."
  },
  {
    "question": "In Databricks, what is the function of the 'Unity Catalog'?",
    "options": [
      "To provide a unified governance solution for all data and AI assets.",
      "To manage cluster configurations and performance.",
      "To develop machine learning models collaboratively.",
      "To offer a centralized dashboard for job monitoring.",
      "To store code repositories for version control."
    ],
    "answer": 0,
    "category": "Data Governance",
    "explanation": "Option A is correct because Unity Catalog is a unified governance solution in Databricks for managing and securing data and AI assets across clouds.\n\nOptions B, C, D, and E describe different features."
  },
  {
    "question": "In Databricks SQL, how do you create a new managed Delta table named 'customers' using data from an existing table 'raw_customers'?",
    "options": [
      "CREATE TABLE customers AS SELECT * FROM raw_customers;",
      "CREATE DELTA TABLE customers FROM raw_customers;",
      "SELECT * INTO customers FROM raw_customers;",
      "MAKE TABLE customers AS SELECT * FROM raw_customers;",
      "INSERT INTO customers SELECT * FROM raw_customers;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Option A is correct because 'CREATE TABLE customers AS SELECT * FROM raw_customers;' creates a new table with data from 'raw_customers'.\n\nOption B is not standard syntax.\n\nOption C syntax varies by SQL dialect.\n\nOption D is incorrect syntax.\n\nOption E assumes 'customers' already exists."
  },
  {
    "question": "In data analysis, what does 'EDA' stand for and what is its purpose?",
    "options": [
      "Enterprise Data Access; to manage user permissions.",
      "Exploratory Data Analysis; to summarize main characteristics of data.",
      "External Data Algorithm; to process data from external sources.",
      "Efficient Data Aggregation; to optimize query performance.",
      "Encoded Data Architecture; to structure data storage."
    ],
    "answer": 1,
    "category": "Data Analysis",
    "explanation": "Option B is correct because EDA stands for Exploratory Data Analysis, which involves analyzing data sets to summarize their main characteristics, often using visual methods."
  },
  {
    "question": "In data visualization, when would you choose to use a heatmap?",
    "options": [
      "To display the distribution of a single variable over time.",
      "To compare parts of a whole using percentages.",
      "To show the relationship between two categorical variables.",
      "To track changes in data across geographical locations.",
      "To display hierarchical data in a tree-like structure."
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because heatmaps are effective for showing relationships between two categorical variables, with color intensity representing the frequency or magnitude.\n\nOption A is better suited for line charts.\n\nOption B is for pie charts.\n\nOption D is for maps or choropleths.\n\nOption E is for tree maps."
  },
  {
    "question": "In machine learning, what is the 'Recall' metric?",
    "options": [
      "The ratio of true positives to all actual positives.",
      "The ratio of true negatives to all predicted negatives.",
      "The proportion of correct predictions over total predictions.",
      "The ability of the model to correctly predict negative classes.",
      "The ratio of false positives to true negatives."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because recall measures the proportion of actual positives that were correctly identified by the model."
  },
  {
    "question": "Which function in SQL would you use to convert a string to uppercase?",
    "options": [
      "UPPER(string)",
      "TO_UPPERCASE(string)",
      "MAKE_UPPER(string)",
      "CAPITALIZE(string)",
      "STRING_UPPER(string)"
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because `UPPER(string)` converts the input string to uppercase in SQL.\n\nOptions B, C, D, and E are not standard SQL functions."
  },
  {
    "question": "No Spark SQL, como você converteria uma coluna 'data_str' com datas em formato string para o tipo Date?",
    "options": [
      "df.withColumn('data', df.data_str.cast('date'))",
      "df.select(to_date(df.data_str))",
      "df.withColumn('data', df.data_str.toDate())",
      "df.transform('data_str', 'date')",
      "df.convert(df.data_str, 'date')"
    ],
    "answer": 0,
    "category": "Transformação de Dados",
    "explanation": "A opção A está correta porque você pode usar `cast('date')` para converter uma coluna string em Date.\n\nAs outras opções não são métodos padrão ou estão incorretas."
  },
  {
    "question": "In data visualization, when is a 'heatmap' most appropriately used?",
    "options": [
      "To display hierarchical data using nested rectangles.",
      "To show the frequency distribution of a single variable.",
      "To represent the correlation between two variables using colors.",
      "To compare proportions of categories within a whole.",
      "To plot individual data points on a two-dimensional plane."
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a heatmap uses color shading to represent values of variables, often used to show correlation between two variables.\n\nOption A describes a treemap.\n\nOption B is better represented by a histogram.\n\nOption D is for pie charts.\n\nOption E describes a scatter plot."
  },
  {
    "question": "No Databricks SQL, qual comando você usaria para exibir as funções disponíveis no ambiente atual?",
    "options": [
      "SHOW FUNCTIONS;",
      "LIST FUNCTIONS;",
      "DESCRIBE FUNCTIONS;",
      "DISPLAY FUNCTIONS;",
      "GET FUNCTIONS;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "A opção A está correta porque o comando `SHOW FUNCTIONS;` exibe a lista de funções disponíveis no ambiente SQL atual.\n\nAs opções B, C, D e E não são comandos SQL padrão para listar funções."
  },
  {
    "question": "What is 'sharding' in the context of databases?",
    "options": [
      "Encrypting data to secure it.",
      "Replicating data across multiple servers for redundancy.",
      "Partitioning data horizontally to distribute it across multiple databases.",
      "Backing up data to prevent loss.",
      "Indexing data to improve query performance."
    ],
    "answer": 2,
    "category": "Data Management",
    "explanation": "Option C is correct because sharding involves partitioning a database horizontally to spread data across multiple machines, improving scalability and performance.\n\nOptions A, B, D, and E describe different concepts."
  },
  {
    "question": "What is 'data lineage' in data governance?",
    "options": [
      "A record of data's origin and transformations over time.",
      "The security protocol for accessing data.",
      "The hierarchical structure of data storage.",
      "The process of cleaning and preparing data.",
      "The method of storing data in cloud environments."
    ],
    "answer": 0,
    "category": "Data Governance",
    "explanation": "Option A is correct because data lineage refers to the lifecycle of data, including its origins and transformations over time."
  },
  {
    "question": "In machine learning, what is 'regularization' used for?",
    "options": [
      "To increase the complexity of a model.",
      "To prevent overfitting by adding a penalty for larger weights.",
      "To reduce the size of the dataset.",
      "To improve the speed of training by simplifying calculations.",
      "To adjust the learning rate during training."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because regularization adds a penalty to the loss function for large weights, helping to prevent overfitting.\n\nOption A is incorrect; it reduces complexity.\n\nOption C is data reduction.\n\nOption D is not the primary purpose.\n\nOption E describes learning rate scheduling."
  },
  {
    "question": "In data warehousing, what is a 'snowflake schema'?",
    "options": [
      "A schema with denormalized dimension tables.",
      "A centralized fact table connected to multiple normalized dimension tables.",
      "A single table containing all data.",
      "A method for indexing data for faster retrieval.",
      "A schema used exclusively for time-series data."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because a snowflake schema is a variation of the star schema where dimension tables are normalized into multiple related tables."
  },
  {
    "question": "Which of the following is a common use case for the 'groupByKey' transformation in Spark?",
    "options": [
      "To reduce data across all keys in the dataset.",
      "To group all the values with the same key into a single sequence.",
      "To filter out keys based on a condition.",
      "To sort the dataset by key.",
      "To join two datasets based on keys."
    ],
    "answer": 1,
    "category": "Spark Transformations",
    "explanation": "Option B is correct because `groupByKey` groups all values associated with the same key.\n\nOption A is for `reduceByKey`.\n\nOption C is for `filter`.\n\nOption D is for `sortByKey`.\n\nOption E is for `join` operations."
  },
  {
    "question": "In Spark SQL, what does the 'CAST' function do?",
    "options": [
      "It changes the data type of a column to a specified type.",
      "It removes duplicates from a DataFrame.",
      "It splits a column into multiple columns.",
      "It filters rows based on a condition.",
      "It merges two DataFrames together."
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `CAST` is used to convert the data type of a column to another data type.\n\nOptions B, C, D, and E describe different operations."
  },
  {
    "question": "In SQL, what does the 'COALESCE' function do?",
    "options": [
      "Returns the first non-null value in a list of expressions.",
      "Combines two strings into one.",
      "Counts the number of non-null values.",
      "Calculates the average of a set of values.",
      "Splits a string into multiple parts."
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because `COALESCE` returns the first non-null value from the list of expressions provided."
  },
  {
    "question": "What is the role of the 'checkpointing' feature in Spark Streaming?",
    "options": [
      "To save intermediate results to optimize performance.",
      "To provide fault tolerance by saving state information to reliable storage.",
      "To distribute data evenly across partitions.",
      "To cache data in memory for faster processing.",
      "To secure data by encrypting it during processing."
    ],
    "answer": 1,
    "category": "Streaming Data",
    "explanation": "Option B is correct because checkpointing saves the state of the streaming application to reliable storage, enabling recovery in case of failures.\n\nOptions A, C, D, and E describe different functionalities."
  },
  {
    "question": "Which SQL function would you use to calculate the total number of rows in a table named 'orders'?",
    "options": [
      "SELECT COUNT(*) FROM orders;",
      "SELECT SUM(*) FROM orders;",
      "SELECT TOTAL(*) FROM orders;",
      "SELECT NUMBER(*) FROM orders;",
      "SELECT LENGTH(*) FROM orders;"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because `COUNT(*)` returns the total number of rows in the table.\n\nOptions B, C, D, and E are not appropriate functions for counting rows."
  },
  {
    "question": "Qual comando SQL você usaria para modificar o tipo de dados da coluna 'preco' na tabela 'produtos' para DECIMAL(10,2)?",
    "options": [
      "ALTER TABLE produtos MODIFY COLUMN preco DECIMAL(10,2);",
      "CHANGE TABLE produtos COLUMN preco TO DECIMAL(10,2);",
      "ALTER TABLE produtos ALTER COLUMN preco DECIMAL(10,2);",
      "UPDATE produtos SET preco TYPE DECIMAL(10,2);",
      "MODIFY TABLE produtos SET preco DECIMAL(10,2);"
    ],
    "answer": [0, 2],
    "category": "SQL",
    "explanation": "As opções A e C estão corretas porque a sintaxe para alterar o tipo de dados de uma coluna varia entre sistemas de banco de dados. A opção A (`ALTER TABLE produtos MODIFY COLUMN preco DECIMAL(10,2);`) é usada em sistemas como MySQL, enquanto a opção C (`ALTER TABLE produtos ALTER COLUMN preco DECIMAL(10,2);`) é utilizada em sistemas como SQL Server.\n\nA opção B não é sintaticamente correta.\n\nA opção D está incorreta; `UPDATE` é usado para alterar dados, não o esquema da tabela.\n\nA opção E não é uma sintaxe SQL válida."
  },
  {
    "question": "In Databricks, what is the function of the 'dbutils' library?",
    "options": [
      "To perform database optimization tasks.",
      "To provide utility functions for file system operations, widgets, and notebook tasks.",
      "To create visualizations in notebooks.",
      "To manage user permissions and authentication.",
      "To develop machine learning models."
    ],
    "answer": 1,
    "category": "Databricks Utilities",
    "explanation": "Option B is correct because `dbutils` provides utility functions in Databricks for various tasks such as file system operations, creating widgets, and controlling notebook execution.\n\nOptions A, C, D, and E are not primary functions of `dbutils`."
  },
  {
    "question": "In Spark, what does the 'collect()' action do when called on a DataFrame or RDD?",
    "options": [
      "It saves the DataFrame or RDD to disk.",
      "It returns all the elements of the DataFrame or RDD to the driver program as a list.",
      "It caches the DataFrame or RDD in memory.",
      "It applies a function to each element without returning any value.",
      "It partitions the DataFrame or RDD across the cluster."
    ],
    "answer": 1,
    "category": "Spark Actions",
    "explanation": "Option B is correct because `collect()` retrieves all the elements from the DataFrame or RDD to the driver as a list.\n\nOption A describes `save` operations.\n\nOption C describes `cache()`.\n\nOption D describes `foreach()`.\n\nOption E describes `repartition()` or `coalesce()`."
  },
  {
    "question": "What is 'data augmentation' in the context of machine learning?",
    "options": [
      "Adding more data by collecting additional samples.",
      "Enhancing the dataset by generating synthetic data to increase diversity.",
      "Reducing the size of the dataset for faster processing.",
      "Combining multiple datasets into one.",
      "Cleaning the dataset by removing outliers."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because data augmentation involves creating synthetic data based on existing data to increase the diversity and size of the training dataset, often used in image processing.\n\nOption A refers to data collection.\n\nOption C is the opposite.\n\nOption D is data merging.\n\nOption E is data cleaning."
  },
  {
    "question": "Which of the following statements about Delta Lake's 'Schema Enforcement' feature is true?",
    "options": [
      "It allows any data to be written to the table, regardless of schema.",
      "It prevents updates to the table schema once created.",
      "It ensures that data written to a table adheres to the table's schema, preventing corrupt data.",
      "It automatically modifies the table schema to match incoming data.",
      "It only applies to streaming data sources."
    ],
    "answer": 2,
    "category": "Delta Lake",
    "explanation": "Option C is correct because Schema Enforcement (also known as schema validation) ensures that incoming data matches the table schema, preventing the introduction of corrupt data.\n\nOption A is incorrect; schema enforcement restricts data that doesn't match the schema.\n\nOption B is incorrect; schema can be altered deliberately.\n\nOption D is incorrect; automatic schema merging must be enabled explicitly.\n\nOption E is incorrect; schema enforcement applies to both batch and streaming data."
  },
  {
    "question": "In data processing, what is 'data partitioning'?",
    "options": [
      "Dividing data into subsets to improve performance and manageability.",
      "Encrypting data for secure transmission.",
      "Backing up data to prevent loss.",
      "Compressing data to save storage space.",
      "Removing duplicate data from datasets."
    ],
    "answer": 0,
    "category": "Data Processing",
    "explanation": "Option A is correct because data partitioning involves dividing data into smaller, manageable pieces, which can improve performance and scalability."
  },
  {
    "question": "In PySpark, which method allows you to write a DataFrame 'df' as a partitioned table by the column 'year'?",
    "options": [
      "df.write.partitionBy('year').save('/path/to/table')",
      "df.write.byPartition('year').save('/path/to/table')",
      "df.saveAsPartitionedTable('/path/to/table', 'year')",
      "df.write.partition('year').save('/path/to/table')",
      "df.write.savePartitioned('/path/to/table', partition='year')"
    ],
    "answer": 0,
    "category": "Data Export",
    "explanation": "Option A is correct because 'df.write.partitionBy('year').save('/path/to/table')' writes the DataFrame partitioned by the 'year' column.\n\nOptions B, C, D, and E are not valid PySpark methods or syntax."
  },
  {
    "question": "In SQL, what does the 'INNER JOIN' clause do?",
    "options": [
      "Returns all rows from both tables, matching and non-matching.",
      "Returns rows when there is a match in either left or right table.",
      "Returns rows when there is a match in both tables.",
      "Returns all rows from the left table, and matched rows from the right table.",
      "Returns all rows from the right table, and matched rows from the left table."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `INNER JOIN` returns rows that have matching values in both tables."
  },
  {
    "question": "In Databricks SQL, which command would you use to display the current databases available in the environment?",
    "options": [
      "SHOW DATABASES;",
      "LIST ALL DATABASES;",
      "DISPLAY DATABASES;",
      "GET DATABASE LIST;",
      "SELECT DATABASES;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Option A is correct because `SHOW DATABASES;` is the standard SQL command to list all databases in the current environment.\n\nOption B is incorrect because `LIST ALL DATABASES;` is not standard SQL syntax.\n\nOption C is incorrect because `DISPLAY DATABASES;` is not a recognized SQL command.\n\nOption D is incorrect because `GET DATABASE LIST;` is not valid SQL syntax.\n\nOption E is incorrect because `SELECT DATABASES;` is not the correct way to retrieve a list of databases."
  },
  {
    "question": "In Databricks, what is 'MLflow' used for?",
    "options": [
      "Managing and deploying machine learning models.",
      "Visualizing data with interactive dashboards.",
      "Optimizing cluster performance.",
      "Storing large datasets.",
      "Encrypting data in transit."
    ],
    "answer": 0,
    "category": "Databricks Machine Learning",
    "explanation": "Option A is correct because MLflow is an open-source platform for managing the end-to-end machine learning lifecycle."
  },
  {
    "question": "Which of the following best defines 'data lake'?",
    "options": [
      "A centralized repository that allows you to store all your structured and unstructured data at any scale.",
      "A database optimized for online transaction processing.",
      "A repository for storing machine learning models.",
      "A visualization tool for big data.",
      "A methodology for data governance."
    ],
    "answer": 0,
    "category": "Data Management",
    "explanation": "Option A is correct because a data lake is a centralized repository that stores structured and unstructured data at any scale.\n\nOptions B, C, D, and E describe different concepts."
  },
  {
    "question": "In data processing, what is an 'ETL' pipeline?",
    "options": [
      "An automated process for evaluating, testing, and logging data.",
      "A process for extracting data, transforming it, and loading it into a target system.",
      "A pipeline for encrypting, transmitting, and logging sensitive data.",
      "A method for estimating, training, and learning in machine learning models.",
      "A real-time data streaming architecture."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because ETL stands for Extract, Transform, Load, which is a common process in data warehousing and processing.\n\nOptions A, C, D, and E describe different processes."
  },
  {
    "question": "What is the purpose of the 'OVER' clause in SQL window functions?",
    "options": [
      "To specify the partitions and ordering of data for the window function.",
      "To filter rows before aggregation.",
      "To join tables based on a condition.",
      "To group rows that have the same values.",
      "To limit the number of rows returned."
    ],
    "answer": 0,
    "category": "SQL Window Functions",
    "explanation": "Option A is correct because the `OVER` clause defines the partitioning and ordering of rows for window functions in SQL.\n\nOptions B, C, D, and E are not related to the `OVER` clause."
  },
  {
    "question": "In SQL, which command is used to remove a view named 'sales_view'?",
    "options": [
      "DROP TABLE sales_view;",
      "DELETE FROM sales_view;",
      "DROP VIEW sales_view;",
      "TRUNCATE VIEW sales_view;",
      "REMOVE VIEW sales_view;"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `DROP VIEW sales_view;` is the command used to remove a view from the database.\n\nOption A would attempt to drop a table, not a view.\n\nOption B deletes data from a table.\n\nOption D is not valid syntax for views.\n\nOption E is not a standard SQL command."
  },
  {
    "question": "In data visualization, which chart is best for showing the distribution of data over time?",
    "options": [
      "Pie Chart",
      "Line Chart",
      "Scatter Plot",
      "Bar Chart",
      "Histogram"
    ],
    "answer": 1,
    "category": "Data Visualization",
    "explanation": "Option B is correct because a line chart is ideal for displaying data trends over time.\n\nOption A shows proportions.\n\nOption C shows relationships between two variables.\n\nOption D can show data over time but is less effective for trends.\n\nOption E shows data distribution, not over time."
  },
  {
    "question": "In PySpark, which method can be used to join DataFrame 'df1' with DataFrame 'df2' on column 'id' using an inner join?",
    "options": [
      "df1.join(df2, 'id', 'inner')",
      "df1.innerJoin(df2, 'id')",
      "df1.merge(df2, on='id')",
      "df1.combine(df2, 'id', 'inner')",
      "df1.union(df2)"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because 'df1.join(df2, 'id', 'inner')' performs an inner join on the 'id' column.\n\nOptions B, C, D, and E are not valid methods for joining DataFrames in PySpark."
  },
  {
    "question": "What is the role of 'Delta Engine' in Databricks?",
    "options": [
      "To provide a query optimization layer for Delta Lake.",
      "To manage clusters and resources.",
      "To encrypt data at rest.",
      "To stream data in real-time.",
      "To version control notebooks."
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because Delta Engine is a query optimizer and execution engine for Delta Lake, enhancing performance for data analytics workloads."
  },
  {
    "question": "Which of the following is an advantage of using Delta Lake over traditional data lakes?",
    "options": [
      "Delta Lake does not require schema definition, allowing for flexible data ingestion.",
      "Delta Lake provides ACID transactions for reliable data operations.",
      "Delta Lake automatically visualizes data without additional tools.",
      "Delta Lake stores data in proprietary formats, enhancing security.",
      "Delta Lake is only compatible with structured data."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because Delta Lake supports ACID transactions, ensuring data reliability and consistency.\n\nOption A is incorrect; Delta Lake enforces schema, which is beneficial.\n\nOption C is incorrect; visualization requires separate tools.\n\nOption D is incorrect; Delta Lake uses open formats like Parquet.\n\nOption E is incorrect; Delta Lake can handle semi-structured data as well."
  },
  {
    "question": "In Spark, what is 'lazy evaluation'?",
    "options": [
      "A strategy where computations are executed immediately.",
      "A method of caching data in memory.",
      "A way to delay the execution of transformations until an action is called.",
      "An optimization technique to repartition data.",
      "A process of checkpointing data."
    ],
    "answer": 2,
    "category": "Spark Concepts",
    "explanation": "Option C is correct because Spark uses lazy evaluation, meaning it waits until an action is called to execute transformations, optimizing the computation."
  },
  {
    "question": "In Spark, what is the function of the 'repartition' method on a DataFrame?",
    "options": [
      "To increase or decrease the number of partitions randomly.",
      "To sort the DataFrame based on a column.",
      "To group data based on a key.",
      "To persist the DataFrame in memory.",
      "To collect data to the driver node."
    ],
    "answer": 0,
    "category": "Spark Optimization",
    "explanation": "Option A is correct because `repartition` reshuffles the data to increase or decrease the number of partitions, distributing data randomly unless a column is specified.\n\nOption B describes `sort` or `orderBy`.\n\nOption C describes `groupBy`.\n\nOption D describes `persist` or `cache`.\n\nOption E describes `collect`."
  },
  {
    "question": "In Spark SQL, what is the result of using the 'GROUPING SETS' clause in a GROUP BY statement?",
    "options": [
      "It creates multiple groupings in a single query, allowing for subtotal computations.",
      "It groups data based on sets of columns and removes duplicates.",
      "It sorts the result set based on multiple columns.",
      "It filters groups based on a condition after aggregation.",
      "It combines the results of multiple SELECT statements."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because 'GROUPING SETS' allows you to specify multiple groupings in the same query, which is useful for computing subtotals and grand totals in reports.\n\nOption B describes removing duplicates, which is not the function of 'GROUPING SETS'.\n\nOption C is related to 'ORDER BY'.\n\nOption D describes the 'HAVING' clause.\n\nOption E refers to the 'UNION' operator."
  },
  {
    "question": "In data modeling, what is a 'factless fact table'?",
    "options": [
      "A table that contains only foreign keys without measurable facts.",
      "A dimension table with no attributes.",
      "A table that combines both fact and dimension data.",
      "A table that stores summary data only.",
      "A deprecated concept in modern data warehousing."
    ],
    "answer": 0,
    "category": "Data Modeling",
    "explanation": "Option A is correct because a factless fact table captures events or conditions but contains no measurable data, only foreign keys to dimensions.\n\nOptions B, C, D, and E are incorrect definitions."
  },
  {
    "question": "What is the role of the 'CACHE' command in Spark SQL when used in Databricks?",
    "options": [
      "It saves a DataFrame to disk for persistence.",
      "It loads data into memory to improve query performance on repeated accesses.",
      "It clears the memory cache to free up resources.",
      "It archives data files into a compressed format.",
      "It checkpoints data for fault tolerance."
    ],
    "answer": 1,
    "category": "Spark SQL",
    "explanation": "Option B is correct because `CACHE` tells Spark to keep the data in memory, which speeds up subsequent actions on the same data.\n\nOption A is incorrect; saving to disk is done with `write`.\n\nOption C is incorrect; to clear cache, you use `UNCACHE`.\n\nOption D is incorrect; caching does not compress data.\n\nOption E is incorrect; checkpointing is different from caching."
  },
  {
    "question": "In PySpark, how do you register a DataFrame as a temporary view named 'sales_view' to run SQL queries against it?",
    "options": [
      "df.createTempView('sales_view')",
      "df.registerAsView('sales_view')",
      "spark.registerDataFrameAsTable(df, 'sales_view')",
      "df.createOrReplaceTempView('sales_view')",
      "df.registerTempTable('sales_view')"
    ],
    "answer": [0, 3],
    "category": "Data Transformation",
    "explanation": "Options A and D are correct because both `df.createTempView('sales_view')` and `df.createOrReplaceTempView('sales_view')` register the DataFrame as a temporary view.\n\nOption A will create the view and throw an error if a view with the same name already exists.\n\nOption D will create the view or replace it if it already exists.\n\nOption B is incorrect; `registerAsView` is not a standard method.\n\nOption C is deprecated in newer versions of Spark.\n\nOption E is deprecated in favor of `createOrReplaceTempView`."
  },
  {
    "question": "In PySpark, how do you convert a DataFrame 'df' to a pandas DataFrame?",
    "options": [
      "df.toPandas()",
      "df.convertToPandas()",
      "df.asPandas()",
      "df.to_pandas()",
      "df.pandas()"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because 'df.toPandas()' converts a PySpark DataFrame to a pandas DataFrame."
  },
  {
    "question": "In SQL, what does the 'GROUP BY' clause do?",
    "options": [
      "Filters rows based on a condition.",
      "Orders the result set.",
      "Aggregates data across multiple rows.",
      "Combines rows from two or more tables.",
      "Limits the number of rows returned."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `GROUP BY` groups rows that have the same values in specified columns and allows aggregate functions to be applied to each group.\n\nOption A describes `WHERE`.\n\nOption B describes `ORDER BY`.\n\nOption D describes `JOIN`.\n\nOption E uses `LIMIT`."
  },
  {
    "question": "In SQL, what is the purpose of the 'UNION' operator?",
    "options": [
      "To combine the results of two queries and include duplicates.",
      "To combine the results of two queries and remove duplicates.",
      "To find the intersection of two queries.",
      "To subtract one query's results from another.",
      "To join two tables based on a condition."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because `UNION` combines the results of two SELECT statements and removes any duplicate rows.\n\nOption A describes `UNION ALL`.\n\nOption C is `INTERSECT`.\n\nOption D is `EXCEPT` or `MINUS`.\n\nOption E describes a `JOIN`."
  },
  {
    "question": "In SQL, what does the 'DISTINCT' keyword do?",
    "options": [
      "Sorts the results in ascending order.",
      "Filters records based on a condition.",
      "Removes duplicate rows from the result set.",
      "Limits the number of rows returned.",
      "Groups rows that have the same values."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because 'DISTINCT' eliminates duplicate rows from the result set of a SELECT statement."
  },
  {
    "question": "In Databricks, what is the purpose of the 'Secret Scope'?",
    "options": [
      "To store and manage sensitive credentials securely.",
      "To define the scope of variables in notebooks.",
      "To set permissions for cluster usage.",
      "To manage database schemas.",
      "To monitor resource utilization."
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because Secret Scopes in Databricks are used to store and manage sensitive information like passwords and keys securely."
  },
  {
    "question": "Which SQL clause is used to combine rows from two or more tables, based on a related column between them?",
    "options": [
      "GROUP BY",
      "ORDER BY",
      "JOIN",
      "HAVING",
      "WHERE"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `JOIN` clauses are used to combine rows from two or more tables based on a related column.\n\nOptions A, B, D, and E serve different purposes and are not used for combining tables in this way."
  },
  {
    "question": "In SQL, what is the purpose of the 'LIKE' operator with wildcards?",
    "options": [
      "To perform arithmetic operations.",
      "To compare exact string matches.",
      "To search for a specified pattern in a column.",
      "To sort query results.",
      "To group results based on a condition."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because the 'LIKE' operator is used with wildcards '%' and '_' to search for patterns within string columns."
  },
  {
    "question": "What is the purpose of the 'UNION' operator in SQL?",
    "options": [
      "To select records that have matching values in both tables.",
      "To combine the result sets of two or more SELECT statements, removing duplicates.",
      "To update records in a table based on another table.",
      "To join tables based on non-matching values.",
      "To subtract one result set from another."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `UNION` operator combines the results of two or more SELECT statements into a single result set, removing duplicates.\n\nOption A describes an INNER JOIN.\n\nOption C is about UPDATE statements.\n\nOption D is more like a FULL OUTER JOIN.\n\nOption E describes the `EXCEPT` or `MINUS` operator."
  },
  {
    "question": "In Delta Lake, what command would you use to revert a table named 'transactions' to a previous version?",
    "options": [
      "RESTORE TABLE transactions TO VERSION AS OF 5;",
      "ROLLBACK TABLE transactions TO 5;",
      "REVERT TABLE transactions TO VERSION 5;",
      "UNDO TABLE transactions TO VERSION 5;",
      "RESET TABLE transactions TO VERSION 5;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because `RESTORE TABLE transactions TO VERSION AS OF 5;` reverts the table to a specific version.\n\nOptions B, C, D, and E are incorrect because they are not valid Delta Lake commands for reverting table versions."
  },
  {
    "question": "Which of the following is a key advantage of using Databricks Delta Live Tables (DLT)?",
    "options": [
      "They allow for manual data pipeline management.",
      "They automatically manage data dependencies and optimize data flows.",
      "They require extensive coding for pipeline setup.",
      "They are designed only for batch data processing.",
      "They do not support data quality checks."
    ],
    "answer": 1,
    "category": "Data Pipeline Management",
    "explanation": "Option B is correct because Delta Live Tables automate data pipeline management, handling dependencies, and optimizing data flows.\n\nOption A is incorrect; DLT reduces the need for manual management.\n\nOption C is incorrect; DLT simplifies pipeline setup with declarative configurations.\n\nOption D is incorrect; DLT supports both streaming and batch data.\n\nOption E is incorrect; DLT supports data quality checks through expectations."
  },
  {
    "question": "In data analytics, what is 'feature engineering'?",
    "options": [
      "The process of collecting raw data.",
      "The creation of new input features from existing ones to improve model performance.",
      "The selection of machine learning algorithms.",
      "The deployment of models into production.",
      "The evaluation of model accuracy using test data."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because feature engineering involves creating new features from existing data to improve the performance of machine learning models.\n\nOptions A, C, D, and E describe other stages of the machine learning workflow."
  },
  {
    "question": "In Spark, what is the difference between 'map' and 'flatMap' transformations?",
    "options": [
      "'map' returns a new RDD by applying a function to each element; 'flatMap' does the same but flattens the result.",
      "'map' groups data; 'flatMap' sorts data.",
      "'map' filters data; 'flatMap' aggregates data.",
      "'map' is an action; 'flatMap' is a transformation.",
      "There is no difference between 'map' and 'flatMap'."
    ],
    "answer": 0,
    "category": "Spark Transformations",
    "explanation": "Option A is correct because `map` applies a function to each element and returns an RDD of the same length, while `flatMap` may return multiple elements for each input element and flattens the result into a single RDD."
  },
  {
    "question": "In Databricks, how can you display the list of files in a directory '/mnt/data'?",
    "options": [
      "dbutils.fs.ls('/mnt/data')",
      "display('/mnt/data')",
      "list('/mnt/data')",
      "dbutils.fs.list('/mnt/data')",
      "fs.ls('/mnt/data')"
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because 'dbutils.fs.ls('/mnt/data')' lists the files in the specified directory."
  },
  {
    "question": "In Delta Lake, which command can you use to view the version history and operation metrics of a table named 'inventory'?",
    "options": [
      "DESCRIBE HISTORY inventory;",
      "SHOW METRICS inventory;",
      "LIST VERSIONS FOR inventory;",
      "GET HISTORY OF inventory;",
      "DISPLAY LOGS FOR inventory;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because `DESCRIBE HISTORY inventory;` shows the history of operations, including version numbers and metrics.\n\nOptions B, C, D, and E are not valid Delta Lake commands for viewing history and metrics."
  },
  {
    "question": "Which of the following is a correct way to create a DataFrame in PySpark from a Python list of tuples?",
    "options": [
      "df = spark.createDataFrame(data_list)",
      "df = spark.DataFrame(data_list)",
      "df = spark.read.DataFrame(data_list)",
      "df = spark.loadDataFrame(data_list)",
      "df = spark.makeDataFrame(data_list)"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `spark.createDataFrame(data_list)` creates a DataFrame from a list of tuples or a list of rows.\n\nOptions B, C, D, and E are not valid methods for creating DataFrames in PySpark."
  },
  {
    "question": "No Delta Lake, qual comando você usaria para remover arquivos órfãos não referenciados por uma tabela chamada 'eventos'?",
    "options": [
      "VACUUM eventos;",
      "CLEAN eventos;",
      "PURGE eventos;",
      "REMOVE ORPHANS FROM eventos;",
      "GC eventos;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "A opção A está correta porque o comando `VACUUM` remove arquivos antigos que não são mais referenciados pela tabela, liberando espaço em armazenamento.\n\nAs opções B, C, D e E não são comandos válidos para essa operação no Delta Lake."
  },
  {
    "question": "In SQL, which clause is used to specify the sort order of the result set?",
    "options": [
      "GROUP BY",
      "ORDER BY",
      "SORT BY",
      "ARRANGE BY",
      "ALIGN BY"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because `ORDER BY` is used to sort the result set in ascending or descending order.\n\nOption A is used for grouping.\n\nOption C is used in some SQL dialects but is not standard.\n\nOptions D and E are not standard SQL clauses."
  },
  {
    "question": "What does the 'collect_set' function do in Spark SQL?",
    "options": [
      "Collects all elements into a list including duplicates.",
      "Aggregates elements into a set, removing duplicates.",
      "Sorts the elements in ascending order.",
      "Collects elements and counts the frequency of each.",
      "Collects elements into a map based on a key."
    ],
    "answer": 1,
    "category": "Spark SQL Functions",
    "explanation": "Option B is correct because `collect_set` returns a set of objects with duplicate elements eliminated.\n\nOption A describes `collect_list`.\n\nOption C is not related to `collect_set`.\n\nOption D describes a counting function.\n\nOption E is incorrect; `collect_set` does not create a map."
  },
  {
    "question": "Which of the following best describes the purpose of the 'OPTIMIZE' command in Databricks Delta Lake?",
    "options": [
      "It compresses the data to save storage space.",
      "It reorganizes the data into larger files to improve query performance.",
      "It updates the statistics for the query optimizer.",
      "It checks the table for inconsistencies and repairs corrupt data.",
      "It removes old versions of data to save storage space."
    ],
    "answer": 1,
    "category": "Delta Lake Optimization",
    "explanation": "Option B is correct because `OPTIMIZE` in Delta Lake compacts small files into larger ones, which can improve query performance.\n\nOption A is partially correct but not the main purpose.\n\nOption C is incorrect; statistics are updated with `ANALYZE TABLE`.\n\nOption D is incorrect; `FSCK REPAIR TABLE` is used for repair.\n\nOption E is incorrect; `VACUUM` removes old versions."
  },
  {
    "question": "In data processing, what is the purpose of 'data partitioning'?",
    "options": [
      "To encrypt data for security purposes.",
      "To divide data into distinct chunks based on a key for parallel processing.",
      "To merge multiple datasets into one.",
      "To backup data to multiple locations.",
      "To compress data for efficient storage."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because data partitioning involves dividing data into chunks based on keys, allowing parallel processing and improved performance.\n\nOptions A, C, D, and E describe different data operations."
  },
  {
    "question": "In PySpark, which method can you use to sort a DataFrame 'df' by the column 'salary' in descending order?",
    "options": [
      "df.sort(df.salary.desc())",
      "df.orderBy('salary')",
      "df.sort_values('salary', ascending=False)",
      "df.arrange(desc('salary'))",
      "df.sortBy('salary', descending=True)"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.sort(df.salary.desc())` sorts the DataFrame by 'salary' in descending order.\n\nOption B sorts in ascending order by default.\n\nOption C is a pandas method.\n\nOption D is not a valid PySpark method.\n\nOption E is not the correct syntax."
  },
  {
    "question": "In data warehousing, what does the term 'slowly changing dimension' (SCD) refer to?",
    "options": [
      "A table that contains only static data.",
      "A dimension table where data changes slowly over time and requires strategies to manage historical data.",
      "A fact table that accumulates data slowly.",
      "A dimension that changes so rapidly that it cannot be tracked.",
      "A temporary table used during ETL processes."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because SCD refers to dimension tables that change slowly and for which historical changes need to be tracked.\n\nOptions A, C, D, and E do not accurately describe SCDs."
  },
  {
    "question": "In PySpark, what does the 'collect()' action do?",
    "options": [
      "Returns all the elements of the DataFrame to the driver node.",
      "Saves the DataFrame to a file.",
      "Performs a group by operation.",
      "Counts the number of rows in the DataFrame.",
      "Caches the DataFrame in memory."
    ],
    "answer": 0,
    "category": "Spark Actions",
    "explanation": "Option A is correct because 'collect()' retrieves all the elements of the DataFrame to the driver node, which can be useful for small datasets but should be used with caution due to memory constraints."
  },
  {
    "question": "In Spark, what is the role of an 'executor'?",
    "options": [
      "To manage the overall Spark application.",
      "To store data permanently.",
      "To run individual tasks and cache data for applications.",
      "To provide a user interface for monitoring.",
      "To compile code into executable programs."
    ],
    "answer": 2,
    "category": "Spark Concepts",
    "explanation": "Option C is correct because executors in Spark are responsible for running individual tasks on worker nodes and can cache data for the application."
  },
  {
    "question": "In Databricks, how can you access secrets stored in Azure Key Vault or AWS Secrets Manager?",
    "options": [
      "Using `dbutils.secrets.get()` with the appropriate scope and key name.",
      "Directly reading from the file system.",
      "Storing secrets in plaintext within the notebook.",
      "Using environment variables set in the cluster configuration.",
      "Secrets cannot be accessed from Databricks notebooks."
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because `dbutils.secrets.get()` allows you to retrieve secrets from configured secret scopes securely.\n\nOptions B and C are insecure or incorrect.\n\nOption D is less secure and not the standard method.\n\nOption E is incorrect; secrets can be accessed."
  },
  {
    "question": "What is the purpose of the 'MERGE INTO' command in Databricks SQL?",
    "options": [
      "To combine multiple tables into one without conditions.",
      "To insert, update, or delete records in a target table based on a source table.",
      "To remove duplicates from a table.",
      "To create a backup of a table.",
      "To split a table into multiple smaller tables."
    ],
    "answer": 1,
    "category": "SQL DML",
    "explanation": "Option B is correct because `MERGE INTO` allows you to perform upserts (insert, update, or delete operations) on a target table based on conditions applied to a source table.\n\nOption A is incorrect because merging without conditions is not the function of `MERGE INTO`.\n\nOption C is incorrect because removing duplicates is done using different methods.\n\nOption D is incorrect; `MERGE INTO` does not create backups.\n\nOption E is incorrect because splitting tables is not related to `MERGE INTO`."
  },
  {
    "question": "In Delta Lake, how do you query a previous version of a table named 'sales_data'?",
    "options": [
      "SELECT * FROM sales_data VERSION AS OF 5;",
      "SELECT * FROM sales_data@5;",
      "SELECT * FROM sales_data WHERE _version = 5;",
      "SELECT * FROM sales_data.history(5);",
      "SELECT * FROM sales_data PREVIOUS VERSION 5;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because Delta Lake supports time travel queries using `VERSION AS OF` or `TIMESTAMP AS OF`."
  },
  {
    "question": "What is the primary purpose of the 'HAVING' clause in SQL?",
    "options": [
      "To filter groups based on aggregate conditions after grouping.",
      "To specify the order in which results are returned.",
      "To filter rows before grouping.",
      "To join tables based on a condition.",
      "To limit the number of rows returned."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because the `HAVING` clause is used to filter groups based on aggregate conditions after the `GROUP BY` clause has been applied.\n\nOption B describes `ORDER BY`.\n\nOption C describes the `WHERE` clause.\n\nOption D describes `JOIN` clauses.\n\nOption E describes `LIMIT`."
  },
  {
    "question": "In SQL, which clause is used to specify a search condition for a group or an aggregate?",
    "options": [
      "WHERE",
      "GROUP BY",
      "HAVING",
      "ORDER BY",
      "FILTER"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because the `HAVING` clause is used to specify conditions on groups created by the `GROUP BY` clause, often involving aggregates.\n\nOption A filters rows before grouping.\n\nOption B is used to group rows.\n\nOption D sorts the result set.\n\nOption E is not a standard SQL clause."
  },
  {
    "question": "In SQL, what is the purpose of the 'INDEX'?",
    "options": [
      "To enforce uniqueness on a column.",
      "To improve the speed of data retrieval operations.",
      "To define relationships between tables.",
      "To store large binary objects.",
      "To backup the database."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because an INDEX is used to improve the speed of data retrieval by providing quick access to rows in a database table."
  },
  {
    "question": "In data warehousing, what is a 'fact table'?",
    "options": [
      "A table that contains historical data only.",
      "A central table in a star schema that contains quantitative data for analysis.",
      "A table that holds metadata about the database schema.",
      "A lookup table used to enforce data integrity.",
      "A table that stores unstructured data."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because a fact table is the central table in a star schema, containing measurable, quantitative data (facts) for analysis.\n\nOption A is incorrect; fact tables can contain current and historical data.\n\nOption C describes a metadata table.\n\nOption D describes a dimension or lookup table.\n\nOption E is incorrect; fact tables store structured data."
  },
  {
    "question": "In SQL, what is the function of the 'GROUP BY' clause?",
    "options": [
      "To filter records based on a condition.",
      "To group rows that have the same values in specified columns.",
      "To sort the result set.",
      "To join tables based on a related column.",
      "To limit the number of rows returned."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the 'GROUP BY' clause groups rows that have the same values in specified columns, allowing aggregate functions to be applied to each group."
  },
  {
    "question": "Em PySpark, qual método você usaria para unir dois DataFrames 'df1' e 'df2' com base em uma coluna comum 'id'?",
    "options": [
      "df1.join(df2, 'id')",
      "df1.merge(df2, on='id')",
      "df1.append(df2, 'id')",
      "df1.concat(df2, 'id')",
      "df1.union(df2, 'id')"
    ],
    "answer": 0,
    "category": "Transformação de Dados",
    "explanation": "A opção A está correta porque o método `join()` é usado para unir dois DataFrames com base em uma coluna comum.\n\nAs opções B, C, D e E não são métodos padrão em PySpark para essa operação."
  },
  {
    "question": "In Databricks, which command would you use to view the history of operations performed on a Delta Lake table, including versioning information?",
    "options": [
      "DESCRIBE HISTORY my_table;",
      "SHOW TRANSACTIONS FOR my_table;",
      "SELECT * FROM my_table VERSION AS OF 'timestamp';",
      "DISPLAY HISTORY my_table;",
      "ALTER TABLE my_table SHOW HISTORY;"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because the `DESCRIBE HISTORY` command in Databricks is used to display the history of operations performed on a Delta Lake table, including versioning information.\n\nOption B is incorrect because `SHOW TRANSACTIONS` is not a valid command in this context.\n\nOption C is incorrect because `SELECT * FROM my_table VERSION AS OF 'timestamp'` is used to query a specific version of the table, not to view the history.\n\nOption D is incorrect because `DISPLAY HISTORY` is not a valid command in Databricks SQL.\n\nOption E is incorrect because `ALTER TABLE my_table SHOW HISTORY` is not a valid syntax."
  },
  {
    "question": "In the context of data visualization, which chart type is most appropriate for showing the relationship between two numerical variables?",
    "options": [
      "Pie Chart",
      "Bar Chart",
      "Scatter Plot",
      "Histogram",
      "Line Chart"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a scatter plot is ideal for displaying the relationship between two numerical variables.\n\nOption A is incorrect; pie charts are for parts of a whole.\n\nOption B is incorrect; bar charts are for categorical data.\n\nOption D is incorrect; histograms show distribution of a single variable.\n\nOption E is incorrect; line charts show trends over time."
  },
  {
    "question": "Em SQL, qual é a finalidade da cláusula 'WITH' no início de uma consulta?",
    "options": [
      "Definir consultas auxiliares (CTEs) que podem ser referenciadas posteriormente na consulta.",
      "Estabelecer uma conexão com o banco de dados.",
      "Especificar opções de configuração para a consulta.",
      "Aplicar filtros iniciais aos dados.",
      "Indicar o usuário com permissões para executar a consulta."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A opção A está correta porque a cláusula `WITH` permite definir Common Table Expressions (CTEs), que são consultas temporárias que podem ser referenciadas na consulta principal.\n\nAs outras opções não refletem o uso da cláusula `WITH`."
  },
  {
    "question": "Which command would you use in SQL to remove all records from a table named 'logs' without deleting the table itself?",
    "options": [
      "DROP TABLE logs;",
      "DELETE FROM logs;",
      "TRUNCATE TABLE logs;",
      "REMOVE * FROM logs;",
      "CLEAR TABLE logs;"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `TRUNCATE TABLE logs;` removes all records from the table efficiently without deleting the table structure.\n\nOption A deletes the entire table.\n\nOption B deletes records but can be slower and may not reset identity counters.\n\nOptions D and E are not valid SQL commands."
  },
  {
    "question": "In machine learning, what is 'precision' in the context of classification metrics?",
    "options": [
      "The ratio of true positives to all predicted positives.",
      "The ratio of true positives to all actual positives.",
      "The ratio of true negatives to all predicted negatives.",
      "The overall accuracy of the model.",
      "The ability of the model to find all relevant cases within a dataset."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because precision measures the proportion of true positives among all positive predictions made by the model."
  },
  {
    "question": "In machine learning, what is 'cross-validation' used for?",
    "options": [
      "To increase the size of the training dataset.",
      "To assess how the results of a model will generalize to an independent dataset.",
      "To test multiple models simultaneously.",
      "To adjust the model parameters to overfit the training data.",
      "To visualize the performance of a model."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because cross-validation is a technique for evaluating how a model will perform on unseen data by partitioning the data into training and validation sets.\n\nOptions A, C, D, and E are not accurate descriptions."
  },
  {
    "question": "What is the default file format for Delta Lake tables in Databricks?",
    "options": [
      "CSV",
      "JSON",
      "ORC",
      "Parquet",
      "Avro"
    ],
    "answer": 3,
    "category": "Delta Lake",
    "explanation": "Option D is correct because Delta Lake uses Parquet as the underlying file format, adding transaction logs for ACID properties.\n\nOptions A, B, C, and E are not the default formats for Delta Lake."
  },
  {
    "question": "In Databricks, what is the purpose of a 'job cluster'?",
    "options": [
      "A cluster that runs interactive queries and notebooks.",
      "A cluster that is dedicated to running a specific job and is terminated when the job completes.",
      "A high-concurrency cluster optimized for multiple users.",
      "A cluster used only for streaming applications.",
      "A cluster that is always running to ensure low-latency job execution."
    ],
    "answer": 1,
    "category": "Databricks Clusters",
    "explanation": "Option B is correct because a job cluster in Databricks is created for a single job and is terminated after the job finishes, optimizing resource usage.\n\nOptions A and C describe different cluster types.\n\nOption D is incorrect; job clusters can run various job types.\n\nOption E is incorrect; job clusters are not always running."
  },
  {
    "question": "In Databricks SQL, how can you list all the tables available in the current database?",
    "options": [
      "SHOW TABLES;",
      "LIST ALL TABLES;",
      "DISPLAY TABLES;",
      "SELECT * FROM information_schema.tables;",
      "GET TABLES;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Option A is correct because `SHOW TABLES;` is the standard SQL command to list all tables in the current database.\n\nOption B is not standard SQL.\n\nOption C is not a valid SQL command.\n\nOption D may not be supported in all SQL dialects or require additional permissions.\n\nOption E is not a valid SQL command."
  },
  {
    "question": "In PySpark, how do you cache a DataFrame 'df' in memory to improve performance for multiple actions?",
    "options": [
      "df.cache()",
      "df.persist(storageLevel=StorageLevel.MEMORY_ONLY)",
      "df.storeInMemory()",
      "df.saveCache()",
      "df.memoryCache()"
    ],
    "answer": [0, 1],
    "category": "Spark Optimization",
    "explanation": "As opções A e B estão corretas. `df.cache()` armazena o DataFrame em cache usando o nível de armazenamento padrão `MEMORY_AND_DISK`, o que significa que os dados podem ser armazenados tanto na memória quanto no disco. Já `df.persist(storageLevel=StorageLevel.MEMORY_ONLY)` armazena o DataFrame somente na memória. Ambas as opções melhoram o desempenho ao reutilizar o DataFrame em múltiplas ações.\n\nAs opções C, D e E não são métodos válidos em PySpark."
  },
  {
    "question": "In Databricks, what is the 'Workspace' used for?",
    "options": [
      "Managing data storage and retrieval.",
      "Configuring clusters and resources.",
      "Developing notebooks and organizing projects.",
      "Monitoring jobs and workflows.",
      "Setting up security policies."
    ],
    "answer": 2,
    "category": "Databricks Features",
    "explanation": "Option C is correct because the Workspace in Databricks is where users develop notebooks, organize projects, and collaborate on code.\n\nOptions A, B, D, and E are managed in other areas of Databricks."
  },
  {
    "question": "In Databricks, what is the 'Jobs API' used for?",
    "options": [
      "Interacting with jobs programmatically to create, run, and manage them.",
      "Accessing and modifying database tables.",
      "Monitoring cluster health and performance.",
      "Implementing security protocols.",
      "Developing custom visualization tools."
    ],
    "answer": 0,
    "category": "Databricks APIs",
    "explanation": "Option A is correct because the Jobs API allows users to programmatically manage Databricks jobs, including creation and execution."
  },
  {
    "question": "Qual cláusula SQL você usaria para ordenar os resultados de uma consulta pela coluna 'nome' em ordem alfabética?",
    "options": [
      "ORDER BY nome ASC;",
      "GROUP BY nome;",
      "SORT BY nome DESC;",
      "HAVING nome;",
      "FILTER BY nome;"
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "A opção A está correta porque `ORDER BY nome ASC;` ordena os resultados pela coluna 'nome' em ordem crescente (alfabética).\n\nA opção B agrupa resultados, não os ordena.\n\nA opção C usa `DESC` para ordem decrescente.\n\nA opção D é usada para filtrar grupos após agregação.\n\nA opção E não é uma cláusula SQL padrão."
  },
  {
    "question": "In SQL, what is a 'correlated subquery'?",
    "options": [
      "A subquery that can be executed independently of the outer query.",
      "A subquery that depends on values from the outer query.",
      "A subquery that uses aggregation functions.",
      "A subquery that joins multiple tables.",
      "A subquery that only returns a single value."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because a correlated subquery is one that uses values from the outer query and must be re-evaluated for each row in the outer query.\n\nOption A describes an independent subquery.\n\nOptions C, D, and E do not specifically define a correlated subquery."
  },
  {
    "question": "In data science, what is 'bias-variance tradeoff'?",
    "options": [
      "The compromise between the complexity of a model and its performance on unseen data.",
      "The adjustment of model parameters to reduce training time.",
      "The balance between the size of the dataset and the number of features.",
      "The process of selecting important features to include in a model.",
      "The tradeoff between model accuracy and computational resources."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because the bias-variance tradeoff refers to the balance between a model's ability to generalize to new data (variance) and its accuracy on the training data (bias)."
  },
  {
    "question": "In the context of machine learning, what is 'overfitting'?",
    "options": [
      "When a model performs well on training data but poorly on new, unseen data.",
      "When a model is too simple to capture underlying patterns in data.",
      "When a model has an error due to randomness in the data.",
      "When a model's predictions are consistently biased in one direction.",
      "When a model's parameters are too large to be processed efficiently."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because overfitting occurs when a model learns the training data too well, including noise, and fails to generalize to new data.\n\nOption B describes underfitting.\n\nOption C refers to variance or noise.\n\nOption D refers to bias.\n\nOption E is about computational efficiency, not overfitting."
  },
  {
    "question": "In data modeling, what is a 'fact table'?",
    "options": [
      "A table that contains dimension data.",
      "A table that stores detailed business measurements or metrics.",
      "A temporary table used during data loading.",
      "A table that contains metadata about the database.",
      "A table used for logging database changes."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because a fact table stores quantitative data for analysis and is typically at the center of a star schema in a data warehouse."
  },
  {
    "question": "In Databricks, what is a 'Notebook' primarily used for?",
    "options": [
      "Scheduling jobs and workflows.",
      "Managing clusters and resources.",
      "Writing code, visualizing results, and documenting analysis interactively.",
      "Configuring user permissions.",
      "Storing raw data files."
    ],
    "answer": 2,
    "category": "Databricks Notebooks",
    "explanation": "Option C is correct because Databricks Notebooks are interactive environments where users can write code, visualize results, and document their analysis.\n\nOptions A, B, D, and E are managed elsewhere in Databricks."
  },
  {
    "question": "In Databricks, what is the 'FileStore' directory used for?",
    "options": [
      "Storing libraries and dependencies.",
      "Sharing files between the driver and worker nodes.",
      "Persisting data generated by notebooks for access via web URLs.",
      "Logging cluster activity.",
      "Storing configuration files."
    ],
    "answer": 2,
    "category": "Databricks Utilities",
    "explanation": "Option C is correct because the 'FileStore' directory in Databricks is used to store files that can be accessed via web URLs, making it easy to share results."
  },
  {
    "question": "What is the primary benefit of 'data caching' in Spark?",
    "options": [
      "It reduces storage costs.",
      "It allows data to be shared between different applications.",
      "It improves performance by keeping frequently accessed data in memory.",
      "It ensures data is stored securely.",
      "It automatically replicates data across clusters."
    ],
    "answer": 2,
    "category": "Spark Optimization",
    "explanation": "Option C is correct because caching data in memory can significantly improve performance for iterative algorithms and interactive queries by avoiding recomputation."
  },
  {
    "question": "In data visualization, what is the primary use of a 'choropleth map'?",
    "options": [
      "To display changes over time.",
      "To represent data values through variations in color on a map.",
      "To show relationships between two variables.",
      "To compare quantities across different categories.",
      "To display hierarchical data."
    ],
    "answer": 1,
    "category": "Data Visualization",
    "explanation": "Option B is correct because a choropleth map represents data values by varying the color intensity on geographic regions."
  },
  {
    "question": "What is the main advantage of using 'vectorized UDFs' (User Defined Functions) in PySpark?",
    "options": [
      "They allow the use of SQL syntax within PySpark code.",
      "They improve performance by processing data in batches using pandas UDFs.",
      "They enable the use of Scala code in PySpark applications.",
      "They automatically parallelize Python code without any additional effort.",
      "They provide built-in functions for machine learning tasks."
    ],
    "answer": 1,
    "category": "Data Processing",
    "explanation": "Option B is correct because vectorized UDFs (also known as pandas UDFs) in PySpark allow for high-performance data processing by utilizing Apache Arrow and pandas for vectorized operations.\n\nOption A is incorrect.\n\nOption C refers to language interoperability.\n\nOption D is partially true but not the main advantage.\n\nOption E is incorrect; vectorized UDFs are not built-in ML functions."
  },
  {
    "question": "In PySpark, how do you read a JSON file located at '/data/records.json' into a DataFrame?",
    "options": [
      "df = spark.read.json('/data/records.json')",
      "df = spark.read.format('json').load('/data/records.json')",
      "df = spark.read('/data/records.json', format='json')",
      "df = spark.load.json('/data/records.json')",
      "Both options A and B are correct"
    ],
    "answer": 4,
    "category": "Data Import",
    "explanation": "Option E is correct because both `spark.read.json()` and `spark.read.format('json').load()` are valid methods to read a JSON file into a DataFrame."
  },
  {
    "question": "Which of the following is a characteristic of structured streaming in Spark?",
    "options": [
      "It processes data in real-time using micro-batches.",
      "It requires data to be fully loaded before processing.",
      "It does not support fault tolerance.",
      "It can only read data from static sources.",
      "It cannot be integrated with Delta Lake."
    ],
    "answer": 0,
    "category": "Streaming Data",
    "explanation": "Option A is correct because structured streaming uses micro-batching to process streaming data in near real-time.\n\nOption B is incorrect; it processes data as it arrives.\n\nOption C is incorrect; it supports fault tolerance.\n\nOption D is incorrect; it can read from streaming sources.\n\nOption E is incorrect; it can integrate with Delta Lake."
  },
  {
    "question": "In PySpark, which method allows you to read data from a JDBC source into a DataFrame?",
    "options": [
      "spark.read.jdbc(url, table, properties)",
      "spark.read.format('jdbc').load(url, table, properties)",
      "spark.read.jdbc(url, table)",
      "spark.read.jdbcSource(url, table, properties)",
      "spark.read.fromJdbc(url, table, properties)"
    ],
    "answer": 0,
    "category": "Data Import",
    "explanation": "Option A is correct because `spark.read.jdbc(url, table, properties)` reads data from a JDBC source into a DataFrame."
  },
  {
    "question": "In Spark, what is a 'broadcast variable' used for?",
    "options": [
      "To share large read-only data efficiently across all worker nodes.",
      "To stream data in real-time to the Spark application.",
      "To collect results from all worker nodes back to the driver.",
      "To partition data based on a hash function.",
      "To cache data in memory for faster access."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.\n\nOption B describes streaming.\n\nOption C is related to actions like `collect()`.\n\nOption D describes partitioning.\n\nOption E describes caching."
  },
  {
    "question": "In PySpark, which function is used to aggregate data after grouping, similar to SQL's GROUP BY clause?",
    "options": [
      "df.groupBy().agg()",
      "df.aggregate()",
      "df.rollup()",
      "df.cube()",
      "df.groupby().sum()"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `df.groupBy().agg()` allows for grouping and aggregation.\n\nOption B is not a standard method.\n\nOption C and D are used for hierarchical aggregations.\n\nOption E is valid but limited to the sum function."
  },
  {
    "question": "In Spark SQL, what does the 'broadcast' function do when used in a join operation?",
    "options": [
      "It distributes the smaller DataFrame to all worker nodes to optimize the join.",
      "It broadcasts the result of the join to all clients.",
      "It partitions the data across different nodes.",
      "It collects data to the driver node.",
      "It sorts the DataFrame before joining."
    ],
    "answer": 0,
    "category": "Spark Optimization",
    "explanation": "Option A is correct because the 'broadcast' function hints Spark to broadcast the smaller DataFrame to all executor nodes, which can greatly improve the performance of joins when one of the DataFrames is small."
  },
  {
    "question": "Which of the following best describes 'data skew' in big data processing?",
    "options": [
      "Even distribution of data across partitions.",
      "Inefficient data serialization formats.",
      "Uneven distribution of data leading to performance bottlenecks.",
      "Data that is encrypted for security purposes.",
      "Outliers in data that affect statistical analysis."
    ],
    "answer": 2,
    "category": "Data Processing",
    "explanation": "Option C is correct because data skew refers to the uneven distribution of data across partitions, which can lead to some tasks taking much longer than others, causing performance issues.\n\nOption A is the opposite of data skew.\n\nOption B refers to serialization inefficiencies.\n\nOption D is about data security.\n\nOption E refers to statistical outliers."
  },
  {
    "question": "Em Databricks, como você pode compartilhar notebooks com outros membros da equipe?",
    "options": [
      "Exportando o notebook como arquivo HTML e enviando por e-mail.",
      "Compartilhando um link direto para o notebook no workspace com as permissões adequadas.",
      "Imprimindo o notebook e distribuindo cópias físicas.",
      "Reescrevendo o conteúdo do notebook em um documento de texto.",
      "Não é possível compartilhar notebooks em Databricks."
    ],
    "answer": 1,
    "category": "Notebooks Databricks",
    "explanation": "A opção B está correta porque em Databricks você pode compartilhar notebooks diretamente através de links, desde que as permissões sejam configuradas adequadamente.\n\nAs outras opções não são práticas ou estão incorretas."
  },
  {
    "question": "Which command in Databricks allows you to install libraries on a cluster?",
    "options": [
      "%install library",
      "dbutils.library.install()",
      "pip install",
      "conda install",
      "Cluster libraries UI"
    ],
    "answer": 4,
    "category": "Databricks Utilities",
    "explanation": "Option E is correct because you can install libraries on a Databricks cluster using the Cluster libraries UI in the Databricks workspace.\n\nOption A is not a valid Databricks command.\n\nOption B is deprecated.\n\nOption C and D are package managers but not the recommended way to install libraries on clusters in Databricks."
  },
  {
    "question": "O que significa o conceito de 'Data Lakehouse' no contexto de gerenciamento de dados?",
    "options": [
      "Uma plataforma que combina as capacidades de Data Lakes e Data Warehouses em um único sistema.",
      "Um repositório de dados exclusivamente para dados não estruturados.",
      "Uma ferramenta de visualização de dados em tempo real.",
      "Um serviço de armazenamento em nuvem para backup de dados.",
      "Um algoritmo de aprendizado de máquina para limpeza de dados."
    ],
    "answer": 0,
    "category": "Arquitetura de Dados",
    "explanation": "A opção A está correta porque o Data Lakehouse é uma arquitetura que une as funcionalidades de Data Lakes (armazenamento de dados brutos) e Data Warehouses (processamento e consulta de dados estruturados) em uma única plataforma.\n\nAs outras opções não definem corretamente o conceito de Data Lakehouse."
  },
  {
    "question": "In data analysis, what does the term 'data lineage' refer to?",
    "options": [
      "The chronological order of data entries in a database.",
      "The historical record of data origins, movements, and transformations.",
      "The family tree of database schemas and tables.",
      "The process of cleaning and organizing raw data.",
      "The relationship between primary and foreign keys in tables."
    ],
    "answer": 1,
    "category": "Data Governance",
    "explanation": "Option B is correct because data lineage tracks the lifecycle of data, including its origins, movements, characteristics, and transformations over time.\n\nOption A is incorrect; data lineage is not just about chronological entries.\n\nOption C is incorrect; it's not about schema hierarchies.\n\nOption D is incorrect; that's data cleansing.\n\nOption E is incorrect; data lineage is broader than key relationships."
  },
  {
    "question": "Qual é o benefício de particionar uma tabela grande no contexto de big data?",
    "options": [
      "Melhorar a segurança dos dados.",
      "Reduzir o espaço em disco utilizado.",
      "Acelerar consultas filtradas em colunas de partição.",
      "Simplificar o esquema da tabela.",
      "Eliminar a necessidade de índices."
    ],
    "answer": 2,
    "category": "Processamento de Dados",
    "explanation": "A opção C está correta porque a partição de tabelas grandes permite que consultas que filtram pelas colunas de partição acessem apenas os dados relevantes, melhorando o desempenho.\n\nAs outras opções não são os principais benefícios da partição."
  },
  {
    "question": "In Databricks, which cluster mode is optimized for concurrent execution of multiple tasks and provides automatic resource management?",
    "options": [
      "Standard Mode",
      "High Concurrency Mode",
      "Single Node Mode",
      "Local Mode",
      "Interactive Mode"
    ],
    "answer": 1,
    "category": "Databricks Clusters",
    "explanation": "Option B is correct because High Concurrency clusters are optimized for concurrent execution and provide features like automatic resource management and improved security.\n\nOptions A, C, D, and E are not optimized in the same way."
  },
  {
    "question": "Which of the following commands in Databricks SQL would you use to create a new database named 'sales_db'?",
    "options": [
      "CREATE DATABASE sales_db;",
      "NEW DATABASE sales_db;",
      "MAKE DATABASE sales_db;",
      "ADD DATABASE sales_db;",
      "INIT DATABASE sales_db;"
    ],
    "answer": 0,
    "category": "Databricks SQL",
    "explanation": "Option A is correct because `CREATE DATABASE sales_db;` is the standard SQL command to create a new database.\n\nOptions B, C, D, and E are not valid SQL commands for creating databases."
  },
  {
    "question": "In Databricks, what is the function of the 'Magic Commands' (e.g., %sql, %python)?",
    "options": [
      "To perform file system operations.",
      "To switch between different programming languages within a notebook.",
      "To configure cluster settings.",
      "To manage libraries and dependencies.",
      "To control access permissions."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because Magic Commands in Databricks notebooks allow users to switch between languages like SQL, Python, R, etc., within different cells."
  },
  {
    "question": "In data processing, what is 'serialization'?",
    "options": [
      "The process of converting an object into a format that can be stored or transmitted.",
      "The process of compressing data to save space.",
      "The process of encrypting data for security.",
      "The process of cleaning data.",
      "The process of dividing data into partitions."
    ],
    "answer": 0,
    "category": "Data Processing",
    "explanation": "Option A is correct because serialization involves converting an object into a format that can be stored or transmitted and reconstructed later."
  },
  {
    "question": "In PySpark, which method would you use to read a Parquet file located at '/data/parquet_files' into a DataFrame?",
    "options": [
      "df = spark.read.parquet('/data/parquet_files')",
      "df = spark.read.format('parquet').load('/data/parquet_files')",
      "df = spark.read('/data/parquet_files')",
      "df = spark.read.load('/data/parquet_files', format='parquet')",
      "All of the above except option C"
    ],
    "answer": 4,
    "category": "Data Import",
    "explanation": "Option E is correct because options A, B, and D are valid methods to read Parquet files in PySpark, whereas option C is incomplete.\n\nOption A uses `read.parquet`.\n\nOption B uses `read.format().load()`.\n\nOption D uses `read.load()` with the format specified."
  },
  {
    "question": "In Databricks, which built-in function would you use to remove leading and trailing whitespace from a string column 'customer_name' in a SELECT statement?",
    "options": [
      "CLEAN(customer_name)",
      "TRIM(customer_name)",
      "STRIP(customer_name)",
      "REMOVE_WHITESPACE(customer_name)",
      "LTRIM(customer_name)"
    ],
    "answer": 1,
    "category": "Data Transformation",
    "explanation": "Option B is correct because the `TRIM` function removes both leading and trailing whitespace from a string.\n\nOption A is incorrect because `CLEAN` is not a standard SQL function in Databricks.\n\nOption C is incorrect because `STRIP` is not a standard SQL function in Databricks SQL.\n\nOption D is incorrect because `REMOVE_WHITESPACE` is not a recognized function.\n\nOption E is incorrect because `LTRIM` removes only leading whitespace, not trailing whitespace."
  },
  {
    "question": "In PySpark, how do you replace all occurrences of a substring 'old' with 'new' in column 'text' of DataFrame 'df'?",
    "options": [
      "df.withColumn('text', regexp_replace('text', 'old', 'new'))",
      "df.replace('old', 'new', 'text')",
      "df.update('text', replace('old', 'new'))",
      "df.withColumn('text', translate('text', 'old', 'new'))",
      "df.transform('text', 'old', 'new')"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `regexp_replace` can be used to replace all occurrences of a substring in a column.\n\nOption B is used for value replacement but not within strings.\n\nOption C is not a valid PySpark method.\n\nOption D replaces characters, not substrings.\n\nOption E is not a standard method."
  },
  {
    "question": "When dealing with skewed data in a Spark DataFrame, which of the following techniques can help improve performance?",
    "options": [
      "Repartition the DataFrame to a single partition.",
      "Use broadcast joins to distribute data evenly.",
      "Increase the number of shuffle partitions to distribute data more evenly.",
      "Cache the DataFrame before performing any operations.",
      "Disable catalyst optimizer to prevent query optimization."
    ],
    "answer": 2,
    "category": "Spark Performance Optimization",
    "explanation": "Option C is correct because increasing the number of shuffle partitions can help distribute skewed data more evenly across the cluster.\n\nOption A is incorrect because repartitioning to a single partition can worsen performance.\n\nOption B is partially correct but broadcast joins are effective when one of the datasets is small.\n\nOption D is incorrect because caching doesn't resolve data skew.\n\nOption E is incorrect; disabling the optimizer generally reduces performance."
  },
  {
    "question": "When configuring a Databricks SQL Warehouse, which of the following settings directly impacts both performance and cost?",
    "options": [
      "The number of users connected to the warehouse.",
      "The size of the warehouse cluster (e.g., Small, Medium, Large).",
      "The region where the warehouse is deployed.",
      "The default database selected in the warehouse configuration.",
      "The color theme of the Databricks SQL UI."
    ],
    "answer": 1,
    "category": "Databricks SQL Warehouses",
    "explanation": "Option B is correct because the size of the warehouse cluster determines the computational resources allocated, which directly affects both performance and cost. Larger clusters offer better performance but at a higher cost.\n\nOption A is incorrect because while the number of users may affect performance due to concurrency, it does not directly impact cost unless it leads to scaling up the cluster.\n\nOption C is incorrect because the region affects latency and data residency but not directly performance and cost in this context.\n\nOption D is incorrect because the default database does not impact performance or cost.\n\nOption E is incorrect because UI color themes have no impact on performance or cost."
  },
  {
    "question": "In Databricks, what is the 'Community Edition'?",
    "options": [
      "A free version of Databricks with limited features for learning and experimentation.",
      "A version of Databricks intended for enterprise deployment.",
      "An open-source alternative to Databricks.",
      "A plugin for integrating Databricks with community forums.",
      "A special edition for academic institutions."
    ],
    "answer": 0,
    "category": "Databricks Features",
    "explanation": "Option A is correct because the Databricks Community Edition is a free version that provides limited features and resources for users to learn and experiment with Databricks."
  },
  {
    "question": "Qual das seguintes opções é uma prática recomendada ao escrever consultas SQL para melhorar a legibilidade?",
    "options": [
      "Escrever todas as palavras-chave em letras minúsculas.",
      "Evitar o uso de aliases para tabelas.",
      "Manter todas as cláusulas em uma única linha longa.",
      "Usar recuo e quebras de linha para separar as diferentes partes da consulta.",
      "Incluir várias instruções em uma única linha separadas por ponto e vírgula."
    ],
    "answer": 3,
    "category": "Boas Práticas SQL",
    "explanation": "A opção D está correta porque usar recuo e quebras de linha melhora a legibilidade de consultas SQL complexas.\n\nAs outras opções não são consideradas boas práticas para legibilidade."
  },
  {
    "question": "What is the main purpose of the 'explode' function in Spark SQL?",
    "options": [
      "To flatten an array or map column into multiple rows.",
      "To split a string into multiple columns.",
      "To merge multiple columns into a struct.",
      "To aggregate multiple rows into an array.",
      "To remove null values from a DataFrame."
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because `explode` turns each element of an array or each key-value pair of a map into separate rows.\n\nOptions B, C, D, and E describe different functions."
  },
  {
    "question": "In PySpark, how do you convert a DataFrame 'df' to a temporary view named 'temp_view'?",
    "options": [
      "df.createTempView('temp_view')",
      "df.registerTempView('temp_view')",
      "df.createOrReplaceTempView('temp_view')",
      "All of the above",
      "None of the above"
    ],
    "answer": 3,
    "category": "Data Transformation",
    "explanation": "Option D is correct because both `createTempView` and `createOrReplaceTempView` can be used to create a temporary view from a DataFrame.\n\n`registerTempView` is deprecated but still works in some versions."
  },
  {
    "question": "In Databricks, what is the 'Delta Live Tables' feature?",
    "options": [
      "A tool for creating and managing streaming pipelines with minimal coding.",
      "A visualization library for real-time dashboards.",
      "A feature for automated machine learning model selection.",
      "An interface for managing user permissions.",
      "A storage optimization technique."
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because Delta Live Tables is a framework for building reliable, maintainable, and testable data processing pipelines."
  },
  {
    "question": "What is the role of the 'SparkContext' in a Spark application?",
    "options": [
      "To execute tasks assigned by the driver.",
      "To coordinate the execution of tasks and manage resources.",
      "To store data permanently.",
      "To provide an interface for programming with Spark.",
      "To manage the cluster nodes."
    ],
    "answer": 3,
    "category": "Spark Concepts",
    "explanation": "Option D is correct because `SparkContext` is the entry point for a Spark application, allowing the programmer to interact with the Spark cluster."
  },
  {
    "question": "In Databricks, which function allows you to display a DataFrame 'df' in a tabular format within a notebook cell?",
    "options": [
      "show(df)",
      "display(df)",
      "print(df)",
      "render(df)",
      "visualize(df)"
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because the `display(df)` function is specific to Databricks notebooks and renders the DataFrame in a tabular format with interactive features.\n\nOption A (`show(df)`) is incorrect syntax; the correct usage is `df.show()`, which prints the DataFrame but without interactive features.\n\nOption C would attempt to print the DataFrame object, leading to less readable output.\n\nOptions D and E are not valid functions in this context."
  },
  {
    "question": "In Delta Lake, what is the default retention period for the 'VACUUM' command to protect against accidental data loss?",
    "options": [
      "7 days",
      "1 day",
      "30 days",
      "0 hours",
      "168 hours"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because the default retention period for `VACUUM` in Delta Lake is 7 days to prevent accidental data deletion.\n\nOption B is incorrect; although 1 day can be specified, it's not the default.\n\nOption C is incorrect.\n\nOption D (0 hours) is not allowed without setting a configuration.\n\nOption E is equivalent to 7 days but less commonly used."
  },
  {
    "question": "In Databricks, how would you grant a user named 'data_analyst' SELECT permissions on a table named 'sales_data' in the database 'sales_db'?",
    "options": [
      "GRANT SELECT ON sales_data TO data_analyst;",
      "GRANT SELECT ON TABLE sales_db.sales_data TO USER data_analyst;",
      "GRANT SELECT TO data_analyst ON sales_db.sales_data;",
      "ALTER TABLE sales_db.sales_data ADD PERMISSION SELECT TO data_analyst;",
      "GIVE data_analyst SELECT RIGHTS ON sales_db.sales_data;"
    ],
    "answer": 1,
    "category": "Access Control",
    "explanation": "Option B is correct because the syntax `GRANT SELECT ON TABLE database.table TO USER username;` is used to grant permissions.\n\nOption A is incorrect because it lacks the 'TABLE' keyword and database reference.\n\nOption C is incorrect syntax.\n\nOption D is incorrect because `ALTER TABLE` does not manage permissions this way.\n\nOption E is incorrect; `GIVE` is not a valid SQL command for permissions."
  },
  {
    "question": "In SQL, what does the 'NULLIF' function do?",
    "options": [
      "Returns NULL if the two expressions are equal, otherwise returns the first expression.",
      "Returns NULL if the expression is NULL, otherwise returns the expression.",
      "Replaces NULL values with a specified replacement value.",
      "Checks if an expression is NULL.",
      "Compares two expressions and returns TRUE if both are NULL."
    ],
    "answer": 0,
    "category": "SQL Functions",
    "explanation": "Option A is correct because `NULLIF(expr1, expr2)` returns NULL if `expr1` equals `expr2`; otherwise, it returns `expr1`.\n\nOption B does not describe `NULLIF`.\n\nOption C describes `ISNULL` or `COALESCE`.\n\nOption D describes `IS NULL`.\n\nOption E is not the function of `NULLIF`."
  },
  {
    "question": "Which of the following is a benefit of using Databricks Workflows over standard scheduling tools?",
    "options": [
      "Support for only SQL-based tasks.",
      "Integration with Git repositories is not supported.",
      "Ability to orchestrate complex data pipelines with dependencies between tasks.",
      "Limited to running on a single cluster.",
      "Inability to send notifications upon task completion."
    ],
    "answer": 2,
    "category": "Job Management",
    "explanation": "Option C is correct because Databricks Workflows allow for orchestration of complex data pipelines, managing dependencies between various tasks.\n\nOption A is incorrect; Workflows support multiple task types.\n\nOption B is incorrect; Git integration is supported.\n\nOption D is incorrect; Workflows can run on multiple clusters.\n\nOption E is incorrect; notifications can be configured."
  },
  {
    "question": "In Databricks, what is the function of the 'Workspace' area?",
    "options": [
      "To manage data storage and retrieval.",
      "To develop and organize notebooks, libraries, and experiments.",
      "To monitor cluster performance and logs.",
      "To configure user permissions and access control.",
      "To schedule and manage jobs."
    ],
    "answer": 1,
    "category": "Databricks Architecture",
    "explanation": "Option B is correct because the Workspace is where users develop notebooks, manage libraries, and organize experiments.\n\nOptions A, C, D, and E are managed in different areas."
  },
  {
    "question": "In Spark, what is a 'DataFrame'?",
    "options": [
      "A distributed collection of data organized into named columns.",
      "A single-node data structure for storing arrays.",
      "A graphical representation of data.",
      "A file format for storing structured data.",
      "A machine learning model."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because a DataFrame in Spark is a distributed collection of data organized into named columns, similar to a table in a relational database."
  },
  {
    "question": "In Databricks, what is the function of the 'dbutils.secrets' module?",
    "options": [
      "To manage and access encrypted secrets like passwords and keys.",
      "To perform file system operations.",
      "To create interactive widgets.",
      "To submit jobs programmatically.",
      "To monitor cluster performance."
    ],
    "answer": 0,
    "category": "Databricks Utilities",
    "explanation": "Option A is correct because 'dbutils.secrets' provides utilities for managing secrets within Databricks, allowing secure access to sensitive credentials."
  },
  {
    "question": "Which method in PySpark is used to union two DataFrames 'df1' and 'df2' with the same schema?",
    "options": [
      "df1.union(df2)",
      "df1.join(df2)",
      "df1.merge(df2)",
      "df1.concat(df2)",
      "df1.append(df2)"
    ],
    "answer": 0,
    "category": "Data Transformation",
    "explanation": "Option A is correct because 'df1.union(df2)' combines the rows of both DataFrames as long as they have the same schema.\n\nOption B performs a join operation based on keys.\n\nOption C is not a standard PySpark method.\n\nOption D is used in pandas, not PySpark.\n\nOption E is not a standard method in PySpark."
  },
  {
    "question": "In Databricks SQL, which command would you use to create a temporary view named 'temp_sales' based on a SELECT query?",
    "options": [
      "CREATE TEMPORARY VIEW temp_sales AS SELECT * FROM sales_data;",
      "CREATE VIEW temp_sales AS SELECT * FROM sales_data;",
      "CREATE GLOBAL TEMPORARY VIEW temp_sales AS SELECT * FROM sales_data;",
      "CREATE TEMP VIEW temp_sales AS SELECT * FROM sales_data;",
      "CREATE TEMPORARY TABLE temp_sales AS SELECT * FROM sales_data;"
    ],
    "answer": 0,
    "category": "SQL Views",
    "explanation": "Option A is correct because the syntax `CREATE TEMPORARY VIEW` creates a temporary view accessible in the current session.\n\nOption B is incorrect because it creates a persistent view, not temporary.\n\nOption C is incorrect because `CREATE GLOBAL TEMPORARY VIEW` creates a temporary view accessible across all sessions but may not be desired.\n\nOption D is incorrect because `CREATE TEMP VIEW` is not standard syntax.\n\nOption E is incorrect because `CREATE TEMPORARY TABLE` creates a temporary table, not a view."
  },
  {
    "question": "In SQL, what does the 'EXCEPT' operator do?",
    "options": [
      "Returns all distinct rows selected by both queries.",
      "Combines the results of two queries, including duplicates.",
      "Returns rows from the first query that are not present in the second query.",
      "Returns the intersection of two queries.",
      "Joins two tables based on a foreign key relationship."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `EXCEPT` returns rows from the first query that are not in the second query.\n\nOption A describes `INTERSECT`.\n\nOption B describes `UNION ALL`.\n\nOption D is incorrect.\n\nOption E describes a JOIN operation."
  },
  {
    "question": "Qual é a função principal do comando 'EXPLAIN' em uma consulta SQL?",
    "options": [
      "Executar a consulta e mostrar os resultados.",
      "Mostrar o plano de execução da consulta sem executá-la.",
      "Depurar erros de sintaxe na consulta.",
      "Converter a consulta em uma exibição armazenada.",
      "Otimizar automaticamente a consulta para melhor desempenho."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "A opção B está correta porque o comando `EXPLAIN` exibe o plano de execução que o banco de dados usará para executar a consulta, sem realmente executá-la.\n\nAs outras opções não refletem o propósito do `EXPLAIN`."
  },
  {
    "question": "Which of the following best describes the concept of 'idempotency' in RESTful APIs?",
    "options": [
      "An operation that always returns the same result when called multiple times.",
      "An operation that cannot be reversed once executed.",
      "An operation that requires authentication.",
      "An operation that is only executed once per client.",
      "An operation that increases system load proportionally with each call."
    ],
    "answer": 0,
    "category": "API Concepts",
    "explanation": "Option A is correct because idempotent operations produce the same result regardless of how many times they are executed.\n\nOptions B, C, D, and E do not accurately describe idempotency."
  },
  {
    "question": "In Databricks, what is the purpose of the 'display()' function when used in a notebook?",
    "options": [
      "To print text output to the console.",
      "To render DataFrames as interactive tables or visualizations.",
      "To execute shell commands.",
      "To save a DataFrame to a file.",
      "To log messages for debugging purposes."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because `display()` renders DataFrames as interactive displays in notebooks, allowing for sorting, filtering, and visualization.\n\nOption A is incorrect; `print()` is used for text output.\n\nOption C is incorrect; `%sh` is used for shell commands.\n\nOption D is incorrect; saving is done with `write` methods.\n\nOption E is incorrect; logging is done differently."
  },
  {
    "question": "Which window function would you use in SQL to assign a unique sequential integer to rows within a partition, without any gaps in numbering?",
    "options": [
      "RANK()",
      "DENSE_RANK()",
      "ROW_NUMBER()",
      "NTILE(n)",
      "LEAD()"
    ],
    "answer": 2,
    "category": "SQL Window Functions",
    "explanation": "Option C is correct because `ROW_NUMBER()` assigns a unique sequential integer to rows within a partition.\n\nOption A (`RANK()`) assigns the same rank to ties, resulting in gaps.\n\nOption B (`DENSE_RANK()`) assigns the same rank to ties without gaps but may not be sequential.\n\nOption D (`NTILE(n)`) distributes rows into n buckets.\n\nOption E (`LEAD()`) accesses data from subsequent rows."
  },
  {
    "question": "In Databricks SQL, which function would you use to extract the year from a timestamp column named 'order_date'?",
    "options": [
      "YEAR(order_date)",
      "EXTRACT(YEAR FROM order_date)",
      "GET_YEAR(order_date)",
      "DATEPART('year', order_date)",
      "FORMAT_DATE('yyyy', order_date)"
    ],
    "answer": 1,
    "category": "SQL Functions",
    "explanation": "Option B is correct because `EXTRACT(YEAR FROM order_date)` is standard SQL syntax supported in Databricks SQL.\n\nOption A is not standard SQL but may work in some dialects.\n\nOption C is incorrect; `GET_YEAR` is not a standard function.\n\nOption D is incorrect; `DATEPART` is T-SQL syntax.\n\nOption E is incorrect; `FORMAT_DATE` returns a string representation."
  },
  {
    "question": "In Databricks, what is the purpose of the 'display()' function in a notebook?",
    "options": [
      "To print text output to the console.",
      "To visualize DataFrames as interactive tables or charts.",
      "To log messages for debugging purposes.",
      "To execute shell commands.",
      "To display images stored in the file system."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because 'display()' in Databricks notebooks renders DataFrames as interactive tables and supports visualization."
  },
  {
    "question": "In data analytics, what does 'ETL' stand for?",
    "options": [
      "Extract, Transform, Load",
      "Evaluate, Train, Learn",
      "Execute, Test, Launch",
      "Encrypt, Transfer, Log",
      "Estimate, Tune, Loop"
    ],
    "answer": 0,
    "category": "Data Processing",
    "explanation": "Option A is correct because ETL stands for Extract, Transform, Load, which is a process used to collect data from various sources, transform it into a suitable format, and load it into a destination system."
  },
  {
    "question": "What is the purpose of the 'CREATE DATABASE' statement in SQL?",
    "options": [
      "To create a new table within an existing database.",
      "To create a new database schema.",
      "To create a backup of an existing database.",
      "To delete an existing database.",
      "To update the structure of an existing database."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because `CREATE DATABASE` is used to create a new database schema.\n\nOption A is done using `CREATE TABLE`.\n\nOption C is not the function of `CREATE DATABASE`.\n\nOption D is done using `DROP DATABASE`.\n\nOption E is done using `ALTER DATABASE`."
  },
  {
    "question": "Which of the following statements about Databricks Notebooks is true?",
    "options": [
      "Notebooks can only be written in Python.",
      "Notebooks support multiple languages within the same notebook using magic commands.",
      "Notebooks cannot be shared between users.",
      "Notebooks are only accessible through the Databricks CLI.",
      "Notebooks do not support version control integration."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because Databricks Notebooks allow users to use multiple languages in the same notebook by using magic commands like `%python`, `%sql`, `%scala`, `%r`.\n\nOption A is incorrect because multiple languages are supported.\n\nOption C is incorrect; notebooks can be shared.\n\nOption D is incorrect; notebooks are accessible through the web UI.\n\nOption E is incorrect; Databricks supports version control integration."
  },
  {
    "question": "In Databricks SQL, how can you limit the number of rows returned by a query to 10?",
    "options": [
      "Use `LIMIT 10` at the end of the query.",
      "Set `MAX_ROWS = 10` in the query.",
      "Include `ROWCOUNT 10` in the SELECT statement.",
      "Use `TOP 10` before the column names.",
      "Configure the dashboard settings to display only 10 rows."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because adding `LIMIT 10` at the end of the query restricts the result set to 10 rows.\n\nOption B is incorrect; `MAX_ROWS` is not standard SQL syntax.\n\nOption C is incorrect; `ROWCOUNT` is not used in this way.\n\nOption D is partially correct; `TOP 10` is valid in some SQL dialects like T-SQL but may not be supported in Databricks SQL.\n\nOption E is incorrect; dashboard settings do not affect the query's result set."
  },
  {
    "question": "In machine learning, what is 'Gradient Descent'?",
    "options": [
      "An optimization algorithm used to minimize the loss function by iteratively moving in the direction of steepest descent.",
      "A statistical method for classification problems.",
      "A technique for dimensionality reduction.",
      "An algorithm for clustering data points.",
      "A method for data normalization."
    ],
    "answer": 0,
    "category": "Machine Learning Concepts",
    "explanation": "Option A is correct because Gradient Descent is an optimization algorithm that minimizes a function by iteratively moving towards the steepest descent as defined by the negative of the gradient."
  },
  {
    "question": "In Spark SQL, what does the 'OVER' clause do in window functions?",
    "options": [
      "Defines the partitioning and ordering of rows.",
      "Filters rows based on a condition.",
      "Groups rows into buckets.",
      "Sorts the result set.",
      "Limits the number of rows returned."
    ],
    "answer": 0,
    "category": "SQL Window Functions",
    "explanation": "Option A is correct because the `OVER` clause specifies how to partition and order rows for window functions."
  },
  {
    "question": "Em visualização de dados, qual tipo de gráfico é mais adequado para mostrar a tendência de uma variável ao longo do tempo?",
    "options": [
      "Gráfico de barras",
      "Gráfico de linhas",
      "Gráfico de setores",
      "Histograma",
      "Gráfico de dispersão"
    ],
    "answer": 1,
    "category": "Visualização de Dados",
    "explanation": "A opção B está correta porque um gráfico de linhas é ideal para mostrar tendências ao longo do tempo.\n\nAs opções A, C, D e E não são as mais indicadas para representar tendências temporais."
  },
  {
    "question": "In the context of data storage, what does the acronym 'ACID' stand for?",
    "options": [
      "Analysis, Computation, Integration, Delivery",
      "Atomicity, Consistency, Isolation, Durability",
      "Automation, Control, Integrity, Deployment",
      "Access, Connectivity, Integration, Data",
      "Algorithm, Calculation, Input, Data"
    ],
    "answer": 1,
    "category": "Data Management",
    "explanation": "Option B is correct because ACID stands for Atomicity, Consistency, Isolation, Durability, which are properties that guarantee reliable processing of database transactions.\n\nOther options do not correctly represent ACID."
  },
  {
    "question": "In data visualization, what is a 'box plot' used for?",
    "options": [
      "To show the distribution of a dataset based on five summary statistics.",
      "To compare parts of a whole.",
      "To display frequencies of different categories.",
      "To plot individual data points in a two-dimensional space.",
      "To show changes over time."
    ],
    "answer": 0,
    "category": "Data Visualization",
    "explanation": "Option A is correct because a box plot displays the distribution of data based on minimum, first quartile, median, third quartile, and maximum values."
  },
  {
    "question": "In SQL, what is the difference between the WHERE and HAVING clauses?",
    "options": [
      "WHERE filters rows before aggregation; HAVING filters groups after aggregation.",
      "HAVING filters rows before aggregation; WHERE filters groups after aggregation.",
      "They are interchangeable and have no difference.",
      "WHERE is used only in DELETE statements; HAVING is used in SELECT statements.",
      "WHERE filters columns; HAVING filters rows."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because WHERE filters data before aggregation occurs, while HAVING filters the aggregated data (groups).\n\nOption B is incorrect; it reverses their functions.\n\nOption C is incorrect; they have distinct purposes.\n\nOption D is incorrect; WHERE is used in multiple statement types.\n\nOption E is incorrect; WHERE filters rows, not columns."
  },
  {
    "question": "In data analysis, what is 'normalization' typically used for?",
    "options": [
      "Reducing redundancy in database tables.",
      "Scaling numerical data to a common range.",
      "Transforming categorical data into numerical data.",
      "Increasing data variance for better analysis.",
      "Encrypting data for secure storage."
    ],
    "answer": 1,
    "category": "Data Preprocessing",
    "explanation": "Option B is correct because normalization in data analysis often refers to scaling numerical data to a common range, such as [0,1], to improve the performance of machine learning algorithms.\n\nOption A is more related to database normalization.\n\nOption C describes encoding.\n\nOption D is the opposite effect.\n\nOption E is about data security."
  },
  {
    "question": "Which of the following is a characteristic of a 'managed table' in Databricks?",
    "options": [
      "Data is stored externally, and Databricks only manages metadata.",
      "Data and metadata are both managed by Databricks, and data is stored in the default storage location.",
      "Only the data is managed by Databricks; metadata is managed externally.",
      "The table schema cannot be altered after creation.",
      "The table supports only read operations."
    ],
    "answer": 1,
    "category": "Databricks SQL",
    "explanation": "Option B is correct because in a managed table, both the data and metadata are managed by Databricks, and the data is stored in the default storage location.\n\nOption A describes an unmanaged (external) table.\n\nOption C is incorrect; both data and metadata are managed.\n\nOption D is incorrect; schemas can be altered.\n\nOption E is incorrect; managed tables support all CRUD operations."
  },
  {
    "question": "Which of the following is a characteristic of a 'NoSQL' database?",
    "options": [
      "It uses structured query language for data manipulation.",
      "It is best suited for relational data models.",
      "It provides flexible schemas and can handle unstructured data.",
      "It does not support distributed computing.",
      "It enforces ACID transactions strictly."
    ],
    "answer": 2,
    "category": "Data Storage",
    "explanation": "Option C is correct because NoSQL databases are designed for flexible schemas and can handle unstructured or semi-structured data.\n\nOptions A and B describe SQL databases.\n\nOption D is incorrect; many NoSQL databases support distributed computing.\n\nOption E is incorrect; NoSQL databases often relax ACID properties in favor of performance and scalability."
  },
  {
    "question": "In data analytics, what is the difference between 'classification' and 'regression' in machine learning?",
    "options": [
      "Classification predicts continuous numerical values; regression predicts categorical outcomes.",
      "Classification predicts categorical outcomes; regression predicts continuous numerical values.",
      "They are the same; both predict future data points.",
      "Classification groups data; regression splits data.",
      "Regression is used for clustering; classification is used for association rules."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because classification algorithms predict categorical outcomes (e.g., yes/no), while regression algorithms predict continuous numerical values (e.g., price).\n\nOption A is incorrect; it reverses the definitions.\n\nOption C is incorrect; they serve different purposes.\n\nOption D is incorrect; grouping and splitting are not accurate descriptions.\n\nOption E is incorrect; regression is not used for clustering."
  },
  {
    "question": "In Databricks, what is a 'Cluster Policy'?",
    "options": [
      "A set of rules to automate cluster creation and termination.",
      "A template that restricts the configuration options available to users when creating clusters.",
      "A method to assign users to specific clusters.",
      "A policy that encrypts data on clusters.",
      "A monitoring tool for cluster performance."
    ],
    "answer": 1,
    "category": "Databricks Administration",
    "explanation": "Option B is correct because Cluster Policies in Databricks are templates that limit the configuration options available to users, ensuring compliance and cost control."
  },
  {
    "question": "In data visualization, which chart type is best suited for showing parts of a whole as percentages?",
    "options": [
      "Bar Chart",
      "Line Chart",
      "Pie Chart",
      "Histogram",
      "Scatter Plot"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a pie chart represents data in a circular graph, showing parts of a whole as slices of the pie, useful for percentages.\n\nOption A is better for comparing quantities across categories.\n\nOption B is for trends over time.\n\nOption D displays distribution of a single variable.\n\nOption E shows relationships between two numerical variables."
  },
  {
    "question": "Em PySpark, qual método você usaria para filtrar um DataFrame 'df' para incluir apenas as linhas onde a coluna 'idade' é maior que 30?",
    "options": [
      "df.filter(df.idade > 30)",
      "df.select(df.idade > 30)",
      "df.where(df.idade > 30)",
      "df.limit(df.idade > 30)",
      "df.groupby(df.idade > 30)"
    ],
    "answer": [0, 2],
    "category": "Transformação de Dados",
    "explanation": "As opções A e C estão corretas porque os métodos `filter()` e `where()` são usados para filtrar linhas com base em uma condição e são funcionalmente equivalentes em PySpark.\n\nA opção B está incorreta; `select()` é usado para selecionar colunas.\n\nA opção D está incorreta; `limit()` é usado para limitar o número de linhas sem condição.\n\nA opção E está incorreta; `groupby()` é usado para agrupar dados."
  },
  {
    "question": "In SQL, what is the difference between 'DELETE' and 'TRUNCATE'?",
    "options": [
      "'DELETE' removes all rows and cannot be rolled back; 'TRUNCATE' removes specific rows and can be rolled back.",
      "'DELETE' removes specific rows and can be rolled back; 'TRUNCATE' removes all rows and cannot be rolled back in some systems.",
      "There is no difference; they perform the same action.",
      "'DELETE' removes the table structure; 'TRUNCATE' only removes data.",
      "'DELETE' is faster than 'TRUNCATE'."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'DELETE' removes specific rows and can be rolled back if within a transaction, while 'TRUNCATE' removes all rows and may not be rolled back depending on the system."
  },
  {
    "question": "Which of the following is a benefit of using Delta Lake for data storage?",
    "options": [
      "It does not support ACID transactions.",
      "It allows for schema enforcement and evolution.",
      "It requires proprietary hardware to operate.",
      "It does not integrate with Apache Spark.",
      "It lacks support for time travel queries."
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because Delta Lake provides schema enforcement and evolution, allowing for robust data pipelines.\n\nOption A is incorrect; Delta Lake supports ACID transactions.\n\nOption C is incorrect; it runs on commodity hardware.\n\nOption D is incorrect; Delta Lake is built to work with Spark.\n\nOption E is incorrect; Delta Lake supports time travel."
  },
  {
    "question": "In data visualization, what does a 'scatter plot' represent?",
    "options": [
      "The distribution of a single variable.",
      "The relationship between two numerical variables.",
      "The composition of categories within a whole.",
      "The trend of data over time.",
      "The hierarchical structure of data."
    ],
    "answer": 1,
    "category": "Data Visualization",
    "explanation": "Option B is correct because a scatter plot displays values for typically two variables for a set of data, showing the relationship or correlation between them."
  },
  {
    "question": "In SQL, what is a 'stored procedure'?",
    "options": [
      "A prepared SQL statement that is stored on the client side.",
      "A function that returns a single value.",
      "A batch of SQL statements that can be executed as a program on the database server.",
      "A temporary table used during query execution.",
      "A constraint applied to a table to enforce data integrity."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because a stored procedure is a set of SQL statements that can be stored in the database and executed as a program."
  },
  {
    "question": "In data visualization, which chart type is best for showing the trend of data over a continuous time interval?",
    "options": [
      "Bar Chart",
      "Pie Chart",
      "Line Chart",
      "Scatter Plot",
      "Histogram"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because line charts are ideal for showing trends over continuous time intervals.\n\nOption A is better for comparing quantities across categories.\n\nOption B shows parts of a whole.\n\nOption D is for relationships between two variables.\n\nOption E shows the distribution of a single variable."
  },
  {
    "question": "In Databricks, which method would you use to save a DataFrame 'df' as a Delta table named 'sales_data' in the default database?",
    "options": [
      "df.write.format('delta').saveAsTable('sales_data')",
      "df.saveAsDelta('sales_data')",
      "df.write.delta('sales_data')",
      "df.createDeltaTable('sales_data')",
      "df.write.saveTable('sales_data')"
    ],
    "answer": 0,
    "category": "Delta Lake",
    "explanation": "Option A is correct because `df.write.format('delta').saveAsTable('sales_data')` saves the DataFrame as a Delta table.\n\nOptions B, C, D, and E are not standard methods for saving DataFrames as Delta tables."
  },
  {
    "question": "In data visualization, what is a primary use case for a 'box plot'?",
    "options": [
      "To display the frequency distribution of a dataset.",
      "To show the relationship between two categorical variables.",
      "To represent the distribution of a dataset through quartiles, highlighting the median and outliers.",
      "To compare proportions of categories within a whole.",
      "To visualize changes over time for multiple categories."
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a box plot (or box-and-whisker plot) displays the distribution of data based on a five-number summary: minimum, first quartile, median, third quartile, and maximum, highlighting outliers.\n\nOption A is better suited for histograms.\n\nOption B is suited for heatmaps or clustered bar charts.\n\nOption D is for pie charts.\n\nOption E is for line charts or area charts."
  },
  {
    "question": "In data modeling, what is 'denormalization'?",
    "options": [
      "The process of organizing data to reduce redundancy.",
      "The process of adding redundant data to optimize read performance.",
      "Encrypting data to secure it.",
      "Partitioning data across multiple tables.",
      "Converting unstructured data into structured form."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because denormalization involves adding redundant data to database tables to improve read performance at the expense of write performance and storage."
  },
  {
    "question": "In SQL, what does the 'HAVING' clause do when used in a SELECT statement with GROUP BY?",
    "options": [
      "It filters rows before grouping.",
      "It filters groups after aggregation.",
      "It sorts the result set.",
      "It specifies the columns to group by.",
      "It limits the number of rows returned."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `HAVING` clause filters groups after the `GROUP BY` and aggregation have been performed.\n\nOption A describes the `WHERE` clause.\n\nOption C describes `ORDER BY`.\n\nOption D is the function of `GROUP BY`.\n\nOption E is achieved using `LIMIT`."
  },
  {
    "question": "In data modeling, what is 'dimensional modeling'?",
    "options": [
      "A technique used to reduce the number of dimensions in data.",
      "A design concept used to build data warehouses for easier reporting and analysis.",
      "A method for encrypting data in databases.",
      "A way to normalize data to the third normal form.",
      "A process of creating flat files from relational databases."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because dimensional modeling is a design approach for data warehouses that optimizes for query performance and ease of use, often involving star or snowflake schemas."
  },
  {
    "question": "What is the main advantage of using the Parquet file format in big data processing?",
    "options": [
      "It is a human-readable text format.",
      "It supports complex data types like arrays and nested structures.",
      "It stores data in row-based format, improving write performance.",
      "It is proprietary and offers enhanced security features.",
      "It automatically indexes data for faster querying."
    ],
    "answer": 1,
    "category": "Data Storage Formats",
    "explanation": "Option B is correct because Parquet supports complex data types and nested structures, making it suitable for big data processing.\n\nOption A is incorrect; Parquet is a binary format.\n\nOption C is incorrect; Parquet is columnar, not row-based.\n\nOption D is incorrect; Parquet is open-source.\n\nOption E is incorrect; while Parquet may improve query performance due to its columnar nature, it doesn't automatically index data."
  },
  {
    "question": "What is a key feature of 'High Concurrency' clusters in Databricks?",
    "options": [
      "They support only a single user at a time.",
      "They are optimized for interactive workloads with low latency.",
      "They do not support Apache Spark jobs.",
      "They require manual scaling of resources.",
      "They do not provide built-in security features."
    ],
    "answer": 1,
    "category": "Databricks Clusters",
    "explanation": "Option B is correct because High Concurrency clusters in Databricks are optimized for concurrent interactive workloads, providing low-latency performance and built-in security features."
  },
  {
    "question": "What is the purpose of the 'DROP' command in SQL?",
    "options": [
      "To delete data from a table.",
      "To remove a table or database entirely.",
      "To truncate data in a table.",
      "To remove duplicates from a result set.",
      "To rollback a transaction."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `DROP` command is used to remove an entire table or database, including its structure and data.\n\nOption A is done with `DELETE`.\n\nOption C uses `TRUNCATE`.\n\nOption D is achieved with `DISTINCT`.\n\nOption E uses the `ROLLBACK` command."
  },
  {
    "question": "In data analysis, what is the 'curse of dimensionality'?",
    "options": [
      "The phenomenon where adding more data improves model performance indefinitely.",
      "The difficulties that arise when analyzing data in high-dimensional spaces.",
      "The problem of having too few data points to make accurate predictions.",
      "The tendency for data to become more correlated as dimensions increase.",
      "The issue of data privacy becoming more complex with more features."
    ],
    "answer": 1,
    "category": "Machine Learning Concepts",
    "explanation": "Option B is correct because the 'curse of dimensionality' refers to various phenomena that arise when analyzing data with a large number of features, making it sparse and harder to model.\n\nOptions A, C, D, and E are incorrect interpretations."
  },
  {
    "question": "In SQL, what is the purpose of the 'CASE' statement?",
    "options": [
      "To perform conditional logic in queries, similar to if-else statements.",
      "To group rows based on a condition.",
      "To create a temporary table.",
      "To enforce uniqueness on a column.",
      "To define a transaction block."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because the `CASE` statement allows you to add conditional logic within SQL queries, functioning like an if-else construct.\n\nOptions B, C, D, and E are not purposes of the `CASE` statement."
  },
  {
    "question": "In data visualization, when is it most appropriate to use a 'histogram'?",
    "options": [
      "To show the distribution of numerical data.",
      "To compare parts of a whole.",
      "To display trends over time.",
      "To represent relationships between two variables.",
      "To map data geographically."
    ],
    "answer": 0,
    "category": "Data Visualization",
    "explanation": "Option A is correct because a histogram is used to display the distribution of numerical data by showing the number of data points that fall within specified ranges (bins)."
  },
  {
    "question": "What is 'Data Lakehouse' in the context of data architecture?",
    "options": [
      "A centralized repository for structured data only.",
      "An architecture that combines features of data lakes and data warehouses.",
      "A methodology for processing streaming data exclusively.",
      "A type of database optimized for OLTP workloads.",
      "A tool for data visualization."
    ],
    "answer": 1,
    "category": "Data Architecture",
    "explanation": "Option B is correct because a Data Lakehouse combines the scalability and flexibility of data lakes with the ACID transactions and schema enforcement of data warehouses."
  },
  {
    "question": "In SQL, how do you rename an existing table 'old_table' to 'new_table'?",
    "options": [
      "RENAME TABLE old_table TO new_table;",
      "ALTER TABLE old_table RENAME TO new_table;",
      "MODIFY TABLE old_table TO new_table;",
      "CHANGE TABLE old_table TO new_table;",
      "UPDATE TABLE old_table SET NAME = new_table;"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'ALTER TABLE old_table RENAME TO new_table;' is the standard SQL syntax for renaming a table.\n\nOption A is not standard in SQL.\n\nOptions C, D, and E are incorrect."
  },
  {
    "question": "In Spark, what is a 'Transformation'?",
    "options": [
      "An operation that returns a new RDD/DataFrame and is lazily evaluated.",
      "An action that triggers computation and returns a value.",
      "A method to persist data in memory.",
      "An operation to repartition data.",
      "A function to collect data to the driver."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because transformations create a new RDD or DataFrame from an existing one and are lazily evaluated, meaning they are not computed until an action is called."
  },
  {
    "question": "What is the primary function of the 'EXISTS' clause in a SQL query?",
    "options": [
      "To check if a table exists in the database.",
      "To return true if a subquery returns any rows.",
      "To join two tables based on a condition.",
      "To select unique values from a column.",
      "To aggregate data over a specified window."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `EXISTS` clause is used to test for the existence of any record in a subquery; it returns true if the subquery returns one or more records.\n\nOption A is incorrect; checking if a table exists is done differently.\n\nOption C is incorrect; joining tables is done with JOIN clauses.\n\nOption D is incorrect; selecting unique values is done with `DISTINCT`.\n\nOption E is incorrect; window functions are used for aggregation over windows."
  },
  {
    "question": "Em um contexto de segurança de dados, o que significa 'anonimização'?",
    "options": [
      "Criptografar os dados para evitar acesso não autorizado.",
      "Remover ou alterar informações pessoalmente identificáveis para proteger a privacidade.",
      "Fazer backup dos dados em um local seguro.",
      "Monitorar o acesso aos dados em tempo real.",
      "Restringir o acesso aos dados apenas a administradores."
    ],
    "answer": 1,
    "category": "Governança de Dados",
    "explanation": "A opção B está correta porque anonimização envolve remover ou alterar informações pessoais para que indivíduos não possam ser identificados.\n\nAs outras opções não definem corretamente a anonimização."
  },
  {
    "question": "In data warehousing, what does the term 'star schema' refer to?",
    "options": [
      "A schema with multiple fact tables connected to a single dimension table.",
      "A central fact table connected to multiple dimension tables in a shape resembling a star.",
      "A schema where all tables are interconnected without a central fact table.",
      "A design that involves nested subqueries for data retrieval.",
      "A schema that supports only real-time data processing."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because in a star schema, a central fact table is connected to multiple dimension tables, resembling a star shape.\n\nOption A is incorrect; typically, there is one fact table.\n\nOption C describes a network or mesh schema.\n\nOption D is about query design, not schema.\n\nOption E is incorrect; star schemas are used in various processing scenarios."
  },
  {
    "question": "What is the primary difference between the functions COLLECT_LIST() and COLLECT_SET() in Spark SQL?",
    "options": [
      "COLLECT_LIST() returns unique elements; COLLECT_SET() allows duplicates.",
      "COLLECT_LIST() maintains order and duplicates; COLLECT_SET() returns unique elements in random order.",
      "COLLECT_LIST() sorts the elements; COLLECT_SET() does not.",
      "There is no difference; both functions behave identically.",
      "COLLECT_LIST() works only with numeric data; COLLECT_SET() works with all data types."
    ],
    "answer": 1,
    "category": "Spark SQL Functions",
    "explanation": "Option B is correct because `COLLECT_LIST()` returns an array including duplicates and maintains the order, while `COLLECT_SET()` returns an array of unique elements without guaranteed order.\n\nOption A is incorrect because it's the opposite.\n\nOption C is incorrect because neither function sorts elements.\n\nOption D is incorrect because they do behave differently.\n\nOption E is incorrect because both functions work with all data types."
  },
  {
    "question": "In Databricks SQL, which function would you use to extract the month from a date column 'order_date'?",
    "options": [
      "MONTH(order_date)",
      "EXTRACT(MONTH FROM order_date)",
      "DATEPART('month', order_date)",
      "TO_MONTH(order_date)",
      "GETMONTH(order_date)"
    ],
    "answer": 1,
    "category": "SQL Functions",
    "explanation": "Option B is correct because `EXTRACT(MONTH FROM order_date)` is the standard SQL syntax for extracting the month from a date.\n\nOption A may work in some SQL dialects but is not standard SQL.\n\nOption C is T-SQL specific.\n\nOption D and E are not standard SQL functions."
  },
  {
    "question": "In PySpark, how do you write a DataFrame 'df' to a CSV file at '/output/data.csv' with headers included?",
    "options": [
      "df.write.csv('/output/data.csv', header=True)",
      "df.write.format('csv').save('/output/data.csv')",
      "df.to_csv('/output/data.csv', header=True)",
      "df.saveAsCsv('/output/data.csv', header=True)",
      "df.exportCsv('/output/data.csv', header=True)"
    ],
    "answer": 0,
    "category": "Data Export",
    "explanation": "Option A is correct because `df.write.csv('/output/data.csv', header=True)` writes the DataFrame to a CSV file with headers."
  },
  {
    "question": "In SQL, what does the 'LIKE' operator do in a WHERE clause?",
    "options": [
      "It checks if a value is equal to another value.",
      "It searches for a specified pattern in a column.",
      "It compares two columns for similarity.",
      "It sorts the result set based on a column.",
      "It groups rows that have the same values."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `LIKE` operator is used in a WHERE clause to search for a specified pattern in a column.\n\nOption A describes `=`.\n\nOption C is not accurate; `LIKE` searches for patterns, not similarity.\n\nOption D describes `ORDER BY`.\n\nOption E describes `GROUP BY`."
  },
  {
    "question": "In Databricks SQL, what is the effect of the following command?\n\n```sql\nCREATE OR REPLACE TABLE my_table AS SELECT * FROM source_table;\n```",
    "options": [
      "It creates 'my_table' only if it does not exist; otherwise, it does nothing.",
      "It appends data from 'source_table' into 'my_table'.",
      "It drops 'my_table' if it exists and then creates a new 'my_table' with data from 'source_table'.",
      "It merges data from 'source_table' into 'my_table'.",
      "It creates a view named 'my_table' based on 'source_table'."
    ],
    "answer": 2,
    "category": "SQL DDL",
    "explanation": "Option C is correct because `CREATE OR REPLACE TABLE` will drop the existing table if it exists and create a new one with the data from the `SELECT` statement.\n\nOption A is incorrect; that behavior is for `CREATE TABLE IF NOT EXISTS`.\n\nOption B is incorrect; it does not append but replaces.\n\nOption D is incorrect; `MERGE INTO` is used for merging.\n\nOption E is incorrect; it creates a table, not a view."
  },
  {
    "question": "In SQL, what is the purpose of the 'UNION ALL' operator?",
    "options": [
      "To combine the results of two SELECT statements and remove duplicates.",
      "To combine the results of two SELECT statements including duplicates.",
      "To find the intersection of two SELECT statements.",
      "To subtract one SELECT statement from another.",
      "To join two tables based on a common column."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because `UNION ALL` combines the results of two SELECT statements and includes all duplicates.\n\nOption A describes `UNION`.\n\nOption C is the purpose of `INTERSECT`.\n\nOption D describes `EXCEPT` or `MINUS`.\n\nOption E is about JOIN operations."
  },
  {
    "question": "In Delta Lake, which command is used to clean up invalid data files and reduce storage costs?",
    "options": [
      "CLEAN TABLE my_table;",
      "VACUUM my_table;",
      "PURGE TABLE my_table;",
      "OPTIMIZE my_table;",
      "REFRESH TABLE my_table;"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because `VACUUM my_table;` removes unreferenced files and helps reduce storage costs.\n\nOption A is not a valid command.\n\nOption C is not standard in Delta Lake.\n\nOption D is used to optimize data layout but not to remove files.\n\nOption E refreshes metadata but does not remove files."
  },
  {
    "question": "In SQL, which keyword is used to fetch only a specified number of records from a table?",
    "options": [
      "FETCH",
      "LIMIT",
      "TOP",
      "ROWNUM",
      "All of the above, depending on the SQL dialect"
    ],
    "answer": 4,
    "category": "SQL",
    "explanation": "Option E is correct because different SQL dialects use different keywords to limit records: `LIMIT` (MySQL, PostgreSQL), `TOP` (SQL Server), `ROWNUM` (Oracle), and `FETCH` (SQL:2008 standard)."
  },
  {
    "question": "Which visualization type in Databricks SQL is most appropriate for showing the distribution of a single numerical variable?",
    "options": [
      "Line Chart",
      "Bar Chart",
      "Histogram",
      "Pie Chart",
      "Scatter Plot"
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a histogram is used to represent the distribution of a single numerical variable by grouping data into bins.\n\nOption A is incorrect because line charts are best for trends over time.\n\nOption B is incorrect because bar charts compare different categories.\n\nOption D is incorrect because pie charts show parts of a whole.\n\nOption E is incorrect because scatter plots show relationships between two numerical variables."
  },
  {
    "question": "In Spark, what is the role of 'SparkSession'?",
    "options": [
      "It is the entry point to programming Spark with the Dataset and DataFrame API.",
      "It manages the cluster resources and node allocation.",
      "It is responsible for executing tasks on worker nodes.",
      "It provides a user interface for monitoring Spark jobs.",
      "It stores metadata about the data sources."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because `SparkSession` is the entry point for using the DataFrame and Dataset API in Spark."
  },
  {
    "question": "In data modeling, what is a 'dimension table'?",
    "options": [
      "A table that contains business facts or measures.",
      "A table that contains historical transaction data.",
      "A table that stores detailed transactional data.",
      "A table that contains attributes to describe business entities.",
      "A temporary table used during ETL processes."
    ],
    "answer": 3,
    "category": "Data Modeling",
    "explanation": "Option D is correct because a dimension table contains attributes that describe business entities, providing context to the data in fact tables.\n\nOption A describes a fact table.\n\nOption B is not specific to dimension tables.\n\nOption C also describes a fact table.\n\nOption E is unrelated."
  },
  {
    "question": "What is the primary role of the 'driver' in a Spark application?",
    "options": [
      "To store data across the cluster nodes.",
      "To execute tasks assigned by the executors.",
      "To coordinate the execution of tasks and manage the SparkContext.",
      "To provide a user interface for monitoring jobs.",
      "To cache data in memory for quick access."
    ],
    "answer": 2,
    "category": "Spark Concepts",
    "explanation": "Option C is correct because the driver program coordinates all the tasks, manages the SparkContext, and schedules tasks on executors.\n\nOption A describes data storage.\n\nOption B is incorrect; executors execute tasks assigned by the driver.\n\nOption D refers to the Spark UI.\n\nOption E describes caching, which is managed by executors."
  },
  {
    "question": "In data visualization, what is the main use of a 'heatmap'?",
    "options": [
      "To show the distribution of a single variable.",
      "To compare parts of a whole.",
      "To display data values as colors in a matrix.",
      "To represent data on a geographical map.",
      "To track changes over time."
    ],
    "answer": 2,
    "category": "Data Visualization",
    "explanation": "Option C is correct because a heatmap displays data values as colors within a matrix, allowing for quick visual identification of patterns and correlations."
  },
  {
    "question": "Qual dos seguintes é um componente essencial do Apache Spark para processamento de dados estruturados?",
    "options": [
      "Spark Streaming",
      "Spark SQL",
      "MLlib",
      "GraphX",
      "Spark Core"
    ],
    "answer": 1,
    "category": "Conceitos Spark",
    "explanation": "A opção B está correta porque o Spark SQL é o componente do Apache Spark usado para processamento de dados estruturados.\n\nAs outras opções são componentes para outras funcionalidades."
  },
  {
    "question": "In Databricks, which command is used to export a DataFrame 'df' to a CSV file at location '/tmp/data.csv'?",
    "options": [
      "df.write.csv('/tmp/data.csv')",
      "df.saveAsCsv('/tmp/data.csv')",
      "df.export.csv('/tmp/data.csv')",
      "df.to_csv('/tmp/data.csv')",
      "df.write.format('csv').save('/tmp/data.csv')"
    ],
    "answer": [0, 4],
    "category": "Data Export",
    "explanation": "As opções A e E estão corretas porque tanto `df.write.csv('/tmp/data.csv')` quanto `df.write.format('csv').save('/tmp/data.csv')` são métodos válidos em PySpark para escrever um DataFrame em um arquivo CSV no local especificado.\n\nOpção A é um atalho para salvar em CSV e funciona corretamente.\n\nOpções B e C não são métodos válidos em PySpark.\n\nOpção D é um método do pandas, não do PySpark."
  },
  {
    "question": "In Databricks, which of the following statements about Unity Catalog is accurate?",
    "options": [
      "Unity Catalog is a data storage system for raw data files.",
      "Unity Catalog provides centralized governance for data and AI assets across clouds.",
      "Unity Catalog is a visualization tool for creating dashboards.",
      "Unity Catalog is a feature for real-time data streaming into Delta Lake.",
      "Unity Catalog manages only machine learning models and their versions."
    ],
    "answer": 1,
    "category": "Unity Catalog",
    "explanation": "Option B is correct because Unity Catalog is designed to provide centralized governance, managing access controls, and audit information across data and AI assets in Databricks.\n\nOptions A, C, D, and E are incorrect because they do not accurately describe Unity Catalog's purpose."
  },
  {
    "question": "In SQL, what is the function of the 'EXPLAIN' statement?",
    "options": [
      "To execute the query and display the results.",
      "To provide a description of the table schema.",
      "To show the execution plan of a query without executing it.",
      "To optimize the query for better performance.",
      "To create a view based on the query."
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because `EXPLAIN` provides the execution plan for a query, helping developers understand how the database will execute it without actually running the query."
  },
  {
    "question": "In SQL, what is the difference between 'DELETE' and 'TRUNCATE' commands?",
    "options": [
      "'DELETE' removes all rows; 'TRUNCATE' removes specific rows based on a condition.",
      "'DELETE' removes specific rows; 'TRUNCATE' removes all rows and cannot be rolled back in some systems.",
      "There is no difference; they are interchangeable.",
      "'DELETE' drops the table structure; 'TRUNCATE' only removes data.",
      "'DELETE' requires more storage space than 'TRUNCATE'."
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because 'DELETE' removes specific rows and can be rolled back, while 'TRUNCATE' removes all rows, often cannot be rolled back, and is faster.\n\nOption A is incorrect.\n\nOption C is incorrect; they are not interchangeable.\n\nOption D is incorrect; 'DELETE' does not drop the table.\n\nOption E is misleading."
  },
  {
    "question": "What is a common reason to use the 'coalesce()' function in Spark?",
    "options": [
      "To combine multiple DataFrames into one.",
      "To increase the number of partitions in a DataFrame.",
      "To reduce the number of partitions in a DataFrame, often after filtering.",
      "To remove null values from a DataFrame.",
      "To cache a DataFrame in memory for faster access."
    ],
    "answer": 2,
    "category": "Spark Optimization",
    "explanation": "Option C is correct because `coalesce()` reduces the number of partitions, which can be useful after a filter operation that significantly reduces data size.\n\nOption A is done with `union()` or `join()`.\n\nOption B is done with `repartition()`.\n\nOption D is achieved with `na.drop()` or filtering.\n\nOption E is done with `cache()`."
  },
  {
    "question": "Which command in Delta Lake is used to combine small files into larger ones to improve read performance?",
    "options": [
      "COMPACT TABLE my_table;",
      "OPTIMIZE my_table;",
      "MERGE FILES IN my_table;",
      "CONSOLIDATE my_table;",
      "VACUUM my_table;"
    ],
    "answer": 1,
    "category": "Delta Lake",
    "explanation": "Option B is correct because `OPTIMIZE my_table;` compacts small files into larger ones in Delta Lake, improving read performance.\n\nOption A is not a valid command.\n\nOption C and D are incorrect; these commands do not exist in Delta Lake.\n\nOption E (`VACUUM`) removes unreferenced files but does not compact files."
  },
  {
    "question": "In Databricks, how can you read data from a JSON file stored in DBFS into a DataFrame using PySpark?",
    "options": [
      "df = spark.read.format('json').load('/path/to/file.json')",
      "df = spark.read.json('/path/to/file.json')",
      "df = spark.load.json('/path/to/file.json')",
      "df = spark.readData('/path/to/file.json', format='json')",
      "Both A and B are correct"
    ],
    "answer": 4,
    "category": "Data Import",
    "explanation": "Option E is correct because both Option A and Option B are valid ways to read a JSON file into a DataFrame using PySpark in Databricks.\n\nOption C is incorrect because `spark.load.json` is not a valid method.\n\nOption D is incorrect because `readData` is not a standard PySpark method.\n\nOption A and B are standard ways to read JSON files."
  },
  {
    "question": "What is the default behavior of the 'JOIN' keyword in SQL when no type (e.g., INNER, LEFT) is specified?",
    "options": [
      "It performs an INNER JOIN.",
      "It performs a LEFT JOIN.",
      "It performs a RIGHT JOIN.",
      "It performs a FULL OUTER JOIN.",
      "An error occurs due to missing JOIN type."
    ],
    "answer": 0,
    "category": "SQL",
    "explanation": "Option A is correct because by default, `JOIN` without a specified type performs an INNER JOIN.\n\nOptions B, C, and D are incorrect.\n\nOption E is incorrect; most SQL dialects default to INNER JOIN without errors."
  },
  {
    "question": "In PySpark, how can you add a constant column named 'status' with value 'active' to DataFrame 'df'?",
    "options": [
      "df.withColumn('status', 'active')",
      "df.withColumn('status', lit('active'))",
      "df.addColumn('status', 'active')",
      "df.withConstantColumn('status', 'active')",
      "df.insertColumn('status', 'active')"
    ],
    "answer": 1,
    "category": "Data Transformation",
    "explanation": "Option B is correct because `lit('active')` creates a column with a constant value in PySpark.\n\nOption A would attempt to add a column with a string column reference, not a constant value.\n\nOptions C, D, and E are not valid PySpark methods."
  },
  {
    "question": "In data warehousing, what is 'data mart'?",
    "options": [
      "A large centralized repository of data.",
      "A subset of a data warehouse focused on a particular subject or department.",
      "A system for real-time data processing.",
      "An unstructured data storage system.",
      "A process for data cleaning and transformation."
    ],
    "answer": 1,
    "category": "Data Modeling",
    "explanation": "Option B is correct because a data mart is a subset of a data warehouse, typically focused on a single subject area or department within an organization.\n\nOption A describes a data warehouse.\n\nOptions C, D, and E are unrelated to data marts."
  },
  {
    "question": "In Spark, what is a 'checkpoint' used for?",
    "options": [
      "To save the current state of an RDD to reliable storage.",
      "To cache data in memory for faster access.",
      "To partition data across nodes.",
      "To collect data back to the driver.",
      "To optimize query execution plans."
    ],
    "answer": 0,
    "category": "Spark Concepts",
    "explanation": "Option A is correct because checkpointing saves the state of an RDD or DataFrame to reliable storage (like HDFS) to prevent data loss and recomputation in case of failures.\n\nOptions B, C, D, and E describe different Spark functionalities."
  },
  {
    "question": "Which of the following is a key characteristic of a 'distributed file system' like HDFS?",
    "options": [
      "Data is stored on a single machine for quick access.",
      "It does not support redundancy or fault tolerance.",
      "Data is spread across multiple machines to enable parallel processing.",
      "It requires proprietary hardware to function.",
      "It is only suitable for small datasets."
    ],
    "answer": 2,
    "category": "Distributed Systems",
    "explanation": "Option C is correct because distributed file systems like HDFS store data across multiple machines, enabling parallel processing and fault tolerance.\n\nOption A is incorrect.\n\nOption B is incorrect; HDFS supports redundancy.\n\nOption D is incorrect; HDFS runs on commodity hardware.\n\nOption E is incorrect; it's designed for large datasets."
  },
  {
    "question": "In data warehousing, what is a 'conformed dimension'?",
    "options": [
      "A dimension that is unique to a single fact table.",
      "A dimension that has multiple hierarchies.",
      "A dimension that is shared across multiple fact tables or data marts.",
      "A dimension that changes rapidly over time.",
      "A dimension that contains calculated measures."
    ],
    "answer": 2,
    "category": "Data Modeling",
    "explanation": "Option C is correct because a conformed dimension is consistent and shared across multiple fact tables or data marts, ensuring data integrity and uniform reporting."
  },
  {
    "question": "Em Spark, qual é o propósito do 'persist()' em comparação com 'cache()'?",
    "options": [
      "'persist()' armazena os dados em memória; 'cache()' armazena em disco.",
      "'persist()' permite escolher o nível de armazenamento; 'cache()' é equivalente a 'persist(StorageLevel.MEMORY_ONLY)'.",
      "'persist()' salva o DataFrame no sistema de arquivos; 'cache()' salva temporariamente.",
      "'persist()' é usado para dados estruturados; 'cache()' para dados não estruturados.",
      "'persist()' é obsoleto e substituído por 'cache()'."
    ],
    "answer": 1,
    "category": "Otimização Spark",
    "explanation": "A opção B está correta porque `persist()` permite especificar o nível de armazenamento (memória, disco, etc.), enquanto `cache()` é um atalho para `persist(StorageLevel.MEMORY_ONLY)`.\n\nAs outras opções não descrevem corretamente as diferenças entre os dois métodos."
  },
  {
    "question": "In Databricks SQL, which function would you use to concatenate two strings 'first_name' and 'last_name' with a space in between?",
    "options": [
      "CONCAT(first_name, ' ', last_name)",
      "first_name || ' ' || last_name",
      "CONCATENATE(first_name, ' ', last_name)",
      "CONCAT_WS(' ', first_name, last_name)",
      "All of the above except option C"
    ],
    "answer": 4,
    "category": "SQL Functions",
    "explanation": "Option E is correct because options A, B, and D are valid methods to concatenate strings in SQL, depending on the SQL dialect. Option C is not a standard SQL function."
  },
  {
    "question": "In SQL, which keyword is used to remove duplicate rows from the result set of a SELECT query?",
    "options": [
      "UNIQUE",
      "DISTINCT",
      "DELETE DUPLICATES",
      "FILTER",
      "GROUP BY"
    ],
    "answer": 1,
    "category": "SQL",
    "explanation": "Option B is correct because the `DISTINCT` keyword ensures that duplicate rows are removed from the result set.\n\nOption A (`UNIQUE`) is not used in SELECT statements.\n\nOption C is not a valid SQL command.\n\nOption D (`FILTER`) is not used for removing duplicates.\n\nOption E (`GROUP BY`) groups rows but does not remove duplicates unless combined with aggregate functions."
  },
  {
    "question": "In SQL, which function would you use to return the current date and time?",
    "options": [
      "GETDATE()",
      "CURRENT_TIMESTAMP",
      "NOW()",
      "TODAY()",
      "DATE()"
    ],
    "answer": 1,
    "category": "SQL Functions",
    "explanation": "Option B is correct because `CURRENT_TIMESTAMP` returns the current date and time in SQL.\n\nOption A (`GETDATE()`) is specific to SQL Server.\n\nOption C (`NOW()`) is used in MySQL.\n\nOption D (`TODAY()`) returns only the date, not time.\n\nOption E (`DATE()`) typically converts a datetime to a date."
  },
  {
    "question": "In a Databricks notebook, what is the main purpose of the MAGIC command '%sql'?",
    "options": [
      "To execute operating system commands from within the notebook.",
      "To switch the notebook's default language to SQL.",
      "To load SQL-specific libraries into the notebook environment.",
      "To comment out a block of code in the notebook.",
      "To render SQL query results as interactive visualizations."
    ],
    "answer": 1,
    "category": "Databricks Notebooks",
    "explanation": "Option B is correct because the MAGIC command `%sql` tells the notebook to interpret the following cell as SQL code.\n\nOption A is incorrect; that's done with `%sh`.\n\nOption C is incorrect; libraries are imported differently.\n\nOption D is incorrect; comments are made with `--` or `/* */`.\n\nOption E is incorrect; while `%sql` can be used before a query that can be visualized, the main purpose is to execute SQL code."
  },
  {
    "question": "In SQL, which operator would you use to check if a value exists within a set of values?",
    "options": [
      "BETWEEN",
      "LIKE",
      "IN",
      "EXISTS",
      "ANY"
    ],
    "answer": 2,
    "category": "SQL",
    "explanation": "Option C is correct because the `IN` operator is used to check if a value exists within a specified set of values."
  }
]